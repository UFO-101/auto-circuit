{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"AutoCircuit","text":"<p>A library for efficient patching and automatic circuit discovery.</p> <p></p>"},{"location":"#installation","title":"Installation","text":"<pre><code>pip install auto-circuit\n</code></pre>"},{"location":"#easy-and-efficient-edge-patching","title":"Easy and Efficient Edge Patching","text":"<pre><code>patch_edges = [\n    \"Resid Start-&gt;MLP 2\",\n    \"MLP 2-&gt;A2.4.Q\",\n    \"A2.4-&gt;Resid End\",\n]\nwith patch_mode(model, ablations, patch_edges):\n    patched_out = model(tokens)\n</code></pre>"},{"location":"#different-ablation-methods","title":"Different Ablation Methods","text":"<pre><code>ablations = src_ablations(model, test_loader, AblationType.TOKENWISE_MEAN_CORRUPT)\n</code></pre>"},{"location":"#automatic-circuit-discovery","title":"Automatic Circuit Discovery","text":"<pre><code>attribution_scores: PruneScores = mask_gradient_prune_scores(\n    model=model,\n    dataloader=train_loader,\n    official_edges=None,\n    grad_function=\"logit\",\n    answer_function=\"avg_diff\",\n    mask_val=0.0,\n)\n</code></pre>"},{"location":"#visualization","title":"Visualization","text":"<pre><code>fig = draw_seq_graph(model, prune_scores)\n</code></pre>"},{"location":"guides/1%29_Getting_Started/","title":"1) Getting Started","text":""},{"location":"guides/1%29_Getting_Started/#installation","title":"Installation","text":"<pre><code>pip install auto-circuit\n</code></pre>"},{"location":"guides/1%29_Getting_Started/#patch-some-edges","title":"Patch some edges","text":""},{"location":"guides/1%29_Getting_Started/#imports","title":"Imports","text":"<pre><code>import torch as t\n\nfrom auto_circuit.data import load_datasets_from_json\nfrom auto_circuit.experiment_utils import load_tl_model\nfrom auto_circuit.types import AblationType\nfrom auto_circuit.utils.ablation_activations import src_ablations\nfrom auto_circuit.utils.graph_utils import patch_mode, patchable_model\nfrom auto_circuit.utils.misc import repo_path_to_abs_path\nfrom auto_circuit.visualize import draw_seq_graph\n</code></pre>"},{"location":"guides/1%29_Getting_Started/#load-a-model","title":"Load a model","text":"<pre><code>device = t.device(\"cuda\" if t.cuda.is_available() else \"cpu\")\nmodel = load_tl_model(\"gpt2\", device)\n</code></pre>"},{"location":"guides/1%29_Getting_Started/#load-a-dataset","title":"Load a dataset","text":"<p>Datasets files must be in the specified format. You can download the example IOI dataset file from the AutoCircuit repo. <pre><code>path = repo_path_to_abs_path(\"datasets/ioi/ioi_vanilla_template_prompts.json\")\ntrain_loader, test_loader = load_datasets_from_json(\n    model=model,\n    path=path,\n    device=device,\n    prepend_bos=True,\n    batch_size=16,\n    train_test_size=(128, 128),\n)\n</code></pre></p>"},{"location":"guides/1%29_Getting_Started/#prepare-the-model-for-patching","title":"Prepare the model for patching","text":"<p>This function builds the computational graph, wraps the model in the PatchableModel class and injects PatchWrapper instances that wrap all of the SrcNodes and DestNodes in the model. <pre><code>model = patchable_model(\n    model,\n    factorized=True,\n    slice_output=\"last_seq\",\n    separate_qkv=True,\n    device=device,\n)\n</code></pre></p>"},{"location":"guides/1%29_Getting_Started/#gather-activations-to-be-patched-into-the-model","title":"Gather activations to be patched into the model","text":"<pre><code>ablations = src_ablations(model, test_loader, AblationType.TOKENWISE_MEAN_CORRUPT)\n</code></pre>"},{"location":"guides/1%29_Getting_Started/#run-the-patched-model","title":"Run the patched model","text":"<pre><code>patch_edges = [\n    \"Resid Start-&gt;MLP 1\",\n    \"MLP 1-&gt;MLP 2\",\n    \"MLP 1-&gt;MLP 3\",\n    \"MLP 2-&gt;A5.2.Q\",\n    \"MLP 3-&gt;A5.2.Q\",\n    \"A5.2-&gt;Resid End\",\n]\n\nwith patch_mode(model, ablations, patch_edges):\n    for batch in test_loader:\n        patched_out = model(batch.clean)\n</code></pre>"},{"location":"guides/1%29_Getting_Started/#visualize-the-patched-edges","title":"Visualize the patched edges","text":"<pre><code>prune_scores = model.current_patch_masks_as_prune_scores()\nfig = draw_seq_graph(model, prune_scores)\n</code></pre>"},{"location":"guides/1%29_Getting_Started/#all-together","title":"All together","text":"<pre><code>import torch as t\n\nfrom auto_circuit.data import load_datasets_from_json\nfrom auto_circuit.experiment_utils import load_tl_model\nfrom auto_circuit.types import AblationType\nfrom auto_circuit.utils.ablation_activations import src_ablations\nfrom auto_circuit.utils.graph_utils import patch_mode, patchable_model\nfrom auto_circuit.utils.misc import repo_path_to_abs_path\nfrom auto_circuit.visualize import draw_seq_graph\n\ndevice = t.device(\"cuda\" if t.cuda.is_available() else \"cpu\")\nmodel = load_tl_model(\"gpt2\", device)\n\npath = repo_path_to_abs_path(\"datasets/ioi/ioi_vanilla_template_prompts.json\")\ntrain_loader, test_loader = load_datasets_from_json(\n    model=model,\n    path=path,\n    device=device,\n    prepend_bos=True,\n    batch_size=16,\n    train_test_size=(128, 128),\n)\n\nmodel = patchable_model(\n    model,\n    factorized=True,\n    slice_output=\"last_seq\",\n    separate_qkv=True,\n    device=device,\n)\n\nablations = src_ablations(model, test_loader, AblationType.TOKENWISE_MEAN_CORRUPT)\n\npatch_edges = [\n    \"Resid Start-&gt;MLP 1\",\n    \"MLP 1-&gt;MLP 2\",\n    \"MLP 1-&gt;MLP 3\",\n    \"MLP 2-&gt;A5.2.Q\",\n    \"MLP 3-&gt;A5.2.Q\",\n    \"A5.2-&gt;Resid End\",\n]\n\nwith patch_mode(model, ablations, patch_edges):\n    for batch in test_loader:\n        patched_out = model(batch.clean)\n\nprune_scores = model.current_patch_masks_as_prune_scores()\nfig = draw_seq_graph(model, prune_scores)\nfig.write_image(repo_path_to_abs_path(\"docs/assets/Small_Circuit_Viz.png\"), scale=4)\n</code></pre>"},{"location":"guides/2%29_Circuit_Discovery/","title":"2) Circuit Discovery","text":""},{"location":"guides/2%29_Circuit_Discovery/#prepare-model-and-data","title":"Prepare model and data","text":"<pre><code>import torch as t\n\nfrom auto_circuit.data import load_datasets_from_json\nfrom auto_circuit.experiment_utils import load_tl_model\nfrom auto_circuit.prune_algos.mask_gradient import mask_gradient_prune_scores\nfrom auto_circuit.types import PruneScores\nfrom auto_circuit.utils.graph_utils import patchable_model\nfrom auto_circuit.utils.misc import repo_path_to_abs_path\nfrom auto_circuit.visualize import draw_seq_graph\n\ndevice = t.device(\"cuda\" if t.cuda.is_available() else \"cpu\")\nmodel = load_tl_model(\"gpt2\", device)\n\npath = repo_path_to_abs_path(\"datasets/ioi/ioi_vanilla_template_prompts.json\")\ntrain_loader, test_loader = load_datasets_from_json(\n    model=model,\n    path=path,\n    device=device,\n    prepend_bos=True,\n    batch_size=16,\n    train_test_size=(128, 128),\n)\n\nmodel = patchable_model(\n    model,\n    factorized=True,\n    slice_output=\"last_seq\",\n    separate_qkv=True,\n    device=device,\n)\n</code></pre>"},{"location":"guides/2%29_Circuit_Discovery/#edge-attribution-patching-circuit-discovery","title":"Edge Attribution Patching circuit discovery","text":"<p>Mask gradients are a faster way to compute Edge Attribution Patching, see the reference documentation for more details. <pre><code>attribution_scores: PruneScores = mask_gradient_prune_scores(\n    model=model,\n    dataloader=train_loader,\n    official_edges=None,\n    grad_function=\"logit\",\n    answer_function=\"avg_diff\",\n    mask_val=0.0,\n)\n</code></pre> AutoCircuit supports a range of circuit discovery methods.</p> <ul> <li>Edge Attribution Patching</li> <li>ACDC</li> <li>Subnetwork Probing</li> </ul> <p>For the full set of available methods, see the reference documentation.</p>"},{"location":"guides/2%29_Circuit_Discovery/#visualize-the-circuit","title":"Visualize the circuit","text":"<p><pre><code>fig = draw_seq_graph(\n    model, attribution_scores, 3.5, layer_spacing=True, orientation=\"v\"\n)\n</code></pre> Blue edges represent positive contributions to the output, red edges represent negative contributions, and the thickness of the edge represents the magnitude of the contribution.</p> <p></p>"},{"location":"guides/2%29_Circuit_Discovery/#token-specific-circuit-discovery","title":"Token specific circuit discovery","text":"<p>AutoCircuit can construct a computation graph that differentiates between different token positions. All prompts in the dataset must have the same sequence length.</p>"},{"location":"guides/2%29_Circuit_Discovery/#get-prompt-sequence-length","title":"Get prompt sequence length","text":"<p>Set <code>return_seq_length=True</code> in load_datasets_from_json to return the sequence length of the prompt. <pre><code>train_loader, test_loader = load_datasets_from_json(\n    model=model,\n    path=path,\n    device=device,\n    prepend_bos=True,\n    batch_size=16,\n    train_test_size=(128, 128),\n    return_seq_length=True,\n)\n</code></pre></p>"},{"location":"guides/2%29_Circuit_Discovery/#create-the-computation-graph-with-token-specific-edges","title":"Create the computation graph with token specific edges","text":"<p>Set <code>seq_length</code> to the sequence length of the prompt in patchable_model. <pre><code>model = patchable_model(\n    model,\n    factorized=True,\n    slice_output=\"last_seq\",\n    seq_len=test_loader.seq_len,\n    separate_qkv=True,\n    device=device,\n)\n</code></pre></p>"},{"location":"guides/2%29_Circuit_Discovery/#edge-attribution-patching-circuit-discovery_1","title":"Edge Attribution Patching circuit discovery","text":"<pre><code>attribution_scores: PruneScores = mask_gradient_prune_scores(\n    model=model,\n    dataloader=train_loader,\n    official_edges=None,\n    grad_function=\"logit\",\n    answer_function=\"avg_diff\",\n    mask_val=0.0,\n)\n</code></pre>"},{"location":"guides/2%29_Circuit_Discovery/#visualize-the-circuit_1","title":"Visualize the circuit","text":"<pre><code>fig = draw_seq_graph(model, attribution_scores, 3.5, seq_labels=train_loader.seq_labels)\n</code></pre>"},{"location":"guides/3%29_Other_Features/","title":"3) Other Features","text":""},{"location":"guides/3%29_Other_Features/#ablation-types","title":"Ablation types","text":"<ul> <li>Resample (aka. patching)</li> <li>Zero</li> <li>Mean (calculated over a batch or PromptDataset)</li> </ul> <p>See AblationType for more details.</p> <pre><code>ablations = src_ablations(model, test_loader, AblationType.RESAMPLE)\n</code></pre>"},{"location":"guides/3%29_Other_Features/#automatic-kv-caching","title":"Automatic KV caching","text":"<p>When <code>tail_divergence</code> is <code>True</code>, <code>load_datasets_from_json</code> automatically computes the KV Cache for the common prefix of all of the prompts in the dataset and removes the prefix from the prompts.</p> <pre><code>train_loader, test_loader = load_datasets_from_json(\n    model=model,\n    path=path,\n    device=device,\n    prepend_bos=True,\n    batch_size=16,\n    train_test_size=(128, 128),\n    tail_divergence=True,\n)\n</code></pre> <p>The KV Caches are stored in the <code>kv_cache</code> attribute of the <code>PromptDataloader</code>s. Pass the caches to the <code>patchable_model</code> function to use them automatically.</p> <pre><code>model = patchable_model(\n    model,\n    factorized=True,\n    slice_output=\"last_seq\",\n    separate_qkv=True,\n    kv_caches=(train_loader.kv_cache, test_loader.kv_cache),\n    device=device,\n)\n</code></pre>"},{"location":"guides/3%29_Other_Features/#automatically-patch-multiple-circuits","title":"Automatically patch multiple circuits","text":"<p>To patch multiple circuits of increasing size (decreasing <code>PruneScores</code>), use the <code>run_circuits</code> function.</p> <pre><code>patch_edges: Dict[str, float] = {\n    \"Resid Start-&gt;MLP 1\": 1.0,\n    \"MLP 1-&gt;MLP 2\": 2.0,\n    \"MLP 1-&gt;MLP 3\": 1.0,\n    \"MLP 2-&gt;A5.2.Q\": 2.0,\n    \"MLP 3-&gt;A5.2.Q\": 1.0,\n    \"A5.2-&gt;Resid End\": 1.0,\n}\nps: PruneScores = model.circuit_prune_scores(edge_dict=patch_edges)\n\ncircuit_outs: CircuitOutputs = run_circuits(\n    model=model,\n    dataloader=test_loader,\n    test_edge_counts=edge_counts_util(model.edges, prune_scores=ps),\n    prune_scores=ps,\n    patch_type=PatchType.EDGE_PATCH,\n    ablation_type=AblationType.RESAMPLE,\n)\n</code></pre>"},{"location":"guides/3%29_Other_Features/#measure-circuit-metrics","title":"Measure circuit metrics","text":"<p><pre><code>kl_divs = measure_kl_div(model, test_loader, circuit_outs)\n</code></pre> For a full list of metrics, see the reference documentation.</p>"},{"location":"guides/4%29_How_it_Works/","title":"4) How it Works","text":""},{"location":"guides/4%29_How_it_Works/#prune-scores","title":"Prune Scores","text":"<p>The most important data structure to understand in AutoCircuit is the <code>PruneScores</code> object. This type is a map: <pre><code>Dict[str, Tensor]\n</code></pre> Where the keys are the <code>module_name</code>s of the <code>DestNode</code>s in the model and the values are tensors with entries corresponding to the attribution scores for each edge that points to that node.</p> <p>You can access the score for a particular <code>Edge</code> by indexing into the tensor at index given by the patch_idx of the edge.</p> <pre><code>score = prune_scores[edge.dest.module_name][edge.patch_idx]\n</code></pre>"},{"location":"guides/4%29_How_it_Works/#patch-masks","title":"Patch Masks","text":"<p>Each <code>DestNode</code> is wrapped by a <code>PatchWrapper</code> that contains a <code>patch_mask</code> Pytorch <code>Parameter</code>. This tensor corresponds exactly to the tensor in the <code>PruneScores</code> object that is indexed by the <code>DestNode</code> <code>module_name</code>.</p> <p>The value of the <code>patch_mask</code> for each edge interpolates between the default value of the edge in the current forward pass and the value of the edge in <code>patch_src_outs</code> when the <code>patch_mode</code> context manager is active.</p> <p>There are helper functions to access the current mask value for a particular edge: <pre><code>score = edge.patch_mask(model).data[edge.patch_idx]\n</code></pre></p> <p>For a more thorough explanation of how patching works, see the announcement post for this library.</p>"},{"location":"guides/contributing/","title":"Contributing","text":""},{"location":"guides/contributing/#getting-started","title":"Getting Started","text":"<ul> <li>Clone the repository: <pre><code>git clone https://github.com/UFO-101/auto-circuit.git\n</code></pre></li> <li>Install poetry</li> <li>Run <code>poetry install --with dev</code> to install dependencies</li> </ul> <p><code>poetry.toml</code> is configured to use system packages. This can be helpful when working on a cluster with PyTorch already available. To change this set <code>options.system-site-packages</code> to <code>false</code> in <code>poetry.toml</code>.</p>"},{"location":"guides/contributing/#linting-and-testing","title":"Linting and Testing","text":"<ul> <li>Pyright is used for type checking. Type hints are required for all functions.</li> <li>Tests are written with Pytest</li> <li>Black is used for formatting.</li> <li>Linting with ruff.</li> </ul> <p>To check / fix your code run: <pre><code>pre-commit run --all-files\n</code></pre> Install the git hook with: <pre><code>pre-commit install\n</code></pre> To run the full test suite: <pre><code>pytest --runslow\n</code></pre></p>"},{"location":"guides/contributing/#development","title":"Development","text":"<p>The code is written in a functional style as far as possible. This means that there should be no global state and no side effects. This means not writing classes except frozen dataclasses (which are essentially just structs) and not using variables outside of functions. Functions should just take in data and return data. The major exception to this is the patching code which injects modules into the main models and patches based on patch_mask instance variables. We use context managers to ensure that state remains local to each function.</p>"},{"location":"guides/contributing/#documentation","title":"Documentation","text":"<p>Documentation is built with Material for MkDocs. Source files are in the <code>docs/</code> directory. Reference documentation is automatically generated from docstrings using MkDocs-Material-Docs To build the documentation locally run: <pre><code>mkdocs serve\n</code></pre> with the python environment activated.</p>"},{"location":"guides/contributing/#running-experiments","title":"Running Experiments","text":"<p>An experiment is defined by a <code>Task</code>, <code>PruneAlgo</code> and <code>Metric</code>. A <code>Task</code> defines a behavior that a model can perform. A <code>PruneAlgo</code> (pruning algorithm) finds edges that perform the behavior. A <code>Metric</code> evaluates the performance of the model on the task after pruning the unimportant edges. Experiments are setup and performed in <code>experiments.py</code>.</p>"},{"location":"guides/contributing/#tasks","title":"Tasks","text":"<p>Tasks are defined in <code>tasks.py</code>. They require a model and a dataset.  - If <code>_model_def</code> is set to a string, then the <code>Task</code> object will try to load a TransformerLens model with that name, otherwise <code>_model_def</code> should just be the actual model object.  - Datasets are defined by a <code>_dataset_name</code>, which should be the name of a JSON file in <code>/datasets</code> (excluding the <code>.json</code> extension). The JSON file should contain a list of <code>prompts</code> with <code>clean</code> and <code>corrupt</code> inputs and <code>correct</code> and <code>incorrect</code> outputs.</p>"},{"location":"guides/contributing/#prunealgos","title":"PruneAlgos","text":"<p>Pruning Algorithms are defined in <code>prune_algos/prune_algos.py</code>. They require a function that takes a <code>Task</code> object and returns a <code>PruneScores</code> object, which is a dictionary mapping from <code>nn.Module</code> names to tensors. Each element of the tensor represents an edge from some SrcNode to some DestNode.</p>"},{"location":"guides/contributing/#metrics","title":"Metrics","text":"<p>Metrics are defined in <code>metrics/</code>. These are usually functions that map a <code>Task</code> object along with a <code>PruneScores</code> or <code>CircuitOutputs</code> object to a list of <code>x,y</code> Measurements. (In prune_metrics/ <code>x</code> is the number of edges and <code>y</code> is some metric of faithfulness).</p>"},{"location":"guides/contributing/#pruning","title":"Pruning","text":"<p>The core of the codebase implements edge patching in a flexible and efficient manner. The <code>Nodes</code> and <code>Edges</code> of a model are computed in <code>/model_utils.py</code>. <code>PatchWrapper</code> modules are injected at the <code>Node</code> positions (see <code>graph_utils.py</code>) and a <code>PatchableModel</code> is returned. When a <code>PatchableModel</code> is run in <code>patch_mode</code> the <code>PatchWrappers</code> at <code>SrcNodes</code> store their outputs in a shared object. And <code>DestNodes</code> compute their patched inputs by multiplying their <code>patch_masks</code> by the difference between the outputs of the incoming <code>SrcNodes</code> on this run, and on the input which is being patched in. This means that activation patching requires two passes. One forward pass computes the output of each <code>SrcNode</code> on the input to be patched in. The second pass adjusts the inputs to each patched <code>DestNode</code> to be the same as the first pass.</p>"},{"location":"reference/data/","title":"Data","text":""},{"location":"reference/data/#auto_circuit.data","title":"auto_circuit.data","text":""},{"location":"reference/data/#auto_circuit.data-attributes","title":"Attributes","text":""},{"location":"reference/data/#auto_circuit.data.BatchKey","title":"BatchKey  <code>module-attribute</code>","text":"<pre><code>BatchKey = int\n</code></pre> <p>A unique key for a <code>PromptPairBatch</code>.</p>"},{"location":"reference/data/#auto_circuit.data-classes","title":"Classes","text":""},{"location":"reference/data/#auto_circuit.data.PromptDataLoader","title":"PromptDataLoader","text":"<pre><code>PromptDataLoader(prompt_dataset: Any, seq_len: Optional[int], diverge_idx: int, kv_cache: Optional[HookedTransformerKeyValueCache] = None, seq_labels: Optional[List[str]] = None, word_idxs: Dict[str, int] = {}, **kwargs: Any)\n</code></pre> <p>             Bases: <code>DataLoader[PromptPairBatch]</code></p> <p>A <code>DataLoader</code> for clean/corrupt prompt pairs with correct/incorrect answers.</p> <p>Parameters:</p> Name Type Description Default <code>prompt_dataset</code> <code>Any</code> <p>A <code>PromptDataset</code> with clean and corrupt prompts.</p> required <code>seq_len</code> <code>Optional[int]</code> <p>The token length of the prompts (if fixed length). This prompt length can be passed to <code>patchable_model</code> to enable patching specific token positions.</p> required <code>diverge_idx</code> <code>int</code> <p>The index at which the clean and corrupt prompts diverge. (See <code>load_datasets_from_json</code> for more information.)</p> required <code>kv_cache</code> <code>Optional[HookedTransformerKeyValueCache]</code> <p>A cache of past key-value pairs for the transformer. Only used if <code>diverge_idx</code> is greater than 0. (See <code>load_datasets_from_json</code> for more information.)</p> <code>None</code> <code>seq_labels</code> <code>Optional[List[str]]</code> <p>A list of strings that label each token for fixed length prompts. Used by <code>draw_seq_graph</code> to label the circuit diagram.</p> <code>None</code> <code>word_idxs</code> <code>Dict[str, int]</code> <p>A dictionary with the token indexes of specific words. Used by official circuit functions.</p> <code>{}</code> <code>kwargs</code> <code>Any</code> <p>Additional arguments to pass to <code>DataLoader</code>.</p> <code>{}</code> Note <p><code>drop_last=True</code> is always passed to the parent <code>DataLoader</code> constructor. So all batches are always the same size. This simplifies the implementation of several functions. For example, the <code>kv_cache</code> only needs caches for a single batch size.</p> Source code in <code>auto_circuit/data.py</code> <pre><code>def __init__(\n    self,\n    prompt_dataset: Any,\n    seq_len: Optional[int],\n    diverge_idx: int,\n    kv_cache: Optional[HookedTransformerKeyValueCache] = None,\n    seq_labels: Optional[List[str]] = None,\n    word_idxs: Dict[str, int] = {},\n    **kwargs: Any,\n):\n    \"\"\"\n    A `DataLoader` for clean/corrupt prompt pairs with correct/incorrect answers.\n\n    Args:\n        prompt_dataset: A [`PromptDataset`][auto_circuit.data.PromptDataset] with\n            clean and corrupt prompts.\n        seq_len: The token length of the prompts (if fixed length). This prompt\n            length can be passed to `patchable_model` to enable patching specific\n            token positions.\n        diverge_idx: The index at which the clean and corrupt prompts diverge. (See\n            [`load_datasets_from_json`][auto_circuit.data.load_datasets_from_json]\n            for more information.)\n        kv_cache: A cache of past key-value pairs for the transformer. Only used if\n            `diverge_idx` is greater than 0. (See\n            [`load_datasets_from_json`][auto_circuit.data.load_datasets_from_json]\n            for more information.)\n        seq_labels: A list of strings that label each token for fixed length\n            prompts. Used by\n            [`draw_seq_graph`][auto_circuit.visualize.draw_seq_graph] to label the\n            circuit diagram.\n        word_idxs: A dictionary with the token indexes of specific words. Used by\n            official circuit functions.\n        kwargs: Additional arguments to pass to `DataLoader`.\n\n    Note:\n        `drop_last=True` is always passed to the parent `DataLoader` constructor. So\n        all batches are always the same size. This simplifies the implementation of\n        several functions. For example, the `kv_cache` only needs caches for a\n        single batch size.\n    \"\"\"\n    super().__init__(\n        prompt_dataset, **kwargs, drop_last=True, collate_fn=collate_fn\n    )\n    self.seq_len = seq_len\n    \"\"\"\n    The token length of the prompts (if fixed length). This prompt length can be\n    passed to `patchable_model` to enable patching specific token positions.\n    \"\"\"\n    self.diverge_idx = diverge_idx\n    \"\"\"\n    The index at which the clean and corrupt prompts diverge. (See\n    [`load_datasets_from_json`][auto_circuit.data.load_datasets_from_json] for more\n    information.)\n    \"\"\"\n    self.seq_labels = seq_labels\n    \"\"\"\n    A list of strings that label each token for fixed length prompts. Used by\n    [`draw_seq_graph`][auto_circuit.visualize.draw_seq_graph] to label the circuit\n    diagram.\n    \"\"\"\n    assert kv_cache is None or diverge_idx &gt; 0\n    self.kv_cache = kv_cache\n    \"\"\"\n    A cache of past key-value pairs for the transformer. Only used if `diverge_idx`\n    is greater than 0. (See\n    [`load_datasets_from_json`][auto_circuit.data.load_datasets_from_json] for more\n    information.)\n    \"\"\"\n    self.word_idxs = word_idxs\n    \"\"\"\n    A dictionary with the token indexes of specific words. Used by official circuit\n    functions.\n    \"\"\"\n</code></pre>"},{"location":"reference/data/#auto_circuit.data.PromptDataLoader-attributes","title":"Attributes","text":""},{"location":"reference/data/#auto_circuit.data.PromptDataLoader.diverge_idx","title":"diverge_idx  <code>instance-attribute</code>","text":"<pre><code>diverge_idx = diverge_idx\n</code></pre> <p>The index at which the clean and corrupt prompts diverge. (See <code>load_datasets_from_json</code> for more information.)</p>"},{"location":"reference/data/#auto_circuit.data.PromptDataLoader.kv_cache","title":"kv_cache  <code>instance-attribute</code>","text":"<pre><code>kv_cache = kv_cache\n</code></pre> <p>A cache of past key-value pairs for the transformer. Only used if <code>diverge_idx</code> is greater than 0. (See <code>load_datasets_from_json</code> for more information.)</p>"},{"location":"reference/data/#auto_circuit.data.PromptDataLoader.seq_labels","title":"seq_labels  <code>instance-attribute</code>","text":"<pre><code>seq_labels = seq_labels\n</code></pre> <p>A list of strings that label each token for fixed length prompts. Used by <code>draw_seq_graph</code> to label the circuit diagram.</p>"},{"location":"reference/data/#auto_circuit.data.PromptDataLoader.seq_len","title":"seq_len  <code>instance-attribute</code>","text":"<pre><code>seq_len = seq_len\n</code></pre> <p>The token length of the prompts (if fixed length). This prompt length can be passed to <code>patchable_model</code> to enable patching specific token positions.</p>"},{"location":"reference/data/#auto_circuit.data.PromptDataLoader.word_idxs","title":"word_idxs  <code>instance-attribute</code>","text":"<pre><code>word_idxs = word_idxs\n</code></pre> <p>A dictionary with the token indexes of specific words. Used by official circuit functions.</p>"},{"location":"reference/data/#auto_circuit.data.PromptDataLoader-functions","title":"Functions","text":""},{"location":"reference/data/#auto_circuit.data.PromptDataset","title":"PromptDataset","text":"<pre><code>PromptDataset(clean_prompts: List[Tensor] | Tensor, corrupt_prompts: List[Tensor] | Tensor, answers: List[Tensor], wrong_answers: List[Tensor])\n</code></pre> <p>             Bases: <code>Dataset</code></p> <p>A dataset of clean/corrupt prompt pairs with correct/incorrect answers.</p> <p>Parameters:</p> Name Type Description Default <code>clean_prompts</code> <code>List[Tensor] | Tensor</code> <p>The 'clean' prompts. These are typically examples of the behavior we want to isolate where the model performs well. If a list, each element is a 1D prompt tensor. If a tensor, it should be 2D with shape (n_prompts, prompt_length).</p> required <code>corrupt_prompts</code> <code>List[Tensor] | Tensor</code> <p>The 'corrupt' prompts. These are typically similar to the 'clean' prompts, but with some crucial difference that changes the model output. If a list, each element is a 1D prompt tensor. If a tensor, it should be 2D with shape (n_prompts, prompt_length).</p> required <code>answers</code> <code>List[Tensor]</code> <p>A list of correct answers. Each element is a 1D tensor with the answer tokens.</p> required <code>wrong_answers</code> <code>List[Tensor]</code> <p>A list of incorrect answers. Each element is a 1D tensor with the wrong answer tokens.</p> required Source code in <code>auto_circuit/data.py</code> <pre><code>def __init__(\n    self,\n    clean_prompts: List[t.Tensor] | t.Tensor,\n    corrupt_prompts: List[t.Tensor] | t.Tensor,\n    answers: List[t.Tensor],\n    wrong_answers: List[t.Tensor],\n):\n    \"\"\"\n    A dataset of clean/corrupt prompt pairs with correct/incorrect answers.\n\n    Args:\n        clean_prompts: The 'clean' prompts. These are typically examples of the\n            behavior we want to isolate where the model performs well.\n            If a list, each element is a 1D prompt tensor.\n            If a tensor, it should be 2D with shape (n_prompts, prompt_length).\n        corrupt_prompts: The 'corrupt' prompts. These are typically similar to the\n            'clean' prompts, but with some crucial difference that changes the model\n            output.\n            If a list, each element is a 1D prompt tensor.\n            If a tensor, it should be 2D with shape (n_prompts, prompt_length).\n        answers: A list of correct answers.\n            Each element is a 1D tensor with the answer tokens.\n        wrong_answers: A list of incorrect answers.\n            Each element is a 1D tensor with the wrong answer tokens.\n    \"\"\"\n\n    self.clean_prompts = clean_prompts\n    self.corrupt_prompts = corrupt_prompts\n    self.answers = answers\n    self.wrong_answers = wrong_answers\n</code></pre>"},{"location":"reference/data/#auto_circuit.data.PromptDataset-functions","title":"Functions","text":""},{"location":"reference/data/#auto_circuit.data.PromptPair","title":"PromptPair","text":"<p>A pair of clean and corrupt prompts with correct and incorrect answers.</p> <p>Parameters:</p> Name Type Description Default <code>clean</code> <p>The 'clean' prompt. This is typically an example of the behavior we want to isolate where the model performs well.</p> required <code>corrupt</code> <p>The 'corrupt' prompt. This is typically similar to the 'clean' prompt, but with some crucial difference that changes the model output.</p> required <code>answers</code> <p>The correct completions for the clean prompt.</p> required <code>wrong_answers</code> <p>The incorrect completions for the clean prompt.</p> required"},{"location":"reference/data/#auto_circuit.data.PromptPairBatch","title":"PromptPairBatch","text":"<p>A batch of prompt pairs.</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <p>A unique integer that identifies the batch.</p> required <code>batch_diverge_idx</code> <p>The minimum index over all prompts at which the clean and corrupt prompts diverge. This is used to automatically cache the key-value activations for the common prefix of the prompts. See <code>load_datasets_from_json</code> for more information.</p> required <code>clean</code> <p>The 'clean' prompts in a 2D tensor. These are typically examples of the behavior we want to isolate where the model performs well.</p> required <code>corrupt</code> <p>The 'corrupt' prompts in a 2D tensor. These are typically similar to the 'clean' prompts, but with some crucial difference that changes the model output.</p> required <code>answers</code> <p>The correct answers completions for the clean prompts. If all prompts have the same number of answers, this is a 2D tensor. If each prompt has a different number of answers, this is a list of 1D tensors. This can make some methods such as <code>batch_answer_diffs</code> much slower.</p> required <code>wrong_answers</code> <p>The incorrect answers. If each prompt has a different number of wrong answers, this is a list of tensors. If all prompts have the same number of wrong answers, this is a 2D tensor. If each prompt has a different number of wrong answers, this is a list of 1D tensors. This can make some methods such as <code>batch_answer_diffs</code> much slower.</p> required"},{"location":"reference/data/#auto_circuit.data-functions","title":"Functions","text":""},{"location":"reference/data/#auto_circuit.data.load_datasets_from_json","title":"load_datasets_from_json","text":"<pre><code>load_datasets_from_json(model: Optional[Module], path: Path | List[Path], device: device, prepend_bos: bool = True, batch_size: int | Tuple[int, int] = 32, train_test_size: Tuple[int, int] = (128, 128), return_seq_length: bool = False, tail_divergence: bool = False, shuffle: bool = True, random_seed: int = 42, pad: bool = True) -&gt; Tuple[PromptDataLoader, PromptDataLoader]\n</code></pre> <p>Load a dataset from a json file. The file should specify a list of dictionaries with keys \"clean_prompt\" and \"corrupt_prompt\".</p> <p>JSON data format: <pre><code>{\n    // Optional: used to label circuit visualization\n    \"seq_labels\": [str, ...],\n\n    // Optional: used by official circuit functions\n    \"word_idxs\": {\n        str: int,\n        ...\n    },\n\n    // Required: the prompt pairs\n    \"prompts\": [\n        {\n            \"clean\": str | [[int, ...], ...],\n            \"corrupt\": str | [[int, ...], ...],\n            \"answers\": [str, ...] | [int, ...],\n            \"wrong_answers\": [str, ...] | [int, ...],\n        },\n        ...\n    ]\n}\n</code></pre></p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>Optional[Module]</code> <p>Model to use for tokenization. If None, data must be pre-tokenized (<code>\"prompts\"</code> is passed as <code>int</code>s).</p> required <code>path</code> <code>Path | List[Path]</code> <p>Path to the json file with the dataset. If a list of paths is passed, the first dataset is parsed in full and for the rest are the <code>prompts</code> are used.</p> required <code>device</code> <code>device</code> <p>Device to load the data on.</p> required <code>prepend_bos</code> <code>bool</code> <p>If True, prepend the <code>BOS</code> token to each prompt. (The <code>prepend_bos</code> flag on TransformerLens <code>HookedTransformer</code>s is ignored.)</p> <code>True</code> <code>batch_size</code> <code>int | Tuple[int, int]</code> <p>The batch size for training and testing. If a single int is passed, the same batch size is used for both.</p> <code>32</code> <code>return_seq_length</code> <code>bool</code> <p>If <code>True</code>, return the sequence length of the prompts. Note: If <code>True</code>, all the prompts must have the same length or an error will be raised. This is used by <code>patchable_model</code> to enable patching specific token positions.</p> <code>False</code> <code>tail_divergence</code> <code>bool</code> <p>If all prompts share a common prefix, remove it and compute the keys and values for each attention head on the prefix. A <code>kv_cache</code> for the prefix is returned in the <code>train_loader</code> and <code>test_loader</code>.</p> <code>False</code> <code>shuffle</code> <code>bool</code> <p>If <code>True</code>, shuffle the dataset before splitting into train and test sets.</p> <code>True</code> <code>random_seed</code> <code>int</code> <p>Seed for the random number generator.</p> <code>42</code> <code>pad</code> <code>bool</code> <p>If <code>True</code>, pad the prompts to the maximum length in the batch. Do not use in conjunction with <code>return_seq_length</code>.</p> <code>True</code> Note <p><code>shuffle</code> only shuffles the order of the prompts once at the beginning. The order is preserved in the train and test loaders (<code>shuffle=False</code> is always passed to the <code>PromptDataLoader</code> constructor). This makes it easier to ensure experiments are deterministic.</p> Source code in <code>auto_circuit/data.py</code> <pre><code>def load_datasets_from_json(\n    model: Optional[t.nn.Module],\n    path: Path | List[Path],\n    device: t.device,\n    prepend_bos: bool = True,\n    batch_size: int | Tuple[int, int] = 32,  # (train, test) if tuple\n    train_test_size: Tuple[int, int] = (128, 128),\n    return_seq_length: bool = False,\n    tail_divergence: bool = False,  # Remove all tokens before divergence\n    shuffle: bool = True,\n    random_seed: int = 42,\n    pad: bool = True,\n) -&gt; Tuple[PromptDataLoader, PromptDataLoader]:\n    \"\"\"\n    Load a dataset from a json file. The file should specify a list of\n    dictionaries with keys \"clean_prompt\" and \"corrupt_prompt\".\n\n    JSON data format:\n    ```\n    {\n        // Optional: used to label circuit visualization\n        \"seq_labels\": [str, ...],\n\n        // Optional: used by official circuit functions\n        \"word_idxs\": {\n            str: int,\n            ...\n        },\n\n        // Required: the prompt pairs\n        \"prompts\": [\n            {\n                \"clean\": str | [[int, ...], ...],\n                \"corrupt\": str | [[int, ...], ...],\n                \"answers\": [str, ...] | [int, ...],\n                \"wrong_answers\": [str, ...] | [int, ...],\n            },\n            ...\n        ]\n    }\n    ```\n\n    Args:\n        model: Model to use for tokenization. If None, data must be pre-tokenized\n            (`\"prompts\"` is passed as `int`s).\n        path: Path to the json file with the dataset. If a list of paths is passed, the\n            first dataset is parsed in full and for the rest are the `prompts` are used.\n        device: Device to load the data on.\n        prepend_bos: If True, prepend the `BOS` token to each prompt. (The `prepend_bos`\n            flag on TransformerLens `HookedTransformer`s is ignored.)\n        batch_size: The batch size for training and testing. If a single int is passed,\n            the same batch size is used for both.\n        return_seq_length: If `True`, return the sequence length of the prompts. **Note:\n            If `True`, all the prompts must have the same length or an error will be\n            raised.** This is used by\n            [`patchable_model`][auto_circuit.utils.graph_utils.patchable_model] to\n            enable patching specific token positions.\n        tail_divergence: If all prompts share a common prefix, remove it and compute the\n            keys and values for each attention head on the prefix. A `kv_cache` for the\n            prefix is returned in the `train_loader` and `test_loader`.\n        shuffle: If `True`, shuffle the dataset before splitting into train and test\n            sets.\n        random_seed: Seed for the random number generator.\n        pad: If `True`, pad the prompts to the maximum length in the batch. Do not use\n            in conjunction with `return_seq_length`.\n\n    Note:\n        `shuffle` only shuffles the order of the prompts once at the beginning. The\n        order is preserved in the train and test loaders (`shuffle=False` is always\n        passed to the [`PromptDataLoader`][auto_circuit.data.PromptDataLoader]\n        constructor). This makes it easier to ensure experiments are deterministic.\n    \"\"\"\n    assert not (prepend_bos and (model is None)), \"Need model tokenizer to prepend bos\"\n\n    # Load a dataset. If path is a list, only the first dataset is fully loaded.\n    first_path = path if isinstance(path, Path) else path[0]\n    assert isinstance(first_path, Path)\n    with open(first_path, \"r\") as f:\n        data = json.load(f)\n    # For other paths, only 'prompts' are added to dataset. (eg. seq_labels is ignored)\n    if isinstance(path, list):\n        assert all([isinstance(p, Path) for p in path])\n        for p in path[1:]:\n            with open(p, \"r\") as f:\n                d = json.load(f)\n                data[\"prompts\"].extend(d[\"prompts\"])\n\n    # Shuffle data and split into train and test\n    random.seed(random_seed)\n    t.random.manual_seed(random_seed)\n    random.shuffle(data[\"prompts\"]) if shuffle else None\n    n_train_and_test = sum(train_test_size)\n    clean_prompts = [d[\"clean\"] for d in data[\"prompts\"]][:n_train_and_test]\n    corrupt_prompts = [d[\"corrupt\"] for d in data[\"prompts\"]][:n_train_and_test]\n    answer_strs = [d[\"answers\"] for d in data[\"prompts\"]][:n_train_and_test]\n    wrong_answer_strs = [d[\"wrong_answers\"] for d in data[\"prompts\"]][:n_train_and_test]\n    seq_labels = data.get(\"seq_labels\", None)\n    word_idxs = data.get(\"word_idxs\", {})\n\n    if prepend_bos:\n        # Adjust word_idxs and seq_labels if prepending bos\n        seq_labels = [\"&lt;|BOS|&gt;\"] + seq_labels if seq_labels is not None else None\n        word_idxs = {k: v + int(prepend_bos) for k, v in word_idxs.items()}\n\n    kvs = []\n    diverge_idx: int = 0\n    if model is None:\n        clean_prompts = [t.tensor(p).to(device) for p in clean_prompts]\n        corrupt_prompts = [t.tensor(p).to(device) for p in corrupt_prompts]\n        answers = [t.tensor(a).to(device) for a in answer_strs]\n        wrong_answers = [t.tensor(a).to(device) for a in wrong_answer_strs]\n        seq_len = clean_prompts[0].shape[0]\n        assert not tail_divergence\n    else:\n        tokenizer: Any = model.tokenizer\n        if prepend_bos:\n            clean_prompts = [tokenizer.bos_token + p for p in clean_prompts]\n            corrupt_prompts = [tokenizer.bos_token + p for p in corrupt_prompts]\n        tokenizer.padding_side = \"left\"\n        clean_prompts = tokenizer(clean_prompts, padding=pad, return_tensors=\"pt\")\n        corrupt_prompts = tokenizer(corrupt_prompts, padding=pad, return_tensors=\"pt\")\n        seq_len = None\n        if return_seq_length:\n            assert t.all(clean_prompts[\"attention_mask\"] == 1)\n            assert t.all(corrupt_prompts[\"attention_mask\"] == 1)\n            seq_len = clean_prompts[\"input_ids\"].shape[1]\n        ans_dicts: List[Dict] = [tokenizer(a, return_tensors=\"pt\") for a in answer_strs]\n        wrong_ans_dicts: List[Dict] = [\n            tokenizer(a, return_tensors=\"pt\") for a in wrong_answer_strs\n        ]\n        clean_prompts = clean_prompts[\"input_ids\"].to(device)\n        corrupt_prompts = corrupt_prompts[\"input_ids\"].to(device)\n        answers = [a[\"input_ids\"].squeeze(-1).to(device) for a in ans_dicts]\n        wrong_answers = [a[\"input_ids\"].squeeze(-1).to(device) for a in wrong_ans_dicts]\n\n        if tail_divergence:\n            diverge_idxs = (~(clean_prompts == corrupt_prompts)).int().argmax(dim=1)\n            diverge_idx = int(diverge_idxs.min().item())\n        if diverge_idx &gt; 0:\n            seq_labels = seq_labels[diverge_idx:] if seq_labels is not None else None\n            prefixs, cfg, device = [], model.cfg, model.cfg.device\n            if isinstance(batch_size, tuple):\n                prefixs.append(clean_prompts[: (bs0 := batch_size[0]), :diverge_idx])\n                prefixs.append(clean_prompts[: (bs1 := batch_size[1]), :diverge_idx])\n                kvs.append(HookedTransformerKeyValueCache.init_cache(cfg, device, bs0))\n                kvs.append(HookedTransformerKeyValueCache.init_cache(cfg, device, bs1))\n            else:\n                prefixs.append(clean_prompts[:batch_size, :diverge_idx])\n                kvs.append(\n                    HookedTransformerKeyValueCache.init_cache(cfg, device, batch_size)\n                )\n\n            for prefix, kv_cache in zip(prefixs, kvs):\n                with t.inference_mode():\n                    model(prefix, past_kv_cache=kv_cache)\n                kv_cache.freeze()\n\n            print(\"seq_len before divergence\", seq_len)\n            if return_seq_length:\n                assert seq_len is not None\n                seq_len -= diverge_idx\n            print(\"seq_len after divergence\", seq_len)\n\n            # This must be done AFTER gathering the kv caches\n            clean_prompts = clean_prompts[:, diverge_idx:]\n            corrupt_prompts = corrupt_prompts[:, diverge_idx:]\n\n    dataset = PromptDataset(clean_prompts, corrupt_prompts, answers, wrong_answers)\n    train_set = Subset(dataset, list(range(train_test_size[0])))\n    test_set = Subset(dataset, list(range(train_test_size[0], n_train_and_test)))\n    train_loader = PromptDataLoader(\n        train_set,\n        seq_len=seq_len,\n        diverge_idx=diverge_idx,\n        kv_cache=kvs[0] if len(kvs) &gt; 0 else None,\n        seq_labels=seq_labels,\n        word_idxs=word_idxs,\n        batch_size=batch_size[0] if isinstance(batch_size, tuple) else batch_size,\n        shuffle=False,\n    )\n    test_loader = PromptDataLoader(\n        test_set,\n        seq_len=seq_len,\n        diverge_idx=diverge_idx,\n        kv_cache=kvs[-1] if len(kvs) &gt; 0 else None,\n        seq_labels=seq_labels,\n        word_idxs=word_idxs,\n        batch_size=batch_size[1] if isinstance(batch_size, tuple) else batch_size,\n        shuffle=False,\n    )\n    return train_loader, test_loader\n</code></pre>"},{"location":"reference/experiment_utils/","title":"Experiment utils","text":""},{"location":"reference/experiment_utils/#auto_circuit.experiment_utils","title":"auto_circuit.experiment_utils","text":""},{"location":"reference/experiment_utils/#auto_circuit.experiment_utils-attributes","title":"Attributes","text":""},{"location":"reference/experiment_utils/#auto_circuit.experiment_utils-classes","title":"Classes","text":""},{"location":"reference/experiment_utils/#auto_circuit.experiment_utils.IOI_CIRCUIT_TYPE","title":"IOI_CIRCUIT_TYPE","text":"<p>             Bases: <code>Enum</code></p> <p>Type of IOI circuit. The original IOI paper discovered important attention heads and interactions between them.</p>"},{"location":"reference/experiment_utils/#auto_circuit.experiment_utils.IOI_CIRCUIT_TYPE-attributes","title":"Attributes","text":""},{"location":"reference/experiment_utils/#auto_circuit.experiment_utils.IOI_CIRCUIT_TYPE.EDGES","title":"EDGES  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>EDGES = 2\n</code></pre> <p><code>EDGES</code> ablates the edges identified by the IOI paper. Note that the IOI paper considered intermediate MLPs to be part of the direct path between two attention heads, so this includes many edges to or from MLPs.</p>"},{"location":"reference/experiment_utils/#auto_circuit.experiment_utils.IOI_CIRCUIT_TYPE.EDGES_MLP_0_ONLY","title":"EDGES_MLP_0_ONLY  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>EDGES_MLP_0_ONLY = 3\n</code></pre> <p>Therefore we also provide <code>EDGES_MLP_0_ONLY</code> which includes only the first MLP layer (as this seems to retain most of the performance of the full <code>EDGES</code> circuit).</p>"},{"location":"reference/experiment_utils/#auto_circuit.experiment_utils.IOI_CIRCUIT_TYPE.NODES","title":"NODES  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>NODES = 1\n</code></pre> <p><code>NODES</code> ablates the outputs of the head in the circuit.</p>"},{"location":"reference/experiment_utils/#auto_circuit.experiment_utils-functions","title":"Functions","text":""},{"location":"reference/experiment_utils/#auto_circuit.experiment_utils.ioi_circuit_single_template_logit_diff_percent","title":"ioi_circuit_single_template_logit_diff_percent","text":"<pre><code>ioi_circuit_single_template_logit_diff_percent(gpt2: HookedTransformer, dataset_size: int, prepend_bos: bool, template: Literal['ABBA', 'BABA'], template_idx: int, factorized: bool = False, circuit: IOI_CIRCUIT_TYPE = IOI_CIRCUIT_TYPE.NODES, ablation_type: AblationType = AblationType.TOKENWISE_MEAN_CORRUPT, tok_pos: bool = True, patch_type: PatchType = PatchType.TREE_PATCH, learned: bool = False, learned_faithfulness_target: SP_FAITHFULNESS_TARGET = 'logit_diff_percent', diff_of_mean_logit_diff: bool = False, batch_size: Optional[int] = None) -&gt; Tuple[int, float, float, Tensor, PruneScores]\n</code></pre> <p>Run a single template format through the IOI circuit and return the logit diff recovered.</p> <p>Parameters:</p> Name Type Description Default <code>gpt2</code> <code>HookedTransformer</code> <p>A GPT2 <code>HookedTransformer</code>.</p> required <code>dataset_size</code> <code>int</code> <p>The size of the dataset to use.</p> required <code>prepend_bos</code> <code>bool</code> <p>Whether to prepend the <code>BOS</code> token to the prompts.</p> required <code>template</code> <code>Literal['ABBA', 'BABA']</code> <p>The type of template to use. (This is the order of names).</p> required <code>template_idx</code> <code>int</code> <p>The index of the template to use (<code>0</code> to <code>14</code>).</p> required <code>factorized</code> <code>bool</code> <p>Use a 'factorized' model (Edge Patching, not Node Patching).</p> <code>False</code> <code>circuit</code> <code>IOI_CIRCUIT_TYPE</code> <p>The type of circuit to use (see <code>IOI_CIRCUIT_TYPE</code>).</p> <code>NODES</code> <code>ablation_type</code> <code>AblationType</code> <p>The type of ablation to use.</p> <code>TOKENWISE_MEAN_CORRUPT</code> <code>tok_pos</code> <code>bool</code> <p>Whether to ablate different token positions separately.</p> <code>True</code> <code>patch_type</code> <code>PatchType</code> <p>The type of patch to use (ablate the circuit or the complement).</p> <code>TREE_PATCH</code> <code>learned</code> <code>bool</code> <p>Whether to learn a new circuit using <code>Subnetwork Probing</code> (in this case <code>IOI_CIRCUIT_TYPE</code> is only used to determine the number of edges in the learned circuit).</p> <code>False</code> <code>learned_faithfulness_target</code> <code>SP_FAITHFULNESS_TARGET</code> <p>The faithfulness target used to learn the circuit.</p> <code>'logit_diff_percent'</code> <code>learned_faithfulness_target</code> <code>SP_FAITHFULNESS_TARGET</code> <p>The faithfulness metric to optimize the learned circuit for.</p> <code>'logit_diff_percent'</code> <code>diff_of_mean_logit_diff</code> <code>bool</code> <p>If <code>true</code> we compute: <pre><code>(mean(circuit) / mean(model)) * 100\n</code></pre> like the IOI paper. If <code>false</code> we compute: <pre><code>(mean(circuit / model)) * 100\n</code></pre></p> <code>False</code> <code>batch_size</code> <code>Optional[int]</code> <p>The batch size to use.</p> <code>None</code> <p>Returns:</p> Type Description <code>Tuple[int, float, float, Tensor, PruneScores]</code> <p>The number of edges in the circuit, the mean logit diff percent, the standard deviation of the logit diff percent, and the prune scores of the circuit.</p> Source code in <code>auto_circuit/experiment_utils.py</code> <pre><code>def ioi_circuit_single_template_logit_diff_percent(\n    gpt2: tl.HookedTransformer,\n    dataset_size: int,\n    prepend_bos: bool,\n    template: Literal[\"ABBA\", \"BABA\"],\n    template_idx: int,\n    factorized: bool = False,\n    circuit: IOI_CIRCUIT_TYPE = IOI_CIRCUIT_TYPE.NODES,\n    ablation_type: AblationType = AblationType.TOKENWISE_MEAN_CORRUPT,\n    tok_pos: bool = True,\n    patch_type: PatchType = PatchType.TREE_PATCH,\n    learned: bool = False,\n    learned_faithfulness_target: SP_FAITHFULNESS_TARGET = \"logit_diff_percent\",\n    diff_of_mean_logit_diff: bool = False,\n    batch_size: Optional[int] = None,\n) -&gt; Tuple[int, float, float, t.Tensor, PruneScores]:\n    \"\"\"\n    Run a single template format through the IOI circuit and return the logit diff\n    recovered.\n\n    Args:\n        gpt2: A GPT2 `HookedTransformer`.\n        dataset_size: The size of the dataset to use.\n        prepend_bos: Whether to prepend the `BOS` token to the prompts.\n        template: The type of template to use. (This is the order of names).\n        template_idx: The index of the template to use (`0` to `14`).\n        factorized: Use a 'factorized' model (Edge Patching, not Node Patching).\n        circuit: The type of circuit to use (see `IOI_CIRCUIT_TYPE`).\n        ablation_type: The type of ablation to use.\n        tok_pos: Whether to ablate different token positions separately.\n        patch_type: The type of patch to use (ablate the circuit or the complement).\n        learned: Whether to learn a new circuit using `Subnetwork Probing` (in this case\n            `IOI_CIRCUIT_TYPE` is only used to determine the number of edges in the\n            learned circuit).\n        learned_faithfulness_target: The faithfulness target used to learn the circuit.\n        learned_faithfulness_target: The faithfulness metric to optimize the learned\n            circuit for.\n        diff_of_mean_logit_diff: If `true` we compute:\n            ```\n            (mean(circuit) / mean(model)) * 100\n            ```\n            like the IOI paper. If `false` we compute:\n            ```\n            (mean(circuit / model)) * 100\n            ```\n        batch_size: The batch size to use.\n\n    Returns:\n        The number of edges in the circuit, the mean logit diff percent, the standard\n            deviation of the logit diff percent, and the prune scores of the circuit.\n    \"\"\"\n    assert gpt2.cfg.model_name == \"gpt2\"\n    assert gpt2.cfg.device is not None\n    if type(circuit) == str and \"Edges\" in circuit:\n        assert factorized\n    if batch_size is None:\n        batch_size = dataset_size\n\n    path = repo_path_to_abs_path(\n        f\"datasets/ioi/ioi_{template}_template_{template_idx}_prompts.json\"\n    )\n    patchable_gpt2 = deepcopy(gpt2)\n    train_loader, test_loader = load_datasets_from_json(\n        model=patchable_gpt2,\n        path=path,\n        device=t.device(gpt2.cfg.device),\n        prepend_bos=prepend_bos,\n        batch_size=batch_size,\n        train_test_size=(8 * dataset_size, dataset_size)\n        if learned\n        else (0, dataset_size),\n        shuffle=False,\n        return_seq_length=tok_pos,\n        tail_divergence=False,\n    )\n\n    patchable_gpt2 = patchable_model(\n        model=patchable_gpt2,\n        factorized=factorized,\n        slice_output=\"last_seq\",\n        seq_len=test_loader.seq_len if tok_pos else None,\n        separate_qkv=True,\n        device=t.device(gpt2.cfg.device),\n    )\n\n    assert test_loader.word_idxs is not None\n    if circuit == IOI_CIRCUIT_TYPE.EDGES:\n        official_circ = ioi_true_edges\n    elif circuit == IOI_CIRCUIT_TYPE.EDGES_MLP_0_ONLY:\n        official_circ = ioi_true_edges_mlp_0_only\n    else:\n        assert circuit == IOI_CIRCUIT_TYPE.NODES\n        official_circ = ioi_head_based_official_edges\n\n    ioi_official_edges = official_circ(\n        patchable_gpt2,\n        word_idxs=test_loader.word_idxs,\n        token_positions=tok_pos,\n        seq_start_idx=test_loader.diverge_idx,\n    )\n    test_edge_counts = len(ioi_official_edges)\n\n    if learned:\n        circuit_ps: PruneScores = circuit_probing_prune_scores(\n            model=patchable_gpt2,\n            dataloader=train_loader,\n            official_edges=ioi_official_edges,\n            epochs=200,\n            learning_rate=0.1,\n            regularize_lambda=0.1,\n            mask_fn=\"hard_concrete\",\n            show_train_graph=True,\n            tree_optimisation=True,\n            circuit_sizes=[\"true_size\"],\n            faithfulness_target=learned_faithfulness_target,\n            validation_dataloader=test_loader,\n        )\n    else:\n        circuit_ps = patchable_gpt2.circuit_prune_scores(ioi_official_edges)\n\n    circ_outs: CircuitOutputs = run_circuits(\n        model=patchable_gpt2,\n        dataloader=test_loader,\n        test_edge_counts=[test_edge_counts],\n        prune_scores=circuit_ps,\n        patch_type=patch_type,\n        ablation_type=ablation_type,\n        render_graph=False,\n    )\n    (\n        logit_diff_percent_mean,\n        logit_diff_percent_std,\n        logit_diff_percents,\n    ) = answer_diff_percent(\n        patchable_gpt2,\n        test_loader,\n        circ_outs,\n        prob_func=\"logits\",\n        diff_of_means=diff_of_mean_logit_diff,\n    )\n\n    assert len(logit_diff_percent_mean) == 1 and len(logit_diff_percent_std) == 1\n    assert type(logit_diff_percent_mean[0][0]) == int\n    assert type(logit_diff_percent_mean[0][1]) == float\n    assert type(logit_diff_percent_std[0][0]) == int\n    assert type(logit_diff_percent_std[0][1]) == float\n    assert type(logit_diff_percents[0][0]) == int\n    assert type(logit_diff_percents[0][1]) == t.Tensor\n    assert (\n        logit_diff_percent_mean[0][0]\n        == logit_diff_percent_std[0][0]\n        == logit_diff_percents[0][0]\n    )\n\n    del patchable_gpt2\n    return (\n        logit_diff_percent_mean[0][0],\n        logit_diff_percent_mean[0][1],\n        logit_diff_percent_std[0][1],\n        logit_diff_percents[0][1],\n        circuit_ps,\n    )\n</code></pre>"},{"location":"reference/experiment_utils/#auto_circuit.experiment_utils.load_tl_model","title":"load_tl_model","text":"<pre><code>load_tl_model(name: str, device: device) -&gt; HookedTransformer\n</code></pre> <p>Load a <code>HookedTransformer</code> model with the necessary config to perform edge patching (with separate edges to Q, K, and V). Sets <code>requires_grad</code> to <code>False</code> for all model weights (this does not affect Mask gradients).</p> Source code in <code>auto_circuit/experiment_utils.py</code> <pre><code>def load_tl_model(name: str, device: t.device) -&gt; tl.HookedTransformer:\n    \"\"\"\n    Load a `HookedTransformer` model with the necessary config to perform edge patching\n    (with separate edges to Q, K, and V). Sets `requires_grad` to `False` for all model\n    weights (this does not affect Mask gradients).\n    \"\"\"\n    tl_model = tl.HookedTransformer.from_pretrained(\n        name,\n        device=device,\n        fold_ln=True,\n        center_writing_weights=True,\n        center_unembed=True,\n    )\n    tl_model.cfg.use_attn_result = True\n    tl_model.cfg.use_attn_in = True\n    tl_model.cfg.use_split_qkv_input = True\n    tl_model.cfg.use_hook_mlp_in = True\n    tl_model.eval()\n    for param in tl_model.parameters():\n        param.requires_grad = False\n    return tl_model\n</code></pre>"},{"location":"reference/prune/","title":"Prune","text":""},{"location":"reference/prune/#auto_circuit.prune","title":"auto_circuit.prune","text":""},{"location":"reference/prune/#auto_circuit.prune-attributes","title":"Attributes","text":""},{"location":"reference/prune/#auto_circuit.prune-classes","title":"Classes","text":""},{"location":"reference/prune/#auto_circuit.prune-functions","title":"Functions","text":""},{"location":"reference/prune/#auto_circuit.prune.run_circuits","title":"run_circuits","text":"<pre><code>run_circuits(model: PatchableModel, dataloader: PromptDataLoader, test_edge_counts: List[int], prune_scores: PruneScores, patch_type: PatchType = PatchType.EDGE_PATCH, ablation_type: AblationType = AblationType.RESAMPLE, reverse_clean_corrupt: bool = False, render_graph: bool = False, render_score_threshold: bool = False, render_file_path: Optional[str] = None) -&gt; CircuitOutputs\n</code></pre> <p>Run the model, pruning edges based on the given <code>prune_scores</code>. Runs the model over the given <code>dataloader</code> for each <code>test_edge_count</code>.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>PatchableModel</code> <p>The model to run</p> required <code>dataloader</code> <code>PromptDataLoader</code> <p>The dataloader to use for input and patches</p> required <code>test_edge_counts</code> <code>List[int]</code> <p>The numbers of edges to prune.</p> required <code>prune_scores</code> <code>PruneScores</code> <p>The scores that determine the ordering of edges for pruning</p> required <code>patch_type</code> <code>PatchType</code> <p>Whether to patch the circuit or the complement.</p> <code>EDGE_PATCH</code> <code>ablation_type</code> <code>AblationType</code> <p>The type of ablation to use.</p> <code>RESAMPLE</code> <code>reverse_clean_corrupt</code> <code>bool</code> <p>Reverse clean and corrupt (for input and patches).</p> <code>False</code> <code>render_graph</code> <code>bool</code> <p>Whether to render the graph using <code>draw_seq_graph</code>.</p> <code>False</code> <code>render_score_threshold</code> <code>bool</code> <p>Edge score threshold, if <code>render_graph</code> is <code>True</code>.</p> <code>False</code> <code>render_file_path</code> <code>Optional[str]</code> <p>Path to save the rendered graph, if <code>render_graph</code> is <code>True</code>.</p> <code>None</code> <p>Returns:</p> Type Description <code>CircuitOutputs</code> <p>A dictionary mapping from the number of pruned edges to a <code>BatchOutputs</code> object, which is a dictionary mapping from <code>BatchKey</code>s to output tensors.</p> Source code in <code>auto_circuit/prune.py</code> <pre><code>def run_circuits(\n    model: PatchableModel,\n    dataloader: PromptDataLoader,\n    test_edge_counts: List[int],\n    prune_scores: PruneScores,\n    patch_type: PatchType = PatchType.EDGE_PATCH,\n    ablation_type: AblationType = AblationType.RESAMPLE,\n    reverse_clean_corrupt: bool = False,\n    render_graph: bool = False,\n    render_score_threshold: bool = False,\n    render_file_path: Optional[str] = None,\n) -&gt; CircuitOutputs:\n    \"\"\"Run the model, pruning edges based on the given `prune_scores`. Runs the model\n    over the given `dataloader` for each `test_edge_count`.\n\n    Args:\n        model: The model to run\n        dataloader: The dataloader to use for input and patches\n        test_edge_counts: The numbers of edges to prune.\n        prune_scores: The scores that determine the ordering of edges for pruning\n        patch_type: Whether to patch the circuit or the complement.\n        ablation_type: The type of ablation to use.\n        reverse_clean_corrupt: Reverse clean and corrupt (for input and patches).\n        render_graph: Whether to render the graph using `draw_seq_graph`.\n        render_score_threshold: Edge score threshold, if `render_graph` is `True`.\n        render_file_path: Path to save the rendered graph, if `render_graph` is `True`.\n\n    Returns:\n        A dictionary mapping from the number of pruned edges to a\n            [`BatchOutputs`][auto_circuit.types.BatchOutputs] object, which is a\n            dictionary mapping from [`BatchKey`s][auto_circuit.types.BatchKey] to output\n            tensors.\n    \"\"\"\n    circ_outs: CircuitOutputs = defaultdict(dict)\n    desc_ps: t.Tensor = desc_prune_scores(prune_scores)\n\n    patch_src_outs: Optional[t.Tensor] = None\n    if ablation_type.mean_over_dataset:\n        patch_src_outs = src_ablations(model, dataloader, ablation_type)\n\n    for batch_idx, batch in enumerate(batch_pbar := tqdm(dataloader)):\n        batch_pbar.set_description_str(f\"Pruning Batch {batch_idx}\", refresh=True)\n        if (patch_type == PatchType.TREE_PATCH and not reverse_clean_corrupt) or (\n            patch_type == PatchType.EDGE_PATCH and reverse_clean_corrupt\n        ):\n            batch_input = batch.clean\n            if not ablation_type.mean_over_dataset:\n                patch_src_outs = src_ablations(model, batch.corrupt, ablation_type)\n        elif (patch_type == PatchType.EDGE_PATCH and not reverse_clean_corrupt) or (\n            patch_type == PatchType.TREE_PATCH and reverse_clean_corrupt\n        ):\n            batch_input = batch.corrupt\n            if not ablation_type.mean_over_dataset:\n                patch_src_outs = src_ablations(model, batch.clean, ablation_type)\n        else:\n            raise NotImplementedError\n\n        assert patch_src_outs is not None\n        with patch_mode(model, patch_src_outs):\n            for edge_count in (edge_pbar := tqdm(test_edge_counts)):\n                edge_pbar.set_description_str(f\"Running Circuit: {edge_count} Edges\")\n                threshold = prune_scores_threshold(desc_ps, edge_count)\n                # When prune_scores are tied we can't prune exactly edge_count edges\n                patch_edge_count = 0\n                for mod_name, patch_mask in prune_scores.items():\n                    dest = module_by_name(model, mod_name)\n                    assert isinstance(dest, PatchWrapper)\n                    assert dest.is_dest and dest.patch_mask is not None\n                    if patch_type == PatchType.EDGE_PATCH:\n                        dest.patch_mask.data = (patch_mask.abs() &gt;= threshold).float()\n                        patch_edge_count += dest.patch_mask.int().sum().item()\n                    else:\n                        assert patch_type == PatchType.TREE_PATCH\n                        dest.patch_mask.data = (patch_mask.abs() &lt; threshold).float()\n                        patch_edge_count += (1 - dest.patch_mask.int()).sum().item()\n                with t.inference_mode():\n                    model_output = model(batch_input)[model.out_slice]\n                circ_outs[patch_edge_count][batch.key] = model_output.detach().clone()\n            if render_graph:\n                draw_seq_graph(\n                    model=model,\n                    score_threshold=render_score_threshold,\n                    show_all_seq_pos=False,\n                    seq_labels=dataloader.seq_labels,\n                    file_path=render_file_path,\n                )\n    del patch_src_outs\n    return circ_outs\n</code></pre>"},{"location":"reference/tasks/","title":"Tasks","text":""},{"location":"reference/tasks/#auto_circuit.tasks","title":"auto_circuit.tasks","text":""},{"location":"reference/tasks/#auto_circuit.tasks-attributes","title":"Attributes","text":""},{"location":"reference/tasks/#auto_circuit.tasks-classes","title":"Classes","text":""},{"location":"reference/tasks/#auto_circuit.tasks.Task","title":"Task  <code>dataclass</code>","text":"<pre><code>Task(key: TaskKey, name: str, batch_size: int | Tuple[int, int], batch_count: int | Tuple[int, int], token_circuit: bool, _model_def: str | Module, _dataset_name: str, factorized: bool = True, separate_qkv: bool = True, _true_edge_func: Optional[Callable[..., Set[Edge]]] = None, slice_output: OutputSlice = 'last_seq', autoencoder_input: Optional[AutoencoderInput] = None, autoencoder_max_latents: Optional[int] = None, autoencoder_pythia_size: Optional[str] = None, autoencoder_prune_with_corrupt: Optional[bool] = None, dtype: dtype = t.float32, __init_complete__: bool = False)\n</code></pre> <p>A task to be used in the auto-circuit experiments.</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>TaskKey</code> <p>A unique identifier for the task.</p> required <code>name</code> <code>str</code> <p>A human-readable name for the task, used in visualizations.</p> required <code>batch_size</code> <code>int | Tuple[int, int]</code> <p>The batch size to use for training and testing.</p> required <code>batch_count</code> <code>int | Tuple[int, int]</code> <p>The number of batches to use for training and testing.</p> required <code>token_circuit</code> <code>bool</code> <p>Whether to patch different token positions separately (<code>True</code>) or not (<code>False</code>).</p> required <code>_model_def</code> <code>str | Module</code> <p>The model to use for the task. If a string, the model will be loaded from the <code>transformer_lens</code> library with the correct config.</p> required <code>_dataset_name</code> <code>str</code> <p>The dataset name to use for the task. The file <code>\"datasets/{_dataset_name}.json\"</code> with be loaded using <code>load_datasets_from_json</code>.</p> required <code>factorized</code> <code>bool</code> <p>Whether to use the factorized model and Edge Patching (<code>True</code>) or the residual model and Node Patching (<code>False</code>).</p> <code>True</code> <code>separate_qkv</code> <code>bool</code> <p>Whether to have separate Q, K, and V input nodes. Outputs from attention heads are the same either way.</p> <code>True</code> <code>_true_edge_func</code> <code>Optional[Callable[..., Set[Edge]]]</code> <p>A function that returns the true edges for the task.</p> <code>None</code> <code>slice_output</code> <code>OutputSlice</code> <p>Specifies the index/slice of the output of the model to be considered for the task. For example, <code>\"last_seq\"</code> will consider the last token's output in transformer models.</p> <code>'last_seq'</code> <code>autoencoder_input</code> <code>Optional[AutoencoderInput]</code> <p>If not <code>None</code>, the model will patch in autoencoder reconstructions at each layer of the model. This variable determines the activations passed to the autoencoder (eg. MLP output or residual stream).</p> <code>None</code> <code>autoencoder_max_latents</code> <code>Optional[int]</code> <p>When loading a model with autoencoders enabled (by setting <code>autoencoder_input</code> to not <code>None</code>), this function uses <code>_prune_latents_with_dataset</code> to first prune the autoencoder latents. <code>_prune_latents_with_datasets</code> runs a batch of data through the model and prunes any latents that are not activated.  This dramatically reduces the number of latent in the autoencoder (and therefore edges in the model), which is generally required to make circuit discovery feasible.  However, there can still be too many feature remaining, so this parameter sets a cap such that we only keep the top <code>autoencoder_max_latents</code> features by activation.</p> <code>None</code> <code>autoencoder_pythia_size</code> <code>Optional[str]</code> <p>The Pythia size to use for the autoencoder.</p> <code>None</code> <code>autoencoder_prune_with_corrupt</code> <code>Optional[bool]</code> <p>Whether to prune the autoencoder with corrupt data.</p> <code>None</code> <code>dtype</code> <code>dtype</code> <p>Sets the data type with which to load <code>transformer_lens</code> models.</p> <code>float32</code> <code>__init_complete__</code> <code>bool</code> <p>Whether the task has been initialized.</p> <code>False</code>"},{"location":"reference/tasks/#auto_circuit.tasks-functions","title":"Functions","text":""},{"location":"reference/types/","title":"Types","text":""},{"location":"reference/types/#auto_circuit.types","title":"auto_circuit.types","text":""},{"location":"reference/types/#auto_circuit.types-attributes","title":"Attributes","text":""},{"location":"reference/types/#auto_circuit.types.AblationMeasurements","title":"AblationMeasurements  <code>module-attribute</code>","text":"<pre><code>AblationMeasurements = Dict[AblationType, PruneMetricMeasurements]\n</code></pre> <p>A dictionary mapping from <code>AblationType</code>s to <code>PruneMetricMeasurements</code>.</p>"},{"location":"reference/types/#auto_circuit.types.AlgoKey","title":"AlgoKey  <code>module-attribute</code>","text":"<pre><code>AlgoKey = str\n</code></pre> <p>A string that uniquely identifies a <code>PruneAlgo</code>.</p>"},{"location":"reference/types/#auto_circuit.types.AlgoMeasurements","title":"AlgoMeasurements  <code>module-attribute</code>","text":"<pre><code>AlgoMeasurements = Dict[AlgoKey, Measurements]\n</code></pre> <p>A dictionary mapping from <code>AlgoKey</code>s to <code>Measurements</code>.</p>"},{"location":"reference/types/#auto_circuit.types.AlgoPruneScores","title":"AlgoPruneScores  <code>module-attribute</code>","text":"<pre><code>AlgoPruneScores = Dict[AlgoKey, PruneScores]\n</code></pre> <p>A dictionary mapping from <code>AlgoKey</code>s to <code>PruneScores</code>.</p>"},{"location":"reference/types/#auto_circuit.types.AutoencoderInput","title":"AutoencoderInput  <code>module-attribute</code>","text":"<pre><code>AutoencoderInput = Literal['mlp_post_act', 'resid_delta_mlp', 'resid']\n</code></pre> <p>The activation in each layer that is replaced by an autoencoder reconstruction.</p>"},{"location":"reference/types/#auto_circuit.types.BatchOutputs","title":"BatchOutputs  <code>module-attribute</code>","text":"<pre><code>BatchOutputs = Dict[BatchKey, Tensor]\n</code></pre> <p>A dictionary mapping from <code>BatchKey</code>s to output tensors.</p>"},{"location":"reference/types/#auto_circuit.types.CircuitOutputs","title":"CircuitOutputs  <code>module-attribute</code>","text":"<pre><code>CircuitOutputs = Dict[int, BatchOutputs]\n</code></pre> <p>A dictionary mapping from the number of pruned edges to <code>BatchOutputs</code></p>"},{"location":"reference/types/#auto_circuit.types.MaskFn","title":"MaskFn  <code>module-attribute</code>","text":"<pre><code>MaskFn = Optional[Literal['hard_concrete', 'sigmoid']]\n</code></pre> <p>Determines how mask values are used to ablate edges.</p> <p>If <code>None</code>, the mask value is used directly to interpolate between the original and ablated values. ie. <code>0.0</code> means the original value, <code>1.0</code> means the ablated value.</p> <p>If <code>\"hard_concrete\"</code>, the mask value parameterizes a \"HardConcrete\" distribution (Louizos et al., 2017, Cao et at., 2021) which is sampled to interpolate between the original and ablated values. The HardConcrete distribution allows us to optimize a continuous variable while still allowing the mask to take values equal to <code>0.0</code> or <code>1.0</code>. And the stochasticity helps to reduce problems from vanishing gradients.</p> <p>If <code>\"sigmoid\"</code>, the mask value is passed through a sigmoid function and then used to interpolate between the original and ablated values.</p>"},{"location":"reference/types/#auto_circuit.types.Measurements","title":"Measurements  <code>module-attribute</code>","text":"<pre><code>Measurements = List[Tuple[int | float, int | float]]\n</code></pre> <p>List of X and Y measurements. X is often the number of edges in the circuit and Y is often some measure of faithfulness.</p>"},{"location":"reference/types/#auto_circuit.types.OutputSlice","title":"OutputSlice  <code>module-attribute</code>","text":"<pre><code>OutputSlice = Optional[Literal['last_seq', 'not_first_seq']]\n</code></pre> <p>The slice of the output that is considered for task evaluation. For example, <code>\"last_seq\"</code> will consider the last token's output in transformer models.</p>"},{"location":"reference/types/#auto_circuit.types.PruneMetricKey","title":"PruneMetricKey  <code>module-attribute</code>","text":"<pre><code>PruneMetricKey = str\n</code></pre> <p>A string the uniquely identifies a <code>PruneMetric</code>.</p>"},{"location":"reference/types/#auto_circuit.types.PruneMetricMeasurements","title":"PruneMetricMeasurements  <code>module-attribute</code>","text":"<pre><code>PruneMetricMeasurements = Dict[PruneMetricKey, TaskMeasurements]\n</code></pre> <p>A dictionary mapping from <code>PruneMetricKey</code>s to <code>TaskMeasurements</code>.</p>"},{"location":"reference/types/#auto_circuit.types.PruneScores","title":"PruneScores  <code>module-attribute</code>","text":"<pre><code>PruneScores = Dict[str, Tensor]\n</code></pre> <p>Dictionary from module names of <code>DestNode</code>s to edge scores. The edge scores are stored as a tensor where each value corresponds to the score of an incoming <code>Edge</code>.</p>"},{"location":"reference/types/#auto_circuit.types.TaskKey","title":"TaskKey  <code>module-attribute</code>","text":"<pre><code>TaskKey = str\n</code></pre> <p>A string that uniquely identifies a <code>Task</code>.</p>"},{"location":"reference/types/#auto_circuit.types.TaskMeasurements","title":"TaskMeasurements  <code>module-attribute</code>","text":"<pre><code>TaskMeasurements = Dict[TaskKey, AlgoMeasurements]\n</code></pre> <p>A dictionary mapping from <code>TaskKey</code>s to <code>AlgoMeasurements</code>.</p>"},{"location":"reference/types/#auto_circuit.types.TaskPruneScores","title":"TaskPruneScores  <code>module-attribute</code>","text":"<pre><code>TaskPruneScores = Dict[TaskKey, AlgoPruneScores]\n</code></pre> <p>A dictionary mapping from <code>TaskKey</code>s to <code>AlgoPruneScores</code>.</p>"},{"location":"reference/types/#auto_circuit.types.TestEdges","title":"TestEdges  <code>module-attribute</code>","text":"<pre><code>TestEdges = EdgeCounts | List[int | float]\n</code></pre> <p>Determines the set of [number of edges to prune] to test. This value is used as a parameter to <code>edge_counts_util</code>.</p> <p>If a list of integers, then these are the edge counts that will be used. If a list of floats, then these proportions of the total number of edges will be used.</p>"},{"location":"reference/types/#auto_circuit.types-classes","title":"Classes","text":""},{"location":"reference/types/#auto_circuit.types.AblationType","title":"AblationType","text":"<p>             Bases: <code>Enum</code></p> <p>Type of activation with which replace an original activation during a forward pass.</p>"},{"location":"reference/types/#auto_circuit.types.AblationType-attributes","title":"Attributes","text":""},{"location":"reference/types/#auto_circuit.types.AblationType.BATCH_ALL_TOK_MEAN","title":"BATCH_ALL_TOK_MEAN  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>BATCH_ALL_TOK_MEAN = 7\n</code></pre> <p>Compute the mean over all tokens in the current batch.</p>"},{"location":"reference/types/#auto_circuit.types.AblationType.BATCH_TOKENWISE_MEAN","title":"BATCH_TOKENWISE_MEAN  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>BATCH_TOKENWISE_MEAN = 6\n</code></pre> <p>Compute the token-wise mean over the current input batch.</p>"},{"location":"reference/types/#auto_circuit.types.AblationType.RESAMPLE","title":"RESAMPLE  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>RESAMPLE = 1\n</code></pre> <p>Use the corresponding activation from the forward pass of the corrupt input.</p>"},{"location":"reference/types/#auto_circuit.types.AblationType.TOKENWISE_MEAN_CLEAN","title":"TOKENWISE_MEAN_CLEAN  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>TOKENWISE_MEAN_CLEAN = 3\n</code></pre> <p>Compute the token-wise mean of the clean input over the entire dataset.</p>"},{"location":"reference/types/#auto_circuit.types.AblationType.TOKENWISE_MEAN_CLEAN_AND_CORRUPT","title":"TOKENWISE_MEAN_CLEAN_AND_CORRUPT  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>TOKENWISE_MEAN_CLEAN_AND_CORRUPT = 5\n</code></pre> <p>Compute the token-wise mean of the clean and corrupt inputs over the entire dataset.</p>"},{"location":"reference/types/#auto_circuit.types.AblationType.TOKENWISE_MEAN_CORRUPT","title":"TOKENWISE_MEAN_CORRUPT  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>TOKENWISE_MEAN_CORRUPT = 4\n</code></pre> <p>Compute the token-wise mean of the corrupt input over the entire dataset.</p>"},{"location":"reference/types/#auto_circuit.types.AblationType.ZERO","title":"ZERO  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>ZERO = 2\n</code></pre> <p>Use a vector of zeros.</p>"},{"location":"reference/types/#auto_circuit.types.DestNode","title":"DestNode  <code>dataclass</code>","text":"<pre><code>DestNode(name: str, module_name: str, layer: int, head_idx: Optional[int] = None, head_dim: Optional[int] = None, weight: Optional[str] = None, weight_head_dim: Optional[int] = None, min_src_idx: int = 0)\n</code></pre> <p>             Bases: <code>Node</code></p> <p>A node that is the destination of an edge.</p>"},{"location":"reference/types/#auto_circuit.types.Edge","title":"Edge  <code>dataclass</code>","text":"<pre><code>Edge(src: SrcNode, dest: DestNode, seq_idx: Optional[int] = None)\n</code></pre> <p>A directed edge from a <code>SrcNode</code> to a <code>DestNode</code> in the computational graph of the model used for ablation.</p> <p>And an optional sequence index that specifies the token position when the <code>PatchableModel</code> has <code>seq_len</code> not <code>None</code>.</p>"},{"location":"reference/types/#auto_circuit.types.Edge-attributes","title":"Attributes","text":""},{"location":"reference/types/#auto_circuit.types.Edge.dest","title":"dest  <code>instance-attribute</code>","text":"<pre><code>dest: DestNode\n</code></pre> <p>The <code>DestNode</code> of the edge.</p>"},{"location":"reference/types/#auto_circuit.types.Edge.name","title":"name  <code>property</code>","text":"<pre><code>name: str\n</code></pre> <p>The name of the edge. Equal to <code>{src.name}-&gt;{dest.name}</code>.</p>"},{"location":"reference/types/#auto_circuit.types.Edge.patch_idx","title":"patch_idx  <code>property</code>","text":"<pre><code>patch_idx: Tuple[int, ...]\n</code></pre> <p>The index of the edge in the <code>patch_mask</code> or <code>PruneScores</code> tensor of the <code>dest</code> node.</p>"},{"location":"reference/types/#auto_circuit.types.Edge.seq_idx","title":"seq_idx  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>seq_idx: Optional[int] = None\n</code></pre> <p>The sequence index of the edge.</p>"},{"location":"reference/types/#auto_circuit.types.Edge.src","title":"src  <code>instance-attribute</code>","text":"<pre><code>src: SrcNode\n</code></pre> <p>The <code>SrcNode</code> of the edge.</p>"},{"location":"reference/types/#auto_circuit.types.Edge-functions","title":"Functions","text":""},{"location":"reference/types/#auto_circuit.types.Edge.patch_mask","title":"patch_mask","text":"<pre><code>patch_mask(model: Any) -&gt; Parameter\n</code></pre> <p>The <code>patch_mask</code> tensor of the <code>dest</code> node.</p> Source code in <code>auto_circuit/types.py</code> <pre><code>def patch_mask(self, model: Any) -&gt; t.nn.Parameter:\n    \"\"\"The `patch_mask` tensor of the `dest` node.\"\"\"\n    return self.dest.module(model).patch_mask\n</code></pre>"},{"location":"reference/types/#auto_circuit.types.Edge.prune_score","title":"prune_score","text":"<pre><code>prune_score(prune_scores: PruneScores) -&gt; Tensor\n</code></pre> <p>The score of the edge in the given <code>PruneScores</code>.</p> Source code in <code>auto_circuit/types.py</code> <pre><code>def prune_score(self, prune_scores: PruneScores) -&gt; t.Tensor:\n    \"\"\"\n    The score of the edge in the given\n    [`PruneScores`][auto_circuit.types.PruneScores].\n    \"\"\"\n    return prune_scores[self.dest.module_name][self.patch_idx]\n</code></pre>"},{"location":"reference/types/#auto_circuit.types.EdgeCounts","title":"EdgeCounts","text":"<p>             Bases: <code>Enum</code></p> <p>Special values for <code>TestEdges</code> that get computed at runtime.</p>"},{"location":"reference/types/#auto_circuit.types.EdgeCounts-attributes","title":"Attributes","text":""},{"location":"reference/types/#auto_circuit.types.EdgeCounts.ALL","title":"ALL  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>ALL = 1\n</code></pre> <p>Test <code>0, 1, 2, ..., n_edges</code> edges.</p>"},{"location":"reference/types/#auto_circuit.types.EdgeCounts.GROUPS","title":"GROUPS  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>GROUPS = 3\n</code></pre> <p>Group edges by <code>PruneScores</code> and cumulatively add the number of edges in each group in descending order by score.</p>"},{"location":"reference/types/#auto_circuit.types.EdgeCounts.LOGARITHMIC","title":"LOGARITHMIC  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>LOGARITHMIC = 2\n</code></pre> <p>Test <code>0, 1, 2, ..., 10, 20, ..., 100, 200, ..., 1000, 2000, ...</code> edges.</p>"},{"location":"reference/types/#auto_circuit.types.Node","title":"Node  <code>dataclass</code>","text":"<pre><code>Node(name: str, module_name: str, layer: int, head_idx: Optional[int] = None, head_dim: Optional[int] = None, weight: Optional[str] = None, weight_head_dim: Optional[int] = None)\n</code></pre> <p>A node in the computational graph of the model used for ablation.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>The name of the node.</p> required <code>module_name</code> <code>str</code> <p>The name of the PyTorch module in the model that the node is in. Modules can have multiple nodes, for example, the multi-head attention module in a transformer model has a node for each head.</p> required <code>layer</code> <code>int</code> <p>The layer of the model that the node is in. Transformer blocks count as 2 layers (one for the attention layer and one for the MLP layer) because we want to connect nodes in the attention layer to nodes in the subsequent MLP layer.</p> required <code>head_idx</code> <code>Optional[int]</code> <p>The index of the head in the multi-head attention module that the node is in.</p> <code>None</code> <code>head_dim</code> <code>Optional[int]</code> <p>The dimension of the head in the multi-head attention layer that the node is in.</p> <code>None</code> <code>weight</code> <code>Optional[str]</code> <p>The name of the weight in the module that corresponds to the node. Not currently used, but could be used by a circuit finding algorithm.</p> <code>None</code> <code>weight_head_dim</code> <code>Optional[int]</code> <p>The dimension of the head in the weight tensor that corresponds to the node. Not currently used, but could be used by a circuit finding algorithm.</p> <code>None</code>"},{"location":"reference/types/#auto_circuit.types.Node-functions","title":"Functions","text":""},{"location":"reference/types/#auto_circuit.types.Node.module","title":"module","text":"<pre><code>module(model: Any) -&gt; PatchWrapper\n</code></pre> <p>Get the <code>PatchWrapper</code> for this node.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>Any</code> <p>The model that the node is in.</p> required <p>Returns:</p> Type Description <code>PatchWrapper</code> <p>The <code>PatchWrapper</code> for this node.</p> Source code in <code>auto_circuit/types.py</code> <pre><code>def module(self, model: Any) -&gt; PatchWrapper:\n    \"\"\"\n    Get the [`PatchWrapper`][auto_circuit.utils.patch_wrapper.PatchWrapper] for this\n    node.\n\n    Args:\n        model: The model that the node is in.\n\n    Returns:\n        The `PatchWrapper` for this node.\n    \"\"\"\n    patch_wrapper = module_by_name(model, self.module_name)\n    assert isinstance(patch_wrapper, PatchWrapper)\n    return patch_wrapper\n</code></pre>"},{"location":"reference/types/#auto_circuit.types.PatchType","title":"PatchType","text":"<p>             Bases: <code>Enum</code></p> <p>Whether to patch the edges in the circuit or the complement of the circuit.</p>"},{"location":"reference/types/#auto_circuit.types.PatchType-attributes","title":"Attributes","text":""},{"location":"reference/types/#auto_circuit.types.PatchType.EDGE_PATCH","title":"EDGE_PATCH  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>EDGE_PATCH = 1\n</code></pre> <p>Patch the edges in the circuit.</p>"},{"location":"reference/types/#auto_circuit.types.PatchType.TREE_PATCH","title":"TREE_PATCH  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>TREE_PATCH = 2\n</code></pre> <p>Patch the edges not in the circuit.</p>"},{"location":"reference/types/#auto_circuit.types.PatchWrapper","title":"PatchWrapper","text":"<pre><code>PatchWrapper()\n</code></pre> <p>             Bases: <code>Module</code>, <code>ABC</code></p> <p>Abstract class for a wrapper around a module that can be patched.</p> Source code in <code>auto_circuit/types.py</code> <pre><code>def __init__(self):\n    super().__init__()\n</code></pre>"},{"location":"reference/types/#auto_circuit.types.SrcNode","title":"SrcNode  <code>dataclass</code>","text":"<pre><code>SrcNode(name: str, module_name: str, layer: int, head_idx: Optional[int] = None, head_dim: Optional[int] = None, weight: Optional[str] = None, weight_head_dim: Optional[int] = None, src_idx: int = 0)\n</code></pre> <p>             Bases: <code>Node</code></p> <p>A node that is the source of an edge.</p>"},{"location":"reference/types/#auto_circuit.types-functions","title":"Functions","text":""},{"location":"reference/visualize/","title":"Visualize","text":""},{"location":"reference/visualize/#auto_circuit.visualize","title":"auto_circuit.visualize","text":""},{"location":"reference/visualize/#auto_circuit.visualize-attributes","title":"Attributes","text":""},{"location":"reference/visualize/#auto_circuit.visualize-classes","title":"Classes","text":""},{"location":"reference/visualize/#auto_circuit.visualize-functions","title":"Functions","text":""},{"location":"reference/visualize/#auto_circuit.visualize.draw_seq_graph","title":"draw_seq_graph","text":"<pre><code>draw_seq_graph(model: PatchableModel, prune_scores: Optional[PruneScores] = None, score_threshold: float = 0.01, show_all_seq_pos: bool = False, seq_labels: Optional[List[str]] = None, layer_spacing: bool = False, orientation: Literal['h', 'v'] = 'h', display_ipython: bool = True, file_path: Optional[str] = None) -&gt; Figure\n</code></pre> <p>Draw the sankey for all token positions in a <code>PatchableModel</code> (drawn separately for each token position if the model has a <code>seq_len</code>).</p> <p>If <code>prune_scores</code> is <code>None</code>, the diagram will show the current activations and mask values of the model. If <code>prune_scores</code> is provided, the diagram will use these edge scores and won't show activations.</p> <p>The mask values or <code>prune_scores</code> are used to set the width of each edge.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>PatchableModel</code> <p>The model to visualize.</p> required <code>prune_scores</code> <code>Optional[PruneScores]</code> <p>The edge scores to use for the visualization. If <code>None</code>, the current activations and mask values of the model will be visualized instead.</p> <code>None</code> <code>score_threshold</code> <code>float</code> <p>The minimum absolute edge score to show in the diagram.</p> <code>0.01</code> <code>show_all_seq_pos</code> <code>bool</code> <p>If <code>True</code>, the diagram will show all token positions, even if they have no non-zero edge values. If <code>False</code>, only token positions with non-zero edge values will be shown.</p> <code>False</code> <code>seq_labels</code> <code>Optional[List[str]]</code> <p>The labels for each token position.</p> <code>None</code> <code>layer_spacing</code> <code>bool</code> <p>If <code>True</code>, all nodes are spaced according to the layer they in. Otherwise, the Plotly automatic spacing is used and nodes in later layers may appear to the left of nodes in earlier layers. If <code>True</code> the output may be much wider if only a few edges are drawn.</p> <code>False</code> <code>orientation</code> <code>Literal['h', 'v']</code> <p>The orientation of the sankey diagram. Can be either <code>\"h\"</code> for horizontal or <code>\"v\"</code> for vertical.</p> <code>'h'</code> <code>display_ipython</code> <code>bool</code> <p>If <code>True</code>, the diagram will be displayed in the current ipython environment.</p> <code>True</code> <code>file_path</code> <code>Optional[str]</code> <p>If provided, the diagram will be saved to this file path. The file extension determines the format of the saved image.</p> <code>None</code> Source code in <code>auto_circuit/visualize.py</code> <pre><code>def draw_seq_graph(\n    model: PatchableModel,\n    prune_scores: Optional[PruneScores] = None,\n    score_threshold: float = 1e-2,\n    show_all_seq_pos: bool = False,\n    seq_labels: Optional[List[str]] = None,\n    layer_spacing: bool = False,\n    orientation: Literal[\"h\", \"v\"] = \"h\",\n    display_ipython: bool = True,\n    file_path: Optional[str] = None,\n) -&gt; go.Figure:\n    \"\"\"\n    Draw the sankey for all token positions in a\n    [`PatchableModel`][auto_circuit.utils.patchable_model.PatchableModel] (drawn\n    separately for each token position if the model has a `seq_len`).\n\n    If `prune_scores` is `None`, the diagram will show the current activations and mask\n    values of the model. If `prune_scores` is provided, the diagram will use these edge\n    scores and won't show activations.\n\n    The mask values or `prune_scores` are used to set the width of each edge.\n\n    Args:\n        model: The model to visualize.\n        prune_scores: The edge scores to use for the visualization. If `None`, the\n            current activations and mask values of the model will be visualized instead.\n        score_threshold: The minimum _absolute_ edge score to show in the diagram.\n        show_all_seq_pos: If `True`, the diagram will show all token positions, even if\n            they have no non-zero edge values. If `False`, only token positions with\n            non-zero edge values will be shown.\n        seq_labels: The labels for each token position.\n        layer_spacing: If `True`, all nodes are spaced according to the layer they in.\n            Otherwise, the Plotly automatic spacing is used and nodes in later layers\n            may appear to the left of nodes in earlier layers. If `True` the output may\n            be much wider if only a few edges are drawn.\n        orientation: The orientation of the sankey diagram. Can be either `\"h\"` for\n            horizontal or `\"v\"` for vertical.\n        display_ipython: If `True`, the diagram will be displayed in the current\n            ipython environment.\n        file_path: If provided, the diagram will be saved to this file path. The file\n            extension determines the format of the saved image.\n    \"\"\"\n    seq_len = model.seq_len or 1\n\n    # Calculate the vertical interval for each sub-diagram\n    if prune_scores is None:\n        edge_scores = model.current_patch_masks_as_prune_scores().values()\n    else:\n        edge_scores = prune_scores.values()\n    ps = [t.clamp(v.abs() - score_threshold, min=0).sum().item() for v in edge_scores]\n    total_ps = max(sum(ps), 1e-2)\n    if seq_len &gt; 1:\n        sankey_heights: Dict[Optional[int], float] = {}\n        for patch_mask in edge_scores:\n            ps_seq_tots = t.clamp(patch_mask.abs() - score_threshold, min=0.0)\n            ps_seq_tots = ps_seq_tots.sum(dim=list(range(1, patch_mask.ndim)))\n            for seq_idx, ps_seq_tot in enumerate(ps_seq_tots):\n                if ps_seq_tot &gt; 0 or show_all_seq_pos:\n                    if seq_idx not in sankey_heights:\n                        sankey_heights[seq_idx] = 0\n                    sankey_heights[seq_idx] += ps_seq_tot.item()\n\n        for seq_idx in model.edge_dict.keys():\n            min_height = total_ps / (len(model.edge_dict) * 2)\n            if show_all_seq_pos:\n                sankey_heights[seq_idx] = max(sankey_heights[seq_idx], min_height)\n        margin_height: float = total_ps / ((n_figs := len(sankey_heights)) * 2)\n        total_height = sum(sankey_heights.values()) + margin_height * (n_figs - 1)\n        intervals, interval_start = {}, total_height\n        for seq_idx, height in sorted(\n            sankey_heights.items(), key=lambda x: (x is None, x)\n        ):\n            interval_end = interval_start - (margin_height if len(intervals) &gt; 0 else 0)\n            interval_start = interval_end - height\n            intervals[seq_idx] = max(interval_start / total_height, 1e-6), min(\n                interval_end / total_height, 1 - 1e-6\n            )\n    else:\n        intervals = {list(model.edge_dict.keys())[0]: (0, 1)}\n\n    # Draw the sankey for each token position\n    sankeys, n_layers = [], 0\n    for seq_idx, vert_interval in intervals.items():\n        edge_set = set(model.edge_dict[seq_idx])\n        viz, n_layers = net_viz(\n            model=model,\n            seq_edges=edge_set,\n            prune_scores=prune_scores,\n            vert_interval=vert_interval,\n            seq_idx=seq_idx,\n            score_threshold=score_threshold,\n            layer_spacing=layer_spacing,\n            orientation=orientation,\n        )\n        sankeys.append(viz)\n\n    if orientation == \"h\":\n        h = max(250 * len(sankeys), 400)\n        w = max(50 * n_layers, 600)\n    else:\n        h = max(50 * n_layers, 600)\n        w = max(700 * len(sankeys), 800)\n    layout = go.Layout(height=h, width=w, plot_bgcolor=\"blue\")\n    fig = go.Figure(data=sankeys, layout=layout)\n    for fig_idx, seq_idx in enumerate(intervals.keys()) if seq_labels else []:\n        assert seq_labels is not None\n        seq_label = \"All tokens\" if seq_idx is None else seq_labels[seq_idx]\n        y_range: Tuple[float, float] = fig.data[fig_idx].domain[\"y\"]  # type: ignore\n        fig.add_annotation(\n            x=-0.17,\n            y=(y_range[0] + y_range[1]) / 2,\n            text=f\"&lt;b&gt;{seq_label}&lt;/b&gt;\",\n            showarrow=False,\n            xref=\"paper\",\n            yref=\"paper\",\n        )\n    if display_ipython:\n        fig.show()\n    if file_path:\n        absolute_path: Path = repo_path_to_abs_path(file_path)\n        fig.write_image(str(absolute_path))\n    return fig\n</code></pre>"},{"location":"reference/visualize/#auto_circuit.visualize.net_viz","title":"net_viz","text":"<pre><code>net_viz(model: PatchableModel, seq_edges: Set[Edge], prune_scores: Optional[PruneScores], vert_interval: Tuple[float, float], seq_idx: Optional[int] = None, score_threshold: float = 0.01, layer_spacing: bool = False, orientation: Literal['h', 'v'] = 'h') -&gt; Tuple[Sankey, int]\n</code></pre> <p>Draw the sankey diagram for a single token position. If <code>prune_scores</code> is <code>None</code>, the diagram will show the current activations and edge scores of the model. If <code>prune_scores</code> is provided, the diagram will use these edge scores and won't show activations.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>PatchableModel</code> <p>The model to visualize.</p> required <code>seq_edges</code> <code>Set[Edge]</code> <p>The edges to visualize. This should be the edges at a single token position if <code>model.seq_len</code> is not <code>None</code>. Otherwise, this should be all the edges in the model.</p> required <code>prune_scores</code> <code>Optional[PruneScores]</code> <p>The edge scores to use for the visualization. If <code>None</code>, the current activations and mask values of the model will be visualized instead.</p> required <code>vert_interval</code> <code>Tuple[float, float]</code> <p>The vertical interval to place the diagram in the figure. Must be in the range <code>(0, 1)</code>. This is used by <code>draw_seq_graph</code> to place the diagrams for each token position in the figure. If you are using this function to create a standalone diagram, you can set this to <code>(0, 1)</code>.</p> required <code>seq_idx</code> <code>Optional[int]</code> <p>The token position being visualized, this is used to get the correct slice of activations (if <code>prune_scores</code> is <code>None</code>) to label the edges.</p> <code>None</code> <code>show_all_edges</code> <p>If <code>True</code>, all edges will be shown, even if their edge score is close to zero. If <code>False</code>, only edges with a non-zero edge score will be shown.</p> required <code>layer_spacing</code> <code>bool</code> <p>If <code>True</code>, all nodes are spaced according to the layer they in. Otherwise, the Plotly automatic spacing is used and nodes in later layers may appear to the left of nodes in earlier layers.</p> <code>False</code> <code>orientation</code> <code>Literal['h', 'v']</code> <p>The orientation of the sankey diagram. Can be either <code>\"h\"</code> for horizontal or <code>\"v\"</code> for vertical.</p> <code>'h'</code> <p>Returns:</p> Type Description <code>Tuple[Sankey, int]</code> <p>The sankey diagram for the given token position.</p> Note <p>This is a lower level function, it is generally recommended to use <code>draw_seq_graph</code> instead.</p> Source code in <code>auto_circuit/visualize.py</code> <pre><code>def net_viz(\n    model: PatchableModel,\n    seq_edges: Set[Edge],\n    prune_scores: Optional[PruneScores],\n    vert_interval: Tuple[float, float],\n    seq_idx: Optional[int] = None,\n    score_threshold: float = 1e-2,\n    layer_spacing: bool = False,\n    orientation: Literal[\"h\", \"v\"] = \"h\",\n) -&gt; Tuple[go.Sankey, int]:\n    \"\"\"\n    Draw the sankey diagram for a single token position.\n    If `prune_scores` is `None`, the diagram will show the current activations and edge\n    scores of the model. If `prune_scores` is provided, the diagram will use these edge\n    scores and won't show activations.\n\n    Args:\n        model: The model to visualize.\n        seq_edges: The edges to visualize. This should be the edges at a single token\n            position if `model.seq_len` is not `None`. Otherwise, this should be all the\n            edges in the model.\n        prune_scores: The edge scores to use for the visualization. If `None`, the\n            current activations and mask values of the model will be visualized instead.\n        vert_interval: The vertical interval to place the diagram in the figure. Must\n            be in the range `(0, 1)`. This is used by\n            [`draw_seq_graph`][auto_circuit.visualize.draw_seq_graph] to place the\n            diagrams for each token position in the figure. If you are using this\n            function to create a standalone diagram, you can set this to `(0, 1)`.\n        seq_idx: The token position being visualized, this is used to get the correct\n            slice of activations (if `prune_scores` is `None`) to label the edges.\n        show_all_edges: If `True`, all edges will be shown, even if their edge score is\n            close to zero. If `False`, only edges with a non-zero edge score will be\n            shown.\n        layer_spacing: If `True`, all nodes are spaced according to the layer they in.\n            Otherwise, the Plotly automatic spacing is used and nodes in later layers\n            may appear to the left of nodes in earlier layers.\n        orientation: The orientation of the sankey diagram. Can be either `\"h\"` for\n            horizontal or `\"v\"` for vertical.\n\n    Returns:\n        The sankey diagram for the given token position.\n\n    Note:\n        This is a lower level function, it is generally recommended to use\n        [`draw_seq_graph`][auto_circuit.visualize.draw_seq_graph] instead.\n\n    \"\"\"\n    nodes: OrderedSet[Node] = OrderedSet(model.nodes)\n    un = False if orientation == \"h\" else True\n\n    # Define the sankey nodes\n    viz_nodes: Dict[str, Node] = dict([(node_name(n.name, un), n) for n in nodes])\n    node_idxs: Dict[str, int] = dict([(n, i) for i, n in enumerate(viz_nodes.keys())])\n    lyr_nodes: Dict[int, List[str]] = defaultdict(list)\n    for n in viz_nodes.values():\n        lyr_nodes[n.layer].append(n.name)\n    graph_nodes = {\n        \"label\": [\"\" for _, _ in viz_nodes.items()],\n        \"color\": [\"rgba(0,0,0,0.0)\" for _, _ in viz_nodes.items()],\n        \"line\": dict(width=0.0),\n    }\n\n    # Define the sankey edges\n    sources, targets, values, labels, colors = [], [], [], [], []\n    included_layer_nodes: Dict[int, List[str]] = defaultdict(list)\n    for e in seq_edges:\n        if prune_scores is None:\n            no_edge_score_error = \"Visualization requires patch mode or PruneScores.\"\n            assert e.dest.module(model).curr_src_outs is not None, no_edge_score_error\n            edge_score = e.patch_mask(model).data[e.patch_idx].item()\n            if edge_score == 1.0:  # Show the patched edge activation\n                lbl = e.dest.module(model).patch_src_outs[e.src.src_idx]\n            else:\n                lbl = e.dest.module(model).curr_src_outs[e.src.src_idx]\n        else:\n            edge_score = prune_scores[e.dest.module_name][e.patch_idx].item()\n            lbl = None\n\n        if abs(edge_score) &lt; score_threshold:\n            continue\n\n        color_idx = len(sources) % len(COLOR_PALETTE)\n        sources.append(node_idxs[node_name(e.src.name, un)])\n        graph_nodes[\"label\"][node_idxs[node_name(e.src.name, un)]] = node_name(\n            e.src.name, un\n        )\n        graph_nodes[\"color\"][node_idxs[node_name(e.src.name, un)]] = COLOR_PALETTE[\n            color_idx\n        ]\n        targets.append(node_idxs[node_name(e.dest.name, un)])\n        graph_nodes[\"label\"][node_idxs[node_name(e.dest.name, un)]] = node_name(\n            e.dest.name, un\n        )\n        graph_nodes[\"color\"][node_idxs[node_name(e.dest.name, un)]] = COLOR_PALETTE[\n            color_idx\n        ]\n        values.append(0.8 if prune_scores is None else abs(edge_score))\n        lbl = t_fmt(lbl, model.seq_dim, seq_idx, \"&lt;br&gt;\")\n        lbl = e.name + \"&lt;br&gt;\" + lbl + f\"&lt;br&gt;{edge_score:.2f}\"\n        labels.append(lbl)\n        if edge_score == 0:\n            edge_color = \"rgba(0,0,0,0.1)\"\n        elif edge_score &gt; 0:\n            edge_color = \"rgba(0,0,255,0.3)\"\n        else:\n            edge_color = \"rgba(255,0,0,0.3)\"\n        colors.append(edge_color)\n        included_layer_nodes[e.src.layer].append(e.src.name)\n        included_layer_nodes[e.dest.layer].append(e.dest.name)\n\n    included_layer_count = len(included_layer_nodes)\n    # Add ghost edges to horizontally align nodes to the correct layer\n    for i in lyr_nodes.keys():\n        if i not in included_layer_nodes:\n            included_layer_nodes[i] = [lyr_nodes[i][0]]\n    if layer_spacing:\n        included_layer_count = len(included_layer_nodes)\n\n    ordered_lyr_nodes = [nodes for _, nodes in sorted(included_layer_nodes.items())]\n\n    # Don't add ghost edges if layer_spacing is False\n    if not layer_spacing:\n        ordered_lyr_nodes = []\n\n    ghost_edge_val = 1e-6\n    for lyr_1_nodes, lyr_2_nodes in zip(ordered_lyr_nodes[:-1], ordered_lyr_nodes[1:]):\n        first_lyr_1_node = lyr_1_nodes[0]\n        first_lyr_2_node = lyr_2_nodes[0]\n        for lyr_1_node in lyr_1_nodes:\n            sources.append(node_idxs[node_name(lyr_1_node, un)])\n            targets.append(node_idxs[node_name(first_lyr_2_node, un)])\n            values.append(ghost_edge_val)\n            labels.append(\"\")\n            colors.append(\"rgba(0,255,0,0.0)\")\n        for lyr_2_node in lyr_2_nodes:\n            sources.append(node_idxs[node_name(first_lyr_1_node, un)])\n            targets.append(node_idxs[node_name(lyr_2_node, un)])\n            values.append(ghost_edge_val)\n            labels.append(\"\")\n            colors.append(\"rgba(0,255,0,0.0)\")\n\n    return (\n        go.Sankey(\n            arrangement=\"perpendicular\",\n            node=graph_nodes,\n            link={\n                \"arrowlen\": 25,\n                \"source\": sources,\n                \"target\": targets,\n                \"value\": values,\n                \"label\": labels,\n                \"color\": colors,\n            },\n            orientation=orientation,\n            domain={\"y\": vert_interval},\n        ),\n        included_layer_count,\n    )\n</code></pre>"},{"location":"reference/metrics/area_under_curve/","title":"Area under curve","text":""},{"location":"reference/metrics/area_under_curve/#auto_circuit.metrics.area_under_curve","title":"auto_circuit.metrics.area_under_curve","text":""},{"location":"reference/metrics/area_under_curve/#auto_circuit.metrics.area_under_curve-attributes","title":"Attributes","text":""},{"location":"reference/metrics/area_under_curve/#auto_circuit.metrics.area_under_curve-functions","title":"Functions","text":""},{"location":"reference/metrics/area_under_curve/#auto_circuit.metrics.area_under_curve.algo_measurements_auc","title":"algo_measurements_auc","text":"<pre><code>algo_measurements_auc(algo_measurements: AlgoMeasurements, log_x: bool, log_y: bool, y_min: Optional[float] = None) -&gt; Dict[AlgoKey, float]\n</code></pre> <p>Wrapper that runs <code>measurements_auc</code> on each algorithm's measurements.</p> Source code in <code>auto_circuit/metrics/area_under_curve.py</code> <pre><code>def algo_measurements_auc(\n    algo_measurements: AlgoMeasurements,\n    log_x: bool,\n    log_y: bool,\n    y_min: Optional[float] = None,\n) -&gt; Dict[AlgoKey, float]:\n    \"\"\"\n    Wrapper that runs\n    [`measurements_auc`][auto_circuit.metrics.area_under_curve.measurements_auc] on each\n    algorithm's measurements.\n    \"\"\"\n    algo_measurements_auc = {}\n    for algo, measurements in algo_measurements.items():\n        if len(measurements) &gt; 1:\n            algo_measurements_auc[algo] = measurements_auc(\n                measurements, log_x, log_y, y_min\n            )\n    return algo_measurements_auc\n</code></pre>"},{"location":"reference/metrics/area_under_curve/#auto_circuit.metrics.area_under_curve.average_auc_plot","title":"average_auc_plot","text":"<pre><code>average_auc_plot(task_measurements: TaskMeasurements, log_x: bool, log_y: bool, y_min: Optional[float], inverse: bool) -&gt; Figure\n</code></pre> <p>A bar chart of the average AUC for each algorithm across all tasks. See <code>measurements_auc</code>.</p> <p>Returns:</p> Type Description <code>Figure</code> <p>A Plotly figure.</p> Source code in <code>auto_circuit/metrics/area_under_curve.py</code> <pre><code>def average_auc_plot(\n    task_measurements: TaskMeasurements,\n    log_x: bool,\n    log_y: bool,\n    y_min: Optional[float],\n    inverse: bool,\n) -&gt; go.Figure:\n    \"\"\"\n    A bar chart of the average AUC for each algorithm across all tasks.\n    See [`measurements_auc`][auto_circuit.metrics.area_under_curve.measurements_auc].\n\n    Returns:\n        A Plotly figure.\n    \"\"\"\n    task_algo_aucs: Dict[TaskKey, Dict[AlgoKey, float]] = task_measurements_auc(\n        task_measurements, log_x, log_y, y_min\n    )\n    data, totals = [], defaultdict(float)\n    for task_key, algo_aucs in task_algo_aucs.items():\n        task = TASK_DICT[task_key]\n        for algo_key, auc in reversed(algo_aucs.items()):\n            algo = PRUNE_ALGO_DICT[algo_key]\n            normalized_auc = auc / (\n                algo_aucs[RANDOM_PRUNE_ALGO.key] * len(task_algo_aucs)\n            )\n            if inverse:\n                normalized_unit = 1 / len(task_algo_aucs)\n                normalized_auc = normalized_unit + (normalized_unit - normalized_auc)\n            data.append(\n                {\n                    \"Algorithm\": algo.name,\n                    \"Task\": task.name,\n                    \"Normalized AUC\": normalized_auc,\n                }\n            )\n            totals[algo] += normalized_auc\n    algo_count = len(set([d[\"Algorithm\"] for d in data]))\n    fig = px.bar(\n        data,\n        y=\"Algorithm\",\n        x=\"Normalized AUC\",\n        color=\"Task\",\n        orientation=\"h\",\n        color_discrete_sequence=COLOR_PALETTE[algo_count:],\n    )\n    fig.update_layout(\n        # title=f\"Normalized Area Under {metric_name} Curve\",\n        template=\"plotly\",\n        width=550,\n        legend=dict(orientation=\"h\", yanchor=\"bottom\", y=-0.55, xanchor=\"left\", x=0.0),\n    )\n    max_total = max(totals.values())\n    fig.add_trace(\n        go.Scatter(\n            x=[totals[algo] + max_total * 0.2 for algo in totals],\n            y=[algo.name for algo in totals.keys()],\n            mode=\"text\",\n            text=[f\"{totals[algo]:.2f}\" for algo in totals],\n            textposition=\"middle left\",\n            showlegend=False,\n        )\n    )\n    fig.add_trace(\n        go.Scatter(\n            x=[0.02 for _ in totals],\n            y=[algo.name for algo in totals.keys()],\n            mode=\"text\",\n            text=[f\"{algo.name}\" for algo in totals.keys()],\n            textposition=\"middle right\",\n            showlegend=False,\n            textfont=dict(color=\"white\"),\n        )\n    )\n    fig.update_yaxes(visible=False, showticklabels=False)\n    fig.update_xaxes(title=\"Normalized Inverse AUC\") if inverse else None\n    return fig\n</code></pre>"},{"location":"reference/metrics/area_under_curve/#auto_circuit.metrics.area_under_curve.measurements_auc","title":"measurements_auc","text":"<pre><code>measurements_auc(points: Measurements, log_x: bool, log_y: bool, y_min: Optional[float], eps: float = 0.001) -&gt; float\n</code></pre> <p>Calculate the area under the curve of a set of points.</p> <p>Parameters:</p> Name Type Description Default <code>points</code> <code>Measurements</code> <p>A list of (x, y) points.</p> required <code>log_x</code> <code>bool</code> <p>Whether to log the x values. All values must be <code>&gt;= 0</code>. <code>0</code> values are mapped to <code>0</code>.</p> required <code>log_y</code> <code>bool</code> <p>Whether to log the y values. (See <code>y_min</code>.)</p> required <code>y_min</code> <code>Optional[float]</code> <p>Must be provided if <code>log_y</code> is <code>True</code> and must be greater than zero. If <code>log_y</code> is <code>True</code>, the y values are all set to <code>max(y, y_min)</code>.</p> required <code>eps</code> <code>float</code> <p>A small value to ensure the returned area is greater than zero.</p> <code>0.001</code> Source code in <code>auto_circuit/metrics/area_under_curve.py</code> <pre><code>def measurements_auc(\n    points: Measurements,\n    log_x: bool,\n    log_y: bool,\n    y_min: Optional[float],\n    eps: float = 1e-3,\n) -&gt; float:\n    \"\"\"\n    Calculate the area under the curve of a set of points.\n\n    Args:\n        points: A list of (x, y) points.\n        log_x: Whether to log the x values. All values must be `&gt;= 0`. `0` values are\n            mapped to `0`.\n        log_y: Whether to log the y values. (See `y_min`.)\n        y_min: Must be provided if `log_y` is `True` and must be greater than zero. If\n            `log_y` is `True`, the y values are all set to `max(y, y_min)`.\n        eps: A small value to ensure the returned area is greater than zero.\n    \"\"\"\n    points = sorted(points, key=lambda x: x[0])\n    assert points[0][0] == 0\n    y_baseline = points[0][1]\n    area = 0.0\n    for i in range(1, len(points)):\n        x1, y1 = points[i - 1]\n        x2, y2 = points[i]\n        assert x1 &lt;= x2\n        if log_y:\n            assert y_min is not None\n            y1 = max(y1, y_min)\n            y2 = max(y2, y_min)\n        if log_x and (x1 == 0 or x2 == 0):\n            continue\n        if log_x:\n            assert x1 &gt; 0 and x2 &gt; 0\n            x1 = math.log(x1, 10)\n            x2 = math.log(x2, 10)\n        if log_y:\n            assert y_min is not None\n            y1 = math.log(y1, 10) - math.log(y_min, 10)\n            y2 = math.log(y2, 10) - math.log(y_min, 10)\n        height_1, height_2 = y1 - y_baseline, y2 - y_baseline\n        area += (x2 - x1) * (height_1 + height_2) / 2.0\n    return max(area, eps)\n</code></pre>"},{"location":"reference/metrics/area_under_curve/#auto_circuit.metrics.area_under_curve.metric_measurements_auc","title":"metric_measurements_auc","text":"<pre><code>metric_measurements_auc(points: PruneMetricMeasurements, log_x: bool, log_y: bool, y_min: Optional[float] = None) -&gt; Dict[PruneMetricKey, Dict[TaskKey, Dict[AlgoKey, float]]]\n</code></pre> <p>Wrapper that runs <code>measurements_auc</code> for each metric, task, and algorithm.</p> Source code in <code>auto_circuit/metrics/area_under_curve.py</code> <pre><code>def metric_measurements_auc(\n    points: PruneMetricMeasurements,\n    log_x: bool,\n    log_y: bool,\n    y_min: Optional[float] = None,\n) -&gt; Dict[PruneMetricKey, Dict[TaskKey, Dict[AlgoKey, float]]]:\n    \"\"\"\n    Wrapper that runs\n    [`measurements_auc`][auto_circuit.metrics.area_under_curve.measurements_auc] for\n    each metric, task, and algorithm.\n    \"\"\"\n    return {\n        metric_key: task_measurements_auc(task_measurements, log_x, log_y, y_min)\n        for metric_key, task_measurements in points.items()\n    }\n</code></pre>"},{"location":"reference/metrics/area_under_curve/#auto_circuit.metrics.area_under_curve.task_measurements_auc","title":"task_measurements_auc","text":"<pre><code>task_measurements_auc(task_measurements: TaskMeasurements, log_x: bool, log_y: bool, y_min: Optional[float] = None) -&gt; Dict[TaskKey, Dict[AlgoKey, float]]\n</code></pre> <p>Wrapper that runs <code>measurements_auc</code> for each task and algorithm.</p> Source code in <code>auto_circuit/metrics/area_under_curve.py</code> <pre><code>def task_measurements_auc(\n    task_measurements: TaskMeasurements,\n    log_x: bool,\n    log_y: bool,\n    y_min: Optional[float] = None,\n) -&gt; Dict[TaskKey, Dict[AlgoKey, float]]:\n    \"\"\"\n    Wrapper that runs\n    [`measurements_auc`][auto_circuit.metrics.area_under_curve.measurements_auc] for\n    each task and algorithm.\n    \"\"\"\n    return {\n        task_key: algo_measurements_auc(algo_measurements, log_x, log_y, y_min)\n        for task_key, algo_measurements in task_measurements.items()\n    }\n</code></pre>"},{"location":"reference/metrics/avoid_edges/","title":"Avoid edges","text":""},{"location":"reference/metrics/avoid_edges/#auto_circuit.metrics.avoid_edges","title":"auto_circuit.metrics.avoid_edges","text":""},{"location":"reference/metrics/avoid_edges/#auto_circuit.metrics.avoid_edges-attributes","title":"Attributes","text":""},{"location":"reference/metrics/avoid_edges/#auto_circuit.metrics.avoid_edges-classes","title":"Classes","text":""},{"location":"reference/metrics/avoid_edges/#auto_circuit.metrics.avoid_edges-functions","title":"Functions","text":""},{"location":"reference/metrics/avoid_edges/#auto_circuit.metrics.avoid_edges.run_constrained_prune_funcs","title":"run_constrained_prune_funcs","text":"<pre><code>run_constrained_prune_funcs(task_prune_scores: TaskPruneScores) -&gt; TaskPruneScores\n</code></pre> <p>For each task and each algorithm's <code>PruneScores</code>, run <code>circuit_probing_prune_scores</code> with the <code>avoid_edges</code> parameter set to the top <code>true_edge_count</code> edges of the <code>PruneScores</code> of the given task and algorithm.</p> <p>This is intended to test if we can find a circuit very different from the original solutions found by the given <code>PruneAlgo</code>s, that still perform well on the task.</p> <p>Parameters:</p> Name Type Description Default <code>task_prune_scores</code> <code>TaskPruneScores</code> <p>Prune scores for each task and algorithm.</p> required <p>Returns:</p> Type Description <code>TaskPruneScores</code> <p>A new set of prune scores for each task and algorithm, that attempts to be as different as possible from the original solutions found by the given algorithms while still performing well on the task.</p> Note <p>This is an experimental function and the internal parameters used to find the new edges may not be well tuned.</p> Source code in <code>auto_circuit/metrics/avoid_edges.py</code> <pre><code>def run_constrained_prune_funcs(task_prune_scores: TaskPruneScores) -&gt; TaskPruneScores:\n    \"\"\"\n    For each task and each algorithm's [`PruneScores`][auto_circuit.types.PruneScores],\n    run\n    [`circuit_probing_prune_scores`][auto_circuit.prune_algos.circuit_probing.circuit_probing_prune_scores]\n    with the `avoid_edges` parameter set to the top `true_edge_count` edges of the\n    [`PruneScores`][auto_circuit.types.PruneScores] of the given task and algorithm.\n\n    This is intended to test if we can find a circuit very different from the original\n    solutions found by the given\n    [`PruneAlgo`s][auto_circuit.prune_algos.prune_algos.PruneAlgo], that still perform\n    well on the task.\n\n    Args:\n        task_prune_scores: Prune scores for each task and algorithm.\n\n    Returns:\n        A new set of prune scores for each task and algorithm, that attempts to be as\n            different as possible from the original solutions found by the given\n            algorithms while still performing well on the task.\n\n    Note:\n        This is an experimental function and the internal parameters used to find the\n        new edges may not be well tuned.\n    \"\"\"\n    constrained_task_prune_scores: TaskPruneScores = {}\n    for task_key in (experiment_pbar := tqdm(task_prune_scores.keys())):\n        task = TASK_DICT[task_key]\n        experiment_pbar.set_description_str(f\"Task: {task.name}\")\n        constrained_ps: AlgoPruneScores = {}\n        algo_prune_scores = task_prune_scores[task_key]\n        for algo_key, algo_ps in (prune_score_pbar := tqdm(algo_prune_scores.items())):\n            if algo_key.startswith(\"Constrained\") or algo_key not in [\n                \"Official Circuit\",\n                \"Tree Probing\",\n            ]:\n                continue\n            sorted_edges: List[Edge] = list(\n                sorted(algo_ps.keys(), key=lambda x: abs(algo_ps[x]), reverse=True)\n            )\n            algo_circuit = set([e for e in sorted_edges[: task.true_edge_count]])\n            prune_score_pbar.set_description_str(f\"Constrained Pruning: {algo_key}\")\n            constrained_algo = PruneAlgo(\n                key=\"Constrained Circuit Probing \" + algo_key,\n                name=f\"Not {PRUNE_ALGO_DICT[algo_key].name} Circuit Probing\",\n                _short_name=f\"\u00ac{PRUNE_ALGO_DICT[algo_key].short_name} TP\",\n                func=partial(\n                    circuit_probing_prune_scores,\n                    learning_rate=0.1,\n                    epochs=2000,\n                    regularize_lambda=0.1,\n                    mask_fn=\"hard_concrete\",\n                    show_train_graph=True,\n                    circuit_sizes=[\"true_size\"],\n                    tree_optimisation=True,\n                    avoid_edges=algo_circuit,\n                    avoid_lambda=0.3,\n                ),\n            )\n            PRUNE_ALGO_DICT[constrained_algo.key] = constrained_algo\n            if constrained_algo.key not in algo_prune_scores:\n                print(f\"Running {constrained_algo.name}\")\n                constrained_ps[constrained_algo.key] = constrained_algo.func(\n                    task.model, task.train_loader, task.true_edges\n                )\n            else:\n                print(f\"Already ran {constrained_algo.name}\")\n        constrained_task_prune_scores[task_key] = constrained_ps\n    return constrained_task_prune_scores\n</code></pre>"},{"location":"reference/metrics/prune_scores_similarity/","title":"Prune scores similarity","text":""},{"location":"reference/metrics/prune_scores_similarity/#auto_circuit.metrics.prune_scores_similarity","title":"auto_circuit.metrics.prune_scores_similarity","text":""},{"location":"reference/metrics/prune_scores_similarity/#auto_circuit.metrics.prune_scores_similarity-attributes","title":"Attributes","text":""},{"location":"reference/metrics/prune_scores_similarity/#auto_circuit.metrics.prune_scores_similarity-functions","title":"Functions","text":""},{"location":"reference/metrics/prune_scores_similarity/#auto_circuit.metrics.prune_scores_similarity.prune_score_similarities","title":"prune_score_similarities","text":"<pre><code>prune_score_similarities(algo_prune_scores: AlgoPruneScores, edge_counts: List[int]) -&gt; Dict[int, Dict[AlgoKey, Dict[AlgoKey, float]]]\n</code></pre> <p>Measure the similarity between the prune scores of different algorithms. Similarity is measured as the proportion of the top N prune scores that are common between two algorithms.</p> <p>Parameters:</p> Name Type Description Default <code>algo_prune_scores</code> <code>AlgoPruneScores</code> <p>Prune scores for each algorithm.</p> required <code>edge_counts</code> <code>List[int]</code> <p>The number of edges in the circuit to consider.</p> required <p>Returns:</p> Type Description <code>Dict[int, Dict[AlgoKey, Dict[AlgoKey, float]]]</code> <p>A dictionary mapping from the number of edges considered to the overlap between <code>PruneScores</code> for each pair of algorithms.</p> Source code in <code>auto_circuit/metrics/prune_scores_similarity.py</code> <pre><code>def prune_score_similarities(\n    algo_prune_scores: AlgoPruneScores, edge_counts: List[int]\n) -&gt; Dict[int, Dict[AlgoKey, Dict[AlgoKey, float]]]:\n    \"\"\"\n    Measure the similarity between the prune scores of different algorithms. Similarity\n    is measured as the proportion of the top N prune scores that are common between two\n    algorithms.\n\n    Args:\n        algo_prune_scores: Prune scores for each algorithm.\n        edge_counts: The number of edges in the circuit to consider.\n\n    Returns:\n        A dictionary mapping from the number of edges considered to the overlap between\n            [`PruneScores`][auto_circuit.types.PruneScores] for each pair of algorithms.\n    \"\"\"\n    desc_ps: Dict[AlgoKey, t.Tensor] = {}\n    for algo_key, prune_scores in algo_prune_scores.items():\n        desc_ps[algo_key] = desc_prune_scores(prune_scores)\n\n    similarity_scores: Dict[int, Dict[AlgoKey, Dict[AlgoKey, float]]] = defaultdict(\n        lambda: defaultdict(dict)\n    )\n\n    for algo_key_1, desc_ps_1 in desc_ps.items():\n        for algo_key_2, desc_ps_2 in desc_ps.items():\n            if algo_key_2 in similarity_scores[edge_counts[0]]:\n                continue\n            for edge_count in edge_counts:\n                threshold_1 = prune_scores_threshold(desc_ps_1, edge_count)\n                threshold_2 = prune_scores_threshold(desc_ps_2, edge_count)\n                # Find the edges common to the top N prune scores for each algorithm\n                real_edge_count_1, real_edge_count_2 = 0, 0\n                common_edge_count = 0\n                for mod, ps_1 in algo_prune_scores[algo_key_1].items():\n                    ps_2 = algo_prune_scores[algo_key_2][mod]\n                    circuit_1 = ps_1.abs() &gt;= threshold_1\n                    circuit_2 = ps_2.abs() &gt;= threshold_2\n                    real_edge_count_1 += circuit_1.sum().item()\n                    real_edge_count_2 += circuit_2.sum().item()\n                    common_edge_count += (circuit_1 &amp; circuit_2).sum().item()\n\n                if real_edge_count_1 != edge_count or real_edge_count_2 != edge_count:\n                    similarity_scores[edge_count][algo_key_1][algo_key_2] = float(\"nan\")\n                else:\n                    similarity_scores[edge_count][algo_key_1][algo_key_2] = (\n                        common_edge_count / edge_count\n                    )\n\n    return similarity_scores\n</code></pre>"},{"location":"reference/metrics/prune_scores_similarity/#auto_circuit.metrics.prune_scores_similarity.prune_score_similarities_plotly","title":"prune_score_similarities_plotly","text":"<pre><code>prune_score_similarities_plotly(task_prune_scores: TaskPruneScores, edge_counts: List[int], ground_truths: bool = False) -&gt; Figure\n</code></pre> <p>Create a Plotly heatmap figure showing the similarity between the prune scores of different algorithms.</p> <p>See <code>prune_score_similarities</code></p> <p>Parameters:</p> Name Type Description Default <code>task_prune_scores</code> <code>TaskPruneScores</code> <p>Prune scores for each algorithm for each task.</p> required <code>edge_counts</code> <code>List[int]</code> <p>The number of edges in the circuit to consider.</p> required <code>ground_truths</code> <code>bool</code> <p>Whether to include the official circuit edge counts in addition to <code>edge_counts</code>.</p> <code>False</code> <p>Returns:</p> Type Description <code>Figure</code> <p>A Plotly figure.</p> Source code in <code>auto_circuit/metrics/prune_scores_similarity.py</code> <pre><code>def prune_score_similarities_plotly(\n    task_prune_scores: TaskPruneScores,\n    edge_counts: List[int],\n    ground_truths: bool = False,\n) -&gt; go.Figure:\n    \"\"\"\n    Create a Plotly heatmap figure showing the similarity between the prune scores of\n    different algorithms.\n\n    See\n    [`prune_score_similarities`][auto_circuit.metrics.prune_scores_similarity.prune_score_similarities]\n\n    Args:\n        task_prune_scores: Prune scores for each algorithm for each task.\n        edge_counts: The number of edges in the circuit to consider.\n        ground_truths: Whether to include the official circuit edge counts in addition\n            to `edge_counts`.\n\n    Returns:\n        A Plotly figure.\n    \"\"\"\n    sims = task_prune_scores_similarities(task_prune_scores, edge_counts, ground_truths)\n\n    row_count = len(sims)\n    col_count = len(edge_counts) + (1 if ground_truths else 0)\n    algo_count = 0\n    fig = subplots.make_subplots(\n        rows=row_count,\n        cols=col_count,\n        shared_xaxes=True,\n        shared_yaxes=True,\n        row_titles=[TASK_DICT[task_key].name for task_key in sims.keys()],\n        column_titles=([\"|Ground Truth| Edges\"] if ground_truths else [])\n        + [f\"{edge_count} Edges\" for edge_count in edge_counts],\n    )\n    for task_idx, edge_count_sims in enumerate(sims.values()):\n        for count_idx, algo_sims in enumerate(edge_count_sims.values()):\n            algo_count = len(algo_sims)\n            x_strs = [PRUNE_ALGO_DICT[a].short_name for a in reversed(algo_sims.keys())]\n            y_strs = [PRUNE_ALGO_DICT[algo].short_name for algo in algo_sims.keys()]\n            heatmap = []\n            for similarity_dict in algo_sims.values():\n                row = [sim_score for sim_score in similarity_dict.values()]\n                heatmap.append(list(reversed(row)))\n            fig.add_trace(\n                go.Heatmap(\n                    x=x_strs,\n                    y=y_strs,\n                    z=heatmap,\n                    colorscale=\"Viridis\",\n                    showscale=False,\n                    text=heatmap,\n                    texttemplate=\"%{text:.0%}\",\n                    textfont={\"size\": 19},\n                ),\n                row=task_idx + 1,\n                col=count_idx + 1,\n            )\n    # fig.update_layout(yaxis_scaleanchor=\"x\")\n    fig.update_layout(plot_bgcolor=\"rgba(0,0,0,0)\")\n    fig.update_layout(\n        width=col_count * 70 * algo_count + 100,\n        height=row_count * 50 * algo_count + 100,\n    )\n    return fig\n</code></pre>"},{"location":"reference/metrics/prune_scores_similarity/#auto_circuit.metrics.prune_scores_similarity.task_prune_scores_similarities","title":"task_prune_scores_similarities","text":"<pre><code>task_prune_scores_similarities(task_prune_scores: TaskPruneScores, edge_counts: List[int], true_edge_counts: bool = False) -&gt; Dict[TaskKey, Dict[int, Dict[AlgoKey, Dict[AlgoKey, float]]]]\n</code></pre> <p>Wrapper around <code>prune_score_similarities</code> for a set of tasks.</p> <p>Parameters:</p> Name Type Description Default <code>true_edge_counts</code> <code>bool</code> <p>Whether to include the official circuit edge count in addition to <code>edge_counts</code>.</p> <code>False</code> Source code in <code>auto_circuit/metrics/prune_scores_similarity.py</code> <pre><code>def task_prune_scores_similarities(\n    task_prune_scores: TaskPruneScores,\n    edge_counts: List[int],\n    true_edge_counts: bool = False,\n) -&gt; Dict[TaskKey, Dict[int, Dict[AlgoKey, Dict[AlgoKey, float]]]]:\n    \"\"\"\n    Wrapper around\n    [`prune_score_similarities`][auto_circuit.metrics.prune_scores_similarity.prune_score_similarities]\n    for a set of tasks.\n\n    Args:\n        true_edge_counts: Whether to include the official circuit edge count in addition\n            to `edge_counts`.\n    \"\"\"\n    task_similarity: Dict[TaskKey, Dict[int, Dict[AlgoKey, Dict[AlgoKey, float]]]] = {}\n\n    for task_key, algo_prune_scores in task_prune_scores.items():\n        task = TASK_DICT[task_key]\n        true_edge_count = []\n        if true_edge_counts:\n            assert task.true_edges is not None\n            true_edge_count = [len(task.true_edges)]\n        task_similarity[task_key] = prune_score_similarities(\n            algo_prune_scores, true_edge_count + edge_counts\n        )\n\n    return task_similarity\n</code></pre>"},{"location":"reference/metrics/completeness_metrics/same_under_knockouts/","title":"Same under knockouts","text":""},{"location":"reference/metrics/completeness_metrics/same_under_knockouts/#auto_circuit.metrics.completeness_metrics.same_under_knockouts","title":"auto_circuit.metrics.completeness_metrics.same_under_knockouts","text":""},{"location":"reference/metrics/completeness_metrics/same_under_knockouts/#auto_circuit.metrics.completeness_metrics.same_under_knockouts-attributes","title":"Attributes","text":""},{"location":"reference/metrics/completeness_metrics/same_under_knockouts/#auto_circuit.metrics.completeness_metrics.same_under_knockouts.AlgoCompletenessScores","title":"AlgoCompletenessScores  <code>module-attribute</code>","text":"<pre><code>AlgoCompletenessScores = Dict[AlgoKey, CompletenessScores]\n</code></pre> <p><code>CompletenessScores</code> for each algorithm.</p>"},{"location":"reference/metrics/completeness_metrics/same_under_knockouts/#auto_circuit.metrics.completeness_metrics.same_under_knockouts.CompletenessScores","title":"CompletenessScores  <code>module-attribute</code>","text":"<pre><code>CompletenessScores = List[Tuple[int, int, float, float]]\n</code></pre> <p>A list of tuples containing:</p> <ol> <li>The size of the circuit.</li> <li>The number of knockouts.</li> <li>The KL divergence between the circuit and the full model.</li> <li>The KL divergence between the circuit with knockouts and the full model with     knockouts.</li> </ol>"},{"location":"reference/metrics/completeness_metrics/same_under_knockouts/#auto_circuit.metrics.completeness_metrics.same_under_knockouts.TaskCompletenessScores","title":"TaskCompletenessScores  <code>module-attribute</code>","text":"<pre><code>TaskCompletenessScores = Dict[TaskKey, AlgoCompletenessScores]\n</code></pre> <p><code>AlgoCompletenessScores</code> for each task and algorithm.</p>"},{"location":"reference/metrics/completeness_metrics/same_under_knockouts/#auto_circuit.metrics.completeness_metrics.same_under_knockouts-classes","title":"Classes","text":""},{"location":"reference/metrics/completeness_metrics/same_under_knockouts/#auto_circuit.metrics.completeness_metrics.same_under_knockouts-functions","title":"Functions","text":""},{"location":"reference/metrics/completeness_metrics/same_under_knockouts/#auto_circuit.metrics.completeness_metrics.same_under_knockouts.measure_same_under_knockouts","title":"measure_same_under_knockouts","text":"<pre><code>measure_same_under_knockouts(circuit_ps: TaskPruneScores, knockout_ps: TaskPruneScores) -&gt; TaskCompletenessScores\n</code></pre> <p>Wrapper of <code>same_under_knockout</code> for each task and algorithm in <code>circuit_ps</code> and <code>knockout_ps</code>.</p> Source code in <code>auto_circuit/metrics/completeness_metrics/same_under_knockouts.py</code> <pre><code>def measure_same_under_knockouts(\n    circuit_ps: TaskPruneScores,\n    knockout_ps: TaskPruneScores,\n) -&gt; TaskCompletenessScores:\n    \"\"\"\n    Wrapper of\n    [`same_under_knockout`][auto_circuit.metrics.completeness_metrics.same_under_knockouts.same_under_knockout]\n    for each task and algorithm in `circuit_ps` and `knockout_ps`.\n    \"\"\"\n    task_completeness_scores: TaskCompletenessScores = {}\n    for task_key, algos_ko_ps in (task_pbar := tqdm(knockout_ps.items())):\n        task = TASK_DICT[task_key]\n        assert task.true_edge_count is not None\n        true_circuit_size: int = task.true_edge_count\n        task_pbar.set_description_str(f\"Task: {task.name}\")\n        algo_completeness_scores: AlgoCompletenessScores = {}\n        for algo_key, algo_ko_ps in (algo_pbar := tqdm(algos_ko_ps.items())):\n            algo = PRUNE_ALGO_DICT[algo_key]\n            algo_pbar.set_description_str(f\"Algo: {algo.name}\")\n            print()\n            print(\"task\", task.name, \"algo\", algo.name)\n            same_under_knockouts: CompletenessScores = same_under_knockout(\n                task=task,\n                circuit_ps=circuit_ps[task_key][algo_key],\n                knockout_ps=algo_ko_ps,\n                circuit_size=true_circuit_size,\n            )\n            algo_completeness_scores[algo_key] = same_under_knockouts\n        task_completeness_scores[task_key] = algo_completeness_scores\n    return task_completeness_scores\n</code></pre>"},{"location":"reference/metrics/completeness_metrics/same_under_knockouts/#auto_circuit.metrics.completeness_metrics.same_under_knockouts.same_under_knockout","title":"same_under_knockout","text":"<pre><code>same_under_knockout(task: Task, circuit_ps: PruneScores, knockout_ps: PruneScores, circuit_size: int, knockout_threshold: float = 0.0) -&gt; CompletenessScores\n</code></pre> <p>Given a circuit and a set of edges to ablate, measure the difference in KL divergence between the circuit and the full model with and without the knockouts.</p> <p>This is the measure of completeness introduced by Wang et al. (2022) to test the IOI circuit.</p> <p>The optimization process that attempts to find the knockouts that maximize the difference is implemented separately in <code>train_same_under_knockout_prune_scores</code>.</p> <p>Parameters:</p> Name Type Description Default <code>task</code> <code>Task</code> <p>The task to measure the completeness for.</p> required <code>circuit_ps</code> <code>PruneScores</code> <p>The circuit to test. The top <code>circuit_size</code> edges are taken to be the circuit.</p> required <code>knockout_ps</code> <code>PruneScores</code> <p>The set of knockouts to test. All edges with scores greater than <code>knockout_threshold</code> are knocked out.</p> required <code>circuit_size</code> <code>int</code> <p>The size of the circuit.</p> required <code>knockout_threshold</code> <code>float</code> <p>The threshold for knockout edges.</p> <code>0.0</code> <p>Returns:</p> Type Description <code>CompletenessScores</code> <p>Tuple of completeness information.</p> Source code in <code>auto_circuit/metrics/completeness_metrics/same_under_knockouts.py</code> <pre><code>def same_under_knockout(\n    task: Task,\n    circuit_ps: PruneScores,\n    knockout_ps: PruneScores,\n    circuit_size: int,\n    knockout_threshold: float = 0.0,\n) -&gt; CompletenessScores:\n    \"\"\"\n    Given a circuit and a set of edges to ablate, measure the difference in KL\n    divergence between the circuit and the full model with and without the knockouts.\n\n    This is the measure of completeness introduced by [Wang et al.\n    (2022)](https://arxiv.org/abs/2211.00593) to test the IOI circuit.\n\n    The optimization process that attempts to find the knockouts that maximize the\n    difference is implemented separately in\n    [`train_same_under_knockout_prune_scores`][auto_circuit.metrics.completeness_metrics.train_same_under_knockouts.train_same_under_knockout_prune_scores].\n\n    Args:\n        task: The task to measure the completeness for.\n        circuit_ps: The circuit to test. The top `circuit_size` edges are taken to be\n            the circuit.\n        knockout_ps: The set of knockouts to test. All edges with scores greater than\n            `knockout_threshold` are knocked out.\n        circuit_size: The size of the circuit.\n        knockout_threshold: The threshold for knockout edges.\n\n    Returns:\n        Tuple of completeness information.\n    \"\"\"\n    model = task.model\n    patch_masks: Dict[str, t.nn.Parameter] = model.patch_masks\n    circuit_threshold = prune_scores_threshold(circuit_ps, circuit_size)\n\n    corrupt_src_outs: Dict[BatchKey, t.Tensor] = batch_src_ablations(\n        model,\n        task.test_loader,\n        ablation_type=AblationType.RESAMPLE,\n        clean_corrupt=\"corrupt\",\n    )\n\n    mask_params = list(patch_masks.values())\n    # Make a boolean copy of the patch_masks that encodes the circuit\n    circ_masks = [circuit_ps[m].abs() &gt;= circuit_threshold for m in patch_masks.keys()]\n    actual_circuit_size: int = int(sum([mask.sum().item() for mask in circ_masks]))\n    knockout_masks = [knockout_ps[m] &gt;= knockout_threshold for m in patch_masks.keys()]\n    # assert actual_circuit_size == circuit_size\n\n    # Test the circuit with the knockouts\n    ko_circ_logprobs, ko_model_logprobs = {}, {}\n    circ_logprobs, model_logprobs = {}, {}\n    with t.no_grad():\n        # Discretize the circuit with knockouts\n        for circ, knockout, patch in zip(circ_masks, knockout_masks, mask_params):\n            # Patch edges where learned mask is greater than knockout_threshold\n            patch.data = knockout.float()\n            # Also patch edges not in the circuit\n            t.where(circ, patch.data, t.ones_like(patch.data), out=patch.data)\n        # Test the circuit with the knockouts\n        for batch in task.test_loader:\n            patch_outs = corrupt_src_outs[batch.key].clone().detach()\n            with patch_mode(model, patch_outs):\n                model_out = model(batch.clean)[model.out_slice]\n                ko_circ_logprobs[batch.key] = log_softmax(model_out, dim=-1)\n\n        # Test the full model with the same knockouts\n        for circ, patch in zip(circ_masks, mask_params):\n            # Don't patch edges not in the circuit (but keep patches in the circuit)\n            t.where(circ, patch.data, t.zeros_like(patch.data), out=patch.data)\n        knockouts_size = int(sum([mask.sum().item() for mask in mask_params]))\n        for batch in task.test_loader:\n            patch_outs = corrupt_src_outs[batch.key].clone().detach()\n            with patch_mode(model, patch_outs):\n                model_out = model(batch.clean)[model.out_slice]\n                ko_model_logprobs[batch.key] = log_softmax(model_out, dim=-1)\n\n        # Test the circuit without knockouts (with tree patching)\n        for circ, patch in zip(circ_masks, mask_params):\n            # Patch every edge not in the circuit\n            patch.data = (~circ).float()\n        for batch in task.test_loader:\n            patch_outs = corrupt_src_outs[batch.key].clone().detach()\n            with patch_mode(model, patch_outs):\n                model_out = model(batch.clean)[model.out_slice]\n                circ_logprobs[batch.key] = log_softmax(model_out, dim=-1)\n\n        # Test the full model without knockouts\n        for batch in task.test_loader:\n            model_out = model(batch.clean)[model.out_slice]\n            model_logprobs[batch.key] = log_softmax(model_out, dim=-1)\n\n    # Sort the logprobs by batch key and stack them\n    ko_circ_logprobs_ten = t.stack([o for _, o in sorted(ko_circ_logprobs.items())])\n    ko_model_logprobs_ten = t.stack([o for _, o in sorted(ko_model_logprobs.items())])\n    knockout_kl = multibatch_kl_div(ko_circ_logprobs_ten, ko_model_logprobs_ten).item()\n\n    circ_logprobs_ten = t.stack([o for _, o in sorted(circ_logprobs.items())])\n    model_logprobs_ten = t.stack([o for _, o in sorted(model_logprobs.items())])\n    normal_kl = multibatch_kl_div(circ_logprobs_ten, model_logprobs_ten).item()\n\n    return [(actual_circuit_size, knockouts_size, normal_kl, knockout_kl)]\n</code></pre>"},{"location":"reference/metrics/completeness_metrics/same_under_knockouts/#auto_circuit.metrics.completeness_metrics.same_under_knockouts.same_under_knockouts_fig","title":"same_under_knockouts_fig","text":"<pre><code>same_under_knockouts_fig(task_completeness_scores: TaskCompletenessScores) -&gt; Figure\n</code></pre> <p>Create a plotly figure showing the difference in KL divergence between the circuit and the full model with and without knockouts for each task and algorithm.</p> <p>Parameters:</p> Name Type Description Default <code>task_completeness_scores</code> <code>TaskCompletenessScores</code> <p>The completeness scores for each task and algorithm.</p> required <p>Returns:</p> Type Description <code>Figure</code> <p>The plotly figure.</p> Source code in <code>auto_circuit/metrics/completeness_metrics/same_under_knockouts.py</code> <pre><code>def same_under_knockouts_fig(\n    task_completeness_scores: TaskCompletenessScores,\n) -&gt; go.Figure:\n    \"\"\"\n    Create a plotly figure showing the difference in KL divergence between the circuit\n    and the full model with and without knockouts for each task and algorithm.\n\n    Args:\n        task_completeness_scores: The completeness scores for each task and algorithm.\n\n    Returns:\n        The plotly figure.\n    \"\"\"\n    n_cols = len(task_completeness_scores)\n    titles = [TASK_DICT[task].name for task in task_completeness_scores.keys()]\n    fig = subplots.make_subplots(rows=1, cols=n_cols, subplot_titles=titles)\n    for col, algo_comp_scores in enumerate(task_completeness_scores.values(), start=1):\n        xs = []\n        for algo_key, completeness_scores in algo_comp_scores.items():\n            for (circ_size, n_knockouts, x, y) in completeness_scores:\n                new_xs = [x, (x + y) / 2]\n                xs.extend(new_xs)\n                scatter = go.Scatter(\n                    x=new_xs,\n                    y=[y, (x + y) / 2],\n                    name=PRUNE_ALGO_DICT[algo_key].name,\n                    hovertext=[f\"circ size: {circ_size}&lt;br&gt;n_knockouts: {n_knockouts}\"],\n                    mode=\"lines+markers\",\n                    showlegend=(col == 1),\n                )\n                fig.add_trace(scatter, row=1, col=col)\n        # Add line y=x without changing the axis limits\n        scatter = go.Scatter(\n            x=[min(xs), max(xs)],\n            y=[min(xs), max(xs)],\n            name=\"y=x\",\n            mode=\"lines\",\n            line=dict(dash=\"dash\"),\n            showlegend=(col == 1),\n        )\n        fig.add_trace(scatter, row=1, col=col)\n    fig.update_layout(\n        title=\"Same Under Knockouts\",\n        xaxis_title=\"KL Div\",\n        yaxis_title=\"Knockout KL Div\",\n        width=1300,\n    )\n    return fig\n</code></pre>"},{"location":"reference/metrics/completeness_metrics/train_same_under_knockouts/","title":"Train same under knockouts","text":""},{"location":"reference/metrics/completeness_metrics/train_same_under_knockouts/#auto_circuit.metrics.completeness_metrics.train_same_under_knockouts","title":"auto_circuit.metrics.completeness_metrics.train_same_under_knockouts","text":""},{"location":"reference/metrics/completeness_metrics/train_same_under_knockouts/#auto_circuit.metrics.completeness_metrics.train_same_under_knockouts-attributes","title":"Attributes","text":""},{"location":"reference/metrics/completeness_metrics/train_same_under_knockouts/#auto_circuit.metrics.completeness_metrics.train_same_under_knockouts-classes","title":"Classes","text":""},{"location":"reference/metrics/completeness_metrics/train_same_under_knockouts/#auto_circuit.metrics.completeness_metrics.train_same_under_knockouts-functions","title":"Functions","text":""},{"location":"reference/metrics/completeness_metrics/train_same_under_knockouts/#auto_circuit.metrics.completeness_metrics.train_same_under_knockouts.train_same_under_knockout_prune_scores","title":"train_same_under_knockout_prune_scores","text":"<pre><code>train_same_under_knockout_prune_scores(task: Task, algo: PruneAlgo, algo_ps: PruneScores, circuit_size: int, learning_rate: float, epochs: int, regularize_lambda: float, mask_fn: MaskFn = 'hard_concrete', faithfulness_target: Literal['kl_div', 'logit_diff'] = 'kl_div') -&gt; PruneScores\n</code></pre> <p>Learn a subset of the circuit to ablate such that when the same edges are ablated from the full model, the KL divergence between the circuit and the full model is maximized.</p> <p>See: <code>same_under_knockouts</code></p> <p>Parameters:</p> Name Type Description Default <code>task</code> <code>Task</code> <p>The task to train on.</p> required <code>algo</code> <code>PruneAlgo</code> <p>The pruning algorithm used to generate the circuit. This value is only used for visualization purposes.</p> required <code>algo_ps</code> <code>PruneScores</code> <p>The pruning scores for the algorithm. The circuit is defined as the top <code>circuit_size</code> edges according to these scores.</p> required <code>circuit_size</code> <code>int</code> <p>The size of the circuit to knockout.</p> required <code>learning_rate</code> <code>float</code> <p>The learning rate for the optimization.</p> required <code>epochs</code> <code>int</code> <p>The number of epochs to train for.</p> required <code>regularize_lambda</code> <code>float</code> <p>The regularization strength for the number of edges that are knocked out. Can reasonably be set to 0.</p> required <code>mask_fn</code> <code>MaskFn</code> <p>The mask parameterization to use for the optimization. <code>hard_concrete</code> is highly recommended.</p> <code>'hard_concrete'</code> <code>faithfulness_target</code> <code>Literal['kl_div', 'logit_diff']</code> <p>The target for the faithfulness term in the loss. The optimizer will try to maximize the difference in this target between the knocked-out circuit and the knocked-out full model.</p> <code>'kl_div'</code> <p>Returns:</p> Type Description <code>PruneScores</code> <p>The learned ordering of edges to knockout.</p> Source code in <code>auto_circuit/metrics/completeness_metrics/train_same_under_knockouts.py</code> <pre><code>def train_same_under_knockout_prune_scores(\n    task: Task,\n    algo: PruneAlgo,\n    algo_ps: PruneScores,\n    circuit_size: int,\n    learning_rate: float,\n    epochs: int,\n    regularize_lambda: float,\n    mask_fn: MaskFn = \"hard_concrete\",\n    faithfulness_target: Literal[\"kl_div\", \"logit_diff\"] = \"kl_div\",\n) -&gt; PruneScores:\n    \"\"\"\n    Learn a subset of the circuit to ablate such that when the same edges are ablated\n    from the full model, the KL divergence between the circuit and the full model is\n    maximized.\n\n    See:\n    [`same_under_knockouts`][auto_circuit.metrics.completeness_metrics.same_under_knockouts.same_under_knockout]\n\n    Args:\n        task: The task to train on.\n        algo: The pruning algorithm used to generate the circuit. This value is only\n            used for visualization purposes.\n        algo_ps: The pruning scores for the algorithm. The circuit is defined as the\n            top `circuit_size` edges according to these scores.\n        circuit_size: The size of the circuit to knockout.\n        learning_rate: The learning rate for the optimization.\n        epochs: The number of epochs to train for.\n        regularize_lambda: The regularization strength for the number of edges that are\n            knocked out. Can reasonably be set to 0.\n        mask_fn: The mask parameterization to use for the optimization. `hard_concrete`\n            is highly recommended.\n        faithfulness_target: The target for the faithfulness term in the loss. The\n            optimizer will try to maximize the difference in this target between the\n            knocked-out circuit and the knocked-out full model.\n\n    Returns:\n        The learned ordering of edges to knockout.\n    \"\"\"\n    circuit_threshold = prune_scores_threshold(algo_ps, circuit_size)\n    model = task.model\n    n_target = int(circuit_size / 5)\n\n    corrupt_src_outs: Dict[BatchKey, t.Tensor] = batch_src_ablations(\n        model,\n        task.test_loader,\n        ablation_type=AblationType.RESAMPLE,\n        clean_corrupt=\"corrupt\",\n    )\n\n    loss_history, faith_history, reg_history = [], [], []\n    with train_mask_mode(model) as patch_masks:\n        mask_params = list(patch_masks.values())\n        set_all_masks(model, val=0.0)\n\n        # Make a boolean copy of the patch_masks that encodes the circuit\n        circ_masks = [algo_ps[m].abs() &gt;= circuit_threshold for m in patch_masks.keys()]\n        actual_circuit_size = sum([mask.sum().item() for mask in circ_masks])\n        print(\"actual_circuit_size\", actual_circuit_size, \"circuit_size\", circuit_size)\n        # assert actual_circuit_size == circuit_size\n\n        set_all_masks(model, val=-init_mask_val)\n        optim = t.optim.Adam(mask_params, lr=learning_rate)\n        for epoch in (epoch_pbar := tqdm(range(epochs))):\n            kl_str = faith_history[-1] if len(faith_history) &gt; 0 else None\n            epoch_pbar.set_description_str(f\"Epoch: {epoch}, KL Div: {kl_str}\")\n            for batch in task.test_loader:\n                patches = corrupt_src_outs[batch.key].clone().detach()\n                with patch_mode(model, patches), mask_fn_mode(model, mask_fn):\n                    optim.zero_grad()\n                    model.zero_grad()\n\n                    # Patch all the edges not in the circuit\n                    with t.no_grad():\n                        for circ, patch in zip(circ_masks, mask_params):\n                            patch_all = t.full_like(patch.data, 99)\n                            t.where(circ, patch.data, patch_all, out=patch.data)\n                    circ_out = model(batch.clean)[model.out_slice]\n\n                    # Don't patch edges not in the circuit\n                    with t.no_grad():\n                        for cir, patch in zip(circ_masks, mask_params):\n                            patch_none = t.full_like(patch.data, -99)\n                            t.where(cir, patch.data, patch_none, out=patch.data)\n                    model_out = model(batch.clean)[model.out_slice]\n\n                    if faithfulness_target == \"kl_div\":\n                        circuit_logprobs = log_softmax(circ_out, dim=-1)\n                        model_logprobs = log_softmax(model_out, dim=-1)\n                        faith = -multibatch_kl_div(circuit_logprobs, model_logprobs)\n                    else:\n                        assert faithfulness_target == \"logit_diff\"\n                        circ_logit_diff = batch_avg_answer_diff(circ_out, batch)\n                        model_logit_diff = batch_avg_answer_diff(model_out, batch)\n                        faith = -(model_logit_diff - circ_logit_diff)\n                    faith_history.append(faith.item())\n\n                    flat_masks = t.cat([mask.flatten() for mask in mask_params])\n                    knockouts_samples = sample_hard_concrete(flat_masks, batch_size=1)\n                    reg_term = t.relu(knockouts_samples.sum() - n_target) / n_target\n                    reg_history.append(reg_term.item() * regularize_lambda)\n\n                    loss = faith + reg_term * regularize_lambda\n                    loss.backward()\n                    loss_history.append(loss.item())\n                    optim.step()\n\n        fig = go.Figure()\n        fig.add_trace(go.Scatter(y=loss_history, name=\"Loss\"))\n        fig.add_trace(go.Scatter(y=faith_history, name=faithfulness_target))\n        fig.add_trace(go.Scatter(y=reg_history, name=\"Regularization\"))\n        fig.update_layout(\n            title=f\"Same Under Knockouts for Task: {task.name}, Algo: {algo.name}\"\n        )\n        fig.show()\n\n    completeness_prune_scores: PruneScores = {}\n    for mod_name, patch_mask in model.patch_masks.items():\n        completeness_prune_scores[mod_name] = patch_mask.detach().clone()\n    return completeness_prune_scores\n</code></pre>"},{"location":"reference/metrics/completeness_metrics/train_same_under_knockouts/#auto_circuit.metrics.completeness_metrics.train_same_under_knockouts.train_same_under_knockouts","title":"train_same_under_knockouts","text":"<pre><code>train_same_under_knockouts(task_prune_scores: TaskPruneScores, algo_keys: List[AlgoKey], learning_rate: float, epochs: int, regularize_lambda: float, faithfulness_target: Literal['kl_div', 'logit_diff'] = 'kl_div') -&gt; TaskPruneScores\n</code></pre> <p>Wrapper of <code>train_same_under_knockout_prune_scores</code> for multiple tasks and algorithms.</p> Source code in <code>auto_circuit/metrics/completeness_metrics/train_same_under_knockouts.py</code> <pre><code>def train_same_under_knockouts(\n    task_prune_scores: TaskPruneScores,\n    algo_keys: List[AlgoKey],\n    learning_rate: float,\n    epochs: int,\n    regularize_lambda: float,\n    faithfulness_target: Literal[\"kl_div\", \"logit_diff\"] = \"kl_div\",\n) -&gt; TaskPruneScores:\n    \"\"\"\n    Wrapper of\n    [`train_same_under_knockout_prune_scores`][auto_circuit.metrics.completeness_metrics.train_same_under_knockouts.train_same_under_knockout_prune_scores]\n    for multiple tasks and algorithms.\n    \"\"\"\n    task_completeness_scores: TaskPruneScores = {}\n    for task_key, algo_prune_scores in (task_pbar := tqdm(task_prune_scores.items())):\n        task = TASK_DICT[task_key]\n        # if task_key != IOI_TOKEN_CIRCUIT_TASK.key:\n        #     continue\n        assert task.true_edge_count is not None\n        true_circuit_size: int = task.true_edge_count\n        task_pbar.set_description_str(f\"Task: {task.name}\")\n        algo_completeness_scores: AlgoPruneScores = {}\n        for algo_key, prune_scores in (algo_pbar := tqdm(algo_prune_scores.items())):\n            # if algo_key not in algo_keys:\n            #     print(\"skipping algo\", algo_key)\n            #     continue\n            algo = PRUNE_ALGO_DICT[algo_key]\n            algo_pbar.set_description_str(f\"Algo: {algo.name}\")\n\n            same_under_knockouts: PruneScores = train_same_under_knockout_prune_scores(\n                task=task,\n                algo=algo,\n                algo_ps=prune_scores,\n                circuit_size=true_circuit_size,\n                learning_rate=learning_rate,\n                epochs=epochs,\n                regularize_lambda=regularize_lambda,\n                faithfulness_target=faithfulness_target,\n            )\n            algo_completeness_scores[algo_key] = same_under_knockouts\n        task_completeness_scores[task_key] = algo_completeness_scores\n    return task_completeness_scores\n</code></pre>"},{"location":"reference/metrics/official_circuits/measure_roc/","title":"Measure roc","text":""},{"location":"reference/metrics/official_circuits/measure_roc/#auto_circuit.metrics.official_circuits.measure_roc","title":"auto_circuit.metrics.official_circuits.measure_roc","text":""},{"location":"reference/metrics/official_circuits/measure_roc/#auto_circuit.metrics.official_circuits.measure_roc-attributes","title":"Attributes","text":""},{"location":"reference/metrics/official_circuits/measure_roc/#auto_circuit.metrics.official_circuits.measure_roc-classes","title":"Classes","text":""},{"location":"reference/metrics/official_circuits/measure_roc/#auto_circuit.metrics.official_circuits.measure_roc-functions","title":"Functions","text":""},{"location":"reference/metrics/official_circuits/measure_roc/#auto_circuit.metrics.official_circuits.measure_roc.measure_roc","title":"measure_roc","text":"<pre><code>measure_roc(task_prune_scores: TaskPruneScores) -&gt; TaskMeasurements\n</code></pre> <p>Wrapper of <code>measure_task_roc</code> that measures the ROC curve for each task and algorithm.</p> Source code in <code>auto_circuit/metrics/official_circuits/measure_roc.py</code> <pre><code>def measure_roc(task_prune_scores: TaskPruneScores) -&gt; TaskMeasurements:\n    \"\"\"\n    Wrapper of\n    [`measure_task_roc`][auto_circuit.metrics.official_circuits.measure_roc.measure_task_roc]\n    that measures the ROC curve for each task and algorithm.\n    \"\"\"\n    task_measurements: TaskMeasurements = {}\n    for task_key, algo_prune_scores in (task_pbar := tqdm(task_prune_scores.items())):\n        task = TASK_DICT[task_key]\n        task_pbar.set_description_str(f\"Measuring ROC Task: {task.name}\")\n        algo_measurements: AlgoMeasurements = {}\n        for algo_key, prune_scores in (algo_pbar := tqdm(algo_prune_scores.items())):\n            algo_pbar.set_description_str(f\"Measuring ROC Pruning with {algo_key}\")\n            official_edge = task.model.true_edges\n            if official_edge is None:\n                raise ValueError(\"This task does not have a true edge function\")\n            algo_measurement = measure_task_roc(task.model, official_edge, prune_scores)\n            algo_measurements[algo_key] = algo_measurement\n        task_measurements[task_key] = algo_measurements\n    return task_measurements\n</code></pre>"},{"location":"reference/metrics/official_circuits/measure_roc/#auto_circuit.metrics.official_circuits.measure_roc.measure_task_roc","title":"measure_task_roc","text":"<pre><code>measure_task_roc(model: PatchableModel, official_edges: Set[Edge], prune_scores: PruneScores, all_edges: bool = False) -&gt; Measurements\n</code></pre> <p>Finds points for the Receiver Operating Characteristic (ROC) curve that measures the performance of the given <code>prune_scores</code> at classifying which edges are in or out of the circuit defined by <code>official_edges</code>.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>PatchableModel</code> <p>The model to measure the ROC curve for.</p> required <code>official_edges</code> <code>Set[Edge]</code> <p>The edges that define the correct circuit.</p> required <code>prune_scores</code> <code>PruneScores</code> <p>The pruning scores to measure the ROC curve for. The scores define an ordering of the edges in the model. We sweep through the scores in descending order, including the top-k edges in the circuit.</p> required <code>all_edges</code> <code>bool</code> <p>By default we calculate the True Positive Rate (TRP) and False Positive Rate (FPR) for the set of [number of edges] determined by passing <code>prune_scores</code> to <code>edge_counts_util</code>. If <code>all_edges</code> is <code>True</code>, we instead calculate the TRP and FPR for every number <code>0, 1, 2, ..., model.n_edges</code> of edges.</p> <code>False</code> <p>Returns:</p> Type Description <code>Measurements</code> <p>A list of points that define the ROC curve. Each point is a tuple of the form <code>(FPR, TPR)</code>. The first point is always <code>(0, 0)</code> and the last point is always <code>(1, 1)</code>.</p> Source code in <code>auto_circuit/metrics/official_circuits/measure_roc.py</code> <pre><code>def measure_task_roc(\n    model: PatchableModel,\n    official_edges: Set[Edge],\n    prune_scores: PruneScores,\n    all_edges: bool = False,\n) -&gt; Measurements:\n    \"\"\"\n    Finds points for the Receiver Operating Characteristic (ROC) curve that\n    measures the performance of the given `prune_scores` at classifying which edges\n    are in or out of the circuit defined by `official_edges`.\n\n    Args:\n        model: The model to measure the ROC curve for.\n        official_edges: The edges that define the correct circuit.\n        prune_scores: The pruning scores to measure the ROC curve for. The scores\n            define an ordering of the edges in the model. We sweep through the scores\n            in descending order, including the top-k edges in the circuit.\n        all_edges: By default we calculate the True Positive Rate (TRP) and False\n            Positive Rate (FPR) for the set of [number of edges] determined by passing\n            `prune_scores` to\n            [`edge_counts_util`][auto_circuit.utils.graph_utils.edge_counts_util]. If\n            `all_edges` is `True`, we instead calculate the TRP and FPR for every number\n            `0, 1, 2, ..., model.n_edges` of edges.\n\n    Returns:\n        A list of points that define the ROC curve. Each point is a tuple of the form\n            `(FPR, TPR)`. The first point is always `(0, 0)` and the last point is\n            always `(1, 1)`.\n    \"\"\"\n    n_official = len(official_edges)\n    n_complement = model.n_edges - n_official\n    if all_edges:\n        test_edge_counts = edge_counts_util(model.edges, test_counts=EdgeCounts.ALL)\n    else:\n        test_edge_counts = edge_counts_util(model.edges, prune_scores=prune_scores)\n    desc_ps = desc_prune_scores(prune_scores)\n\n    correct_ps: PruneScores = model.circuit_prune_scores(official_edges, bool=True)\n    incorrect_edges: PruneScores = dict([(mod, ~ps) for mod, ps in correct_ps.items()])\n\n    points: List[Tuple[float, float]] = [(0.0, 0.0)]\n    for edge_count in (edge_count_pbar := tqdm(test_edge_counts)):\n        edge_count_pbar.set_description_str(f\"ROC for {edge_count} edges\")\n        threshold = prune_scores_threshold(desc_ps, edge_count)\n        true_positives, false_positives = 0, 0\n        for mod, ps in prune_scores.items():\n            ps_circuit = ps.abs() &gt;= threshold\n            correct_circuit, incorrect_circuit = correct_ps[mod], incorrect_edges[mod]\n            true_positives += (ps_circuit &amp; correct_circuit).sum().item()\n            false_positives += (ps_circuit &amp; incorrect_circuit).sum().item()\n        true_positive_rate = true_positives / n_official\n        false_positive_rate = false_positives / n_complement\n        points.append((false_positive_rate, true_positive_rate))\n    points.append((1.0, 1.0))\n    return points\n</code></pre>"},{"location":"reference/metrics/official_circuits/roc_plot/","title":"Roc plot","text":""},{"location":"reference/metrics/official_circuits/roc_plot/#auto_circuit.metrics.official_circuits.roc_plot","title":"auto_circuit.metrics.official_circuits.roc_plot","text":""},{"location":"reference/metrics/official_circuits/roc_plot/#auto_circuit.metrics.official_circuits.roc_plot-attributes","title":"Attributes","text":""},{"location":"reference/metrics/official_circuits/roc_plot/#auto_circuit.metrics.official_circuits.roc_plot-functions","title":"Functions","text":""},{"location":"reference/metrics/official_circuits/roc_plot/#auto_circuit.metrics.official_circuits.roc_plot.roc_plot","title":"roc_plot","text":"<pre><code>roc_plot(taskname_measurements: Dict[str, Dict[str, Measurements]], variable_width: bool = False) -&gt; Figure\n</code></pre> <p>Plots the pessimistic Receiver Operating Characteristic (ROC) curve for a nested dictionary of measurements. The outer dictionary has keys that are the task names and the inner dictionary has keys that are the algorithm names.</p> <p>Parameters:</p> Name Type Description Default <code>taskname_measurements</code> <code>Dict[str, Dict[str, Measurements]]</code> <p>A nested dictionary with keys corresponding to task names (outer), algorithm names (inner), and values corresponding to the points of the ROC curve.</p> required <code>variable_width</code> <code>bool</code> <p>If True, the lines corresponding to different to different algorithms will have different widths. This helps distinguish overlapping lines.</p> <code>False</code> <p>Returns:</p> Type Description <code>Figure</code> <p>A plotly figure.</p> Source code in <code>auto_circuit/metrics/official_circuits/roc_plot.py</code> <pre><code>def roc_plot(\n    taskname_measurements: Dict[str, Dict[str, Measurements]],\n    variable_width: bool = False,\n) -&gt; go.Figure:\n    \"\"\"\n    Plots the pessimistic Receiver Operating Characteristic (ROC) curve for a nested\n    dictionary of measurements. The outer dictionary has keys that are the task names\n    and the inner dictionary has keys that are the algorithm names.\n\n    Args:\n        taskname_measurements: A nested dictionary with keys corresponding to task names\n            (outer), algorithm names (inner), and values corresponding to the points of\n            the ROC curve.\n        variable_width: If True, the lines corresponding to different to different\n            algorithms will have different widths. This helps distinguish overlapping\n            lines.\n\n    Returns:\n        A plotly figure.\n    \"\"\"\n    titles = list(taskname_measurements.keys())\n    fig = make_subplots(rows=1, cols=len(taskname_measurements), subplot_titles=titles)\n    fig.update_traces(line=dict(shape=\"hv\"), mode=\"lines\")\n    taskname_measurements = dict(\n        sorted(taskname_measurements.items(), key=lambda x: x[0])\n    )\n    for task_idx, (task_key, algo_measurements) in enumerate(\n        taskname_measurements.items()\n    ):\n        for algo_idx, (algo_name, measurements) in enumerate(algo_measurements.items()):\n            width_delta = 8\n            max_width = (width_delta / 2) + (len(algo_measurements) - 1) * width_delta\n            line_width = max_width - algo_idx * width_delta\n            fig.add_scatter(\n                row=1,\n                col=task_idx + 1,\n                x=[x for x, _ in measurements],\n                y=[y for _, y in measurements],\n                mode=\"markers+text\" if len(measurements) == 1 else \"lines\",\n                text=algo_name,\n                line=dict(width=line_width if variable_width else 2),\n                textposition=\"middle right\",\n                showlegend=task_idx == 0,\n                # marker=dict(color=\"black\", size=10, symbol=\"x-thin\"),\n                marker_line_width=2,\n                name=algo_name,\n                marker_color=COLOR_PALETTE[algo_idx],\n            )\n    fig.update_xaxes(\n        matches=None,\n        title=\"False Positive Rate\",\n        scaleanchor=\"y\",\n        scaleratio=1,\n        range=[-0.02, 1.02],\n    )\n    fig.update_yaxes(range=[-0.02, 1.02], scaleanchor=\"x\", scaleratio=1)\n    fig.update_layout(\n        # title=f\"{main_title}: ROC Curves\",\n        template=\"plotly\",\n        yaxis_title=\"True Positive Rate\",\n        height=500,\n        # width=335 * len(set([d[\"Task\"] for d in data])) + 280,\n        width=365 * len(taskname_measurements) - 10,\n        legend=dict(\n            orientation=\"h\",\n            yanchor=\"bottom\",\n            y=-0.6,\n            xanchor=\"left\",\n            # x=0.0,\n            entrywidthmode=\"fraction\",\n            entrywidth=0.6,\n        ),\n    )\n    return fig\n</code></pre>"},{"location":"reference/metrics/official_circuits/roc_plot/#auto_circuit.metrics.official_circuits.roc_plot.task_roc_plot","title":"task_roc_plot","text":"<pre><code>task_roc_plot(task_measurements: TaskMeasurements) -&gt; Figure\n</code></pre> <p>Wrapper that takes the output of <code>measure_roc</code>, extracts the names of the tasks and algorithms, and plots the ROC curves using <code>roc_plot</code>.</p> Source code in <code>auto_circuit/metrics/official_circuits/roc_plot.py</code> <pre><code>def task_roc_plot(task_measurements: TaskMeasurements) -&gt; go.Figure:\n    \"\"\"\n    Wrapper that takes the output of\n    [`measure_roc`][auto_circuit.metrics.official_circuits.measure_roc.measure_roc],\n    extracts the names of the tasks and algorithms, and plots the ROC curves\n    using [`roc_plot`][auto_circuit.metrics.official_circuits.roc_plot.roc_plot].\n    \"\"\"\n    task_algo_name_measurements: Dict[str, Dict[str, Measurements]] = defaultdict(dict)\n    for task_key, algo_measurements in task_measurements.items():\n        task_name = TASK_DICT[task_key].name\n        for algo_key, measurements in algo_measurements.items():\n            algo_name = PRUNE_ALGO_DICT[algo_key].short_name\n            task_algo_name_measurements[task_name][algo_name] = measurements\n    return roc_plot(task_algo_name_measurements)\n</code></pre>"},{"location":"reference/metrics/official_circuits/circuits/docstring_official/","title":"Docstring official","text":""},{"location":"reference/metrics/official_circuits/circuits/docstring_official/#auto_circuit.metrics.official_circuits.circuits.docstring_official","title":"auto_circuit.metrics.official_circuits.circuits.docstring_official","text":""},{"location":"reference/metrics/official_circuits/circuits/docstring_official/#auto_circuit.metrics.official_circuits.circuits.docstring_official-classes","title":"Classes","text":""},{"location":"reference/metrics/official_circuits/circuits/docstring_official/#auto_circuit.metrics.official_circuits.circuits.docstring_official-functions","title":"Functions","text":""},{"location":"reference/metrics/official_circuits/circuits/docstring_official/#auto_circuit.metrics.official_circuits.circuits.docstring_official.docstring_node_based_official_edges","title":"docstring_node_based_official_edges","text":"<pre><code>docstring_node_based_official_edges(model: PatchableModel, token_positions: bool = False, word_idxs: Dict[str, int] = {}, seq_start_idx: int = 0) -&gt; Set[Edge]\n</code></pre> <p>The attention heads in the Docstring circuit, from the blog post by Stefan Heimersheim and Jett Janiak (2023).</p> <p>In the blog post, to measure the overall performance of the circuit, the authors Node Patch the heads in the circuit, rather than Edge Patching the specific edges they find. We include this variation to enable replication of these results.</p> <p>The token positions are based on my reading of the paper, but some were unspecified so in those cases we include all token positions between <code>A_def</code> and <code>param_3</code>.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>PatchableModel</code> <p>A patchable TransformerLens GPT-2 <code>HookedTransformer</code> model.</p> required <code>token_positions</code> <code>bool</code> <p>Whether to distinguish between token positions when returning the set of circuit edges. If <code>True</code>, we require that the <code>model</code> has <code>seq_len</code> not <code>None</code> (ie. separate edges for each token position) and that <code>word_idxs</code> is provided.</p> <code>False</code> <code>word_idxs</code> <code>Dict[str, int]</code> <p>A dictionary defining the index of specific named tokens in the circuit definition. For this circuit, the required tokens positions are:</p> <ul> <li><code>A_def</code></li> <li><code>B_def</code></li> <li><code>,_B</code></li> <li><code>C_def</code></li> <li><code>A_doc</code></li> <li><code>B_doc</code></li> <li><code>param_3</code></li> </ul> <code>{}</code> <code>seq_start_idx</code> <code>int</code> <p>Offset to add to all of the token positions in <code>word_idxs</code>. This is useful when using KV caching to skip the common prefix of the prompt.</p> <code>0</code> <p>Returns:</p> Type Description <code>Set[Edge]</code> <p>The set of edges in the circuit.</p> Source code in <code>auto_circuit/metrics/official_circuits/circuits/docstring_official.py</code> <pre><code>def docstring_node_based_official_edges(\n    model: PatchableModel,\n    token_positions: bool = False,\n    word_idxs: Dict[str, int] = {},\n    seq_start_idx: int = 0,\n) -&gt; Set[Edge]:\n    \"\"\"\n    The attention heads in the Docstring circuit, from the\n    [blog post](https://www.alignmentforum.org/posts/u6KXXmKFbXfWzoAXn) by Stefan\n    Heimersheim and Jett Janiak (2023).\n\n    In the blog post, to measure\n    [the overall performance](https://www.alignmentforum.org/posts/u6KXXmKFbXfWzoAXn#Putting_it_all_together)\n    of the circuit, the authors Node Patch the heads in the circuit, rather than Edge\n    Patching the specific edges they find. We include this variation to enable\n    replication of these results.\n\n    The token positions are based on my reading of the paper, but some were unspecified\n    so in those cases we include all token positions between `A_def` and `param_3`.\n\n    Args:\n        model: A patchable TransformerLens GPT-2 `HookedTransformer` model.\n        token_positions: Whether to distinguish between token positions when returning\n            the set of circuit edges. If `True`, we require that the `model` has\n            `seq_len` not `None` (ie. separate edges for each token position) and that\n            `word_idxs` is provided.\n        word_idxs: A dictionary defining the index of specific named tokens in the\n            circuit definition. For this circuit, the required tokens positions are:\n            &lt;ul&gt;\n                &lt;li&gt;&lt;code&gt;A_def&lt;/code&gt;&lt;/li&gt;\n                &lt;li&gt;&lt;code&gt;B_def&lt;/code&gt;&lt;/li&gt;\n                &lt;li&gt;&lt;code&gt;,_B&lt;/code&gt;&lt;/li&gt;\n                &lt;li&gt;&lt;code&gt;C_def&lt;/code&gt;&lt;/li&gt;\n                &lt;li&gt;&lt;code&gt;A_doc&lt;/code&gt;&lt;/li&gt;\n                &lt;li&gt;&lt;code&gt;B_doc&lt;/code&gt;&lt;/li&gt;\n                &lt;li&gt;&lt;code&gt;param_3&lt;/code&gt;&lt;/li&gt;\n            &lt;/ul&gt;\n        seq_start_idx: Offset to add to all of the token positions in `word_idxs`.\n            This is useful when using KV caching to skip the common prefix of the\n            prompt.\n\n    Returns:\n        The set of edges in the circuit.\n    \"\"\"\n\n    word_idxs.get(\"A_def\", 0)\n    B_def_tok_idx = word_idxs.get(\"B_def\", 0)\n    B_comma_tok_idx = word_idxs.get(\",_B\", 0)\n    C_def_tok_idx = word_idxs.get(\"C_def\", 0)\n    A_doc_tok_idx = word_idxs.get(\"A_doc\", 0)\n    # param_2_tok_idx = word_idxs.get(\"param_2\", 0) A0.2 not included for some reason\n    B_doc_tok_idx = word_idxs.get(\"B_doc\", 0)\n    param_3_tok_idx = word_idxs.get(\"param_3\", 0)\n\n    all_tok_idxs = [\n        B_def_tok_idx,\n        B_comma_tok_idx,\n        C_def_tok_idx,\n        A_doc_tok_idx,\n        # param_2_tok_idx,\n        B_doc_tok_idx,\n        param_3_tok_idx,\n    ]\n    tok_idx_range: List[int] = list(range(min(all_tok_idxs), max(all_tok_idxs) + 1))\n\n    if token_positions:\n        assert param_3_tok_idx &gt; 0, \"Must provide word_idxs if token_positions is True\"\n\n    heads_in_circuit: Dict[str, List[int]] = {\n        \"A1.4\": [B_comma_tok_idx],\n        \"A2.0\": [C_def_tok_idx],\n        \"A3.0\": [param_3_tok_idx],\n        \"A3.6\": [param_3_tok_idx],\n        \"A0.5\": [B_def_tok_idx, C_def_tok_idx, B_doc_tok_idx],\n        \"A1.2\": [A_doc_tok_idx, B_doc_tok_idx],\n        \"A0.2\": [B_doc_tok_idx],\n        \"A0.4\": [param_3_tok_idx],\n        # Extras for improved performance. Post does not specify token positions.\n        \"A1.0\": tok_idx_range,\n        \"A0.1\": tok_idx_range,\n        \"A2.3\": tok_idx_range,\n    }\n\n    heads_to_keep: Set[Tuple[str, Optional[int]]] = set()\n    for head_name, tok_idxs in heads_in_circuit.items():\n        if token_positions:\n            for tok_idx in tok_idxs:\n                heads_to_keep.add((head_name, tok_idx))\n        else:\n            heads_to_keep.add((head_name, None))\n\n    official_edges: Set[Edge] = set()\n    not_official_edges: Set[Edge] = set()\n    for edge in model.edges:\n        src_is_head = edge.src.head_idx is not None\n        if token_positions:\n            assert edge.seq_idx is not None\n            src_head_key = (edge.src.name, edge.seq_idx + seq_start_idx)\n        else:\n            assert not token_positions\n            src_head_key = (edge.src.name, None)\n\n        if src_is_head and src_head_key not in heads_to_keep:\n            not_official_edges.add(edge)\n            continue\n\n        official_edges.add(edge)\n    return official_edges\n</code></pre>"},{"location":"reference/metrics/official_circuits/circuits/docstring_official/#auto_circuit.metrics.official_circuits.circuits.docstring_official.docstring_true_edges","title":"docstring_true_edges","text":"<pre><code>docstring_true_edges(model: PatchableModel, token_positions: bool = False, word_idxs: Dict[str, int] = {}, seq_start_idx: int = 0) -&gt; Set[Edge]\n</code></pre> <p>The Docstring circuit, from the blog post by Stefan Heimersheim and Jett Janiak (2023).</p> <p>The exact set of edges was defined by Stephan in the ACDC repo. The token positions are based on my reading of the paper, but I have been over them briefly with Stefan and he endorsed them as reasonable.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>PatchableModel</code> <p>A patchable TransformerLens GPT-2 <code>HookedTransformer</code> model.</p> required <code>token_positions</code> <code>bool</code> <p>Whether to distinguish between token positions when returning the set of circuit edges. If <code>True</code>, we require that the <code>model</code> has <code>seq_len</code> not <code>None</code> (ie. separate edges for each token position) and that <code>word_idxs</code> is provided.</p> <code>False</code> <code>word_idxs</code> <code>Dict[str, int]</code> <p>A dictionary defining the index of specific named tokens in the circuit definition. For this circuit, the required tokens positions are:</p> <ul> <li><code>A_def</code></li> <li><code>B_def</code></li> <li><code>,_B</code></li> <li><code>C_def</code></li> <li><code>A_doc</code></li> <li><code>B_doc</code></li> <li><code>param_3</code></li> </ul> <code>{}</code> <code>seq_start_idx</code> <code>int</code> <p>Offset to add to all of the token positions in <code>word_idxs</code>. This is useful when using KV caching to skip the common prefix of the prompt.</p> <code>0</code> <p>Returns:</p> Type Description <code>Set[Edge]</code> <p>The set of edges in the circuit.</p> Source code in <code>auto_circuit/metrics/official_circuits/circuits/docstring_official.py</code> <pre><code>def docstring_true_edges(\n    model: PatchableModel,\n    token_positions: bool = False,\n    word_idxs: Dict[str, int] = {},\n    seq_start_idx: int = 0,\n) -&gt; Set[Edge]:\n    \"\"\"\n    The Docstring circuit, from the\n    [blog post](https://www.alignmentforum.org/posts/u6KXXmKFbXfWzoAXn) by Stefan\n    Heimersheim and Jett Janiak (2023).\n\n    The exact set of edges was defined by Stephan in the\n    [ACDC repo](https://github.com/ArthurConmy/Automatic-Circuit-Discovery/blob/main/acdc/docstring/utils.py).\n    The token positions are based on my reading of the paper, but I have been over them\n    briefly with Stefan and he endorsed them as reasonable.\n\n    Args:\n        model: A patchable TransformerLens GPT-2 `HookedTransformer` model.\n        token_positions: Whether to distinguish between token positions when returning\n            the set of circuit edges. If `True`, we require that the `model` has\n            `seq_len` not `None` (ie. separate edges for each token position) and that\n            `word_idxs` is provided.\n        word_idxs: A dictionary defining the index of specific named tokens in the\n            circuit definition. For this circuit, the required tokens positions are:\n            &lt;ul&gt;\n                &lt;li&gt;&lt;code&gt;A_def&lt;/code&gt;&lt;/li&gt;\n                &lt;li&gt;&lt;code&gt;B_def&lt;/code&gt;&lt;/li&gt;\n                &lt;li&gt;&lt;code&gt;,_B&lt;/code&gt;&lt;/li&gt;\n                &lt;li&gt;&lt;code&gt;C_def&lt;/code&gt;&lt;/li&gt;\n                &lt;li&gt;&lt;code&gt;A_doc&lt;/code&gt;&lt;/li&gt;\n                &lt;li&gt;&lt;code&gt;B_doc&lt;/code&gt;&lt;/li&gt;\n                &lt;li&gt;&lt;code&gt;param_3&lt;/code&gt;&lt;/li&gt;\n            &lt;/ul&gt;\n        seq_start_idx: Offset to add to all of the token positions in `word_idxs`.\n            This is useful when using KV caching to skip the common prefix of the\n            prompt.\n\n    Returns:\n        The set of edges in the circuit.\n    \"\"\"\n    assert model.cfg.model_name == \"Attn_Only_4L512W_C4_Code\"\n    assert model.separate_qkv\n\n    A_def_tok_idx = word_idxs.get(\"A_def\", 0)\n    B_def_tok_idx = word_idxs.get(\"B_def\", 0)\n    B_comma_tok_idx = word_idxs.get(\",_B\", 0)\n    C_def_tok_idx = word_idxs.get(\"C_def\", 0)\n    A_doc_tok_idx = word_idxs.get(\"A_doc\", 0)\n    # param_2_tok_idx = word_idxs.get(\"param_2\", 0) A0.2 not included for some reason\n    B_doc_tok_idx = word_idxs.get(\"B_doc\", 0)\n    param_3_tok_idx = word_idxs.get(\"param_3\", 0)\n\n    if token_positions:\n        assert param_3_tok_idx &gt; 0, \"Must provide word_idxs if token_positions is True\"\n\n    edges_present: Dict[str, List[int]] = {}\n    edges_present[\"A0.5-&gt;A1.4.V\"] = [B_def_tok_idx, B_doc_tok_idx]\n    edges_present[\"Resid Start-&gt;A0.5.V\"] = [B_def_tok_idx, C_def_tok_idx, B_doc_tok_idx]\n    edges_present[\"Resid Start-&gt;A2.0.Q\"] = [C_def_tok_idx]\n    edges_present[\"A0.5-&gt;A2.0.Q\"] = [C_def_tok_idx]\n    edges_present[\"Resid Start-&gt;A2.0.K\"] = []  # [B_comma_tok_idx] ',_B' doesn't vary?\n    edges_present[\"A0.5-&gt;A2.0.K\"] = [B_def_tok_idx]  # Not entirely sure about this\n    edges_present[\"A1.4-&gt;A2.0.V\"] = [B_comma_tok_idx]\n    edges_present[\"Resid Start-&gt;A1.4.V\"] = [B_def_tok_idx, B_doc_tok_idx]\n    edges_present[\"Resid Start-&gt;A1.2.K\"] = [\n        A_def_tok_idx,\n        B_def_tok_idx,\n        A_doc_tok_idx,\n        B_doc_tok_idx,\n    ]  # This is an inclusive guess as to which tokens might matter\n    edges_present[\"Resid Start-&gt;A1.2.Q\"] = [A_doc_tok_idx, B_doc_tok_idx]\n    edges_present[\"A0.5-&gt;A1.2.Q\"] = [B_doc_tok_idx]\n    edges_present[\"A0.5-&gt;A1.2.K\"] = [B_def_tok_idx, B_doc_tok_idx]\n\n    for layer_3_head in [\"0\", \"6\"]:\n        edges_present[f\"A3.{layer_3_head}-&gt;Resid End\"] = [param_3_tok_idx]\n        edges_present[f\"A1.4-&gt;A3.{layer_3_head}.Q\"] = [param_3_tok_idx]\n        edges_present[f\"Resid Start-&gt;A3.{layer_3_head}.V\"] = [C_def_tok_idx]\n        edges_present[f\"A0.5-&gt;A3.{layer_3_head}.V\"] = [C_def_tok_idx]\n        edges_present[f\"A2.0-&gt;A3.{layer_3_head}.K\"] = [C_def_tok_idx]\n        edges_present[f\"A1.2-&gt;A3.{layer_3_head}.K\"] = [A_doc_tok_idx, B_doc_tok_idx]\n\n    true_edges: Set[Edge] = set()\n    for edge in model.edges:\n        if edge.name in edges_present.keys():\n            if token_positions:\n                assert edge.seq_idx is not None\n                if (edge.seq_idx + seq_start_idx) in edges_present[edge.name]:\n                    true_edges.add(edge)\n            else:\n                true_edges.add(edge)\n\n    return true_edges\n</code></pre>"},{"location":"reference/metrics/official_circuits/circuits/greaterthan_official/","title":"Greaterthan official","text":""},{"location":"reference/metrics/official_circuits/circuits/greaterthan_official/#auto_circuit.metrics.official_circuits.circuits.greaterthan_official","title":"auto_circuit.metrics.official_circuits.circuits.greaterthan_official","text":""},{"location":"reference/metrics/official_circuits/circuits/greaterthan_official/#auto_circuit.metrics.official_circuits.circuits.greaterthan_official-classes","title":"Classes","text":""},{"location":"reference/metrics/official_circuits/circuits/greaterthan_official/#auto_circuit.metrics.official_circuits.circuits.greaterthan_official-functions","title":"Functions","text":""},{"location":"reference/metrics/official_circuits/circuits/greaterthan_official/#auto_circuit.metrics.official_circuits.circuits.greaterthan_official.greaterthan_true_edges","title":"greaterthan_true_edges","text":"<pre><code>greaterthan_true_edges(model: PatchableModel, token_positions: bool = False, word_idxs: Dict[str, int] = {}, seq_start_idx: int = 0) -&gt; Set[Edge]\n</code></pre> <p>The Greaterthan circuit, discovered by Hanna et al. 2023.</p> <p>Based on the ACDC implementation.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>PatchableModel</code> <p>A patchable TransformerLens GPT-2 <code>HookedTransformer</code> model.</p> required <code>token_positions</code> <code>bool</code> <p>Whether to distinguish between token positions when returning the set of circuit edges. If <code>True</code>, we require that the <code>model</code> has <code>seq_len</code> not <code>None</code> (ie. separate edges for each token position) and that <code>word_idxs</code> is provided.</p> <code>False</code> <code>word_idxs</code> <code>Dict[str, int]</code> <p>A dictionary defining the index of specific named tokens in the circuit definition. For this circuit, token positions are not specified, so this parameter is not used.</p> <code>{}</code> <code>seq_start_idx</code> <code>int</code> <p>Offset to add to all of the token positions in <code>word_idxs</code>. For this circuit, token positions are not specified, so this parameter is not used.</p> <code>0</code> <p>Returns:</p> Type Description <code>Set[Edge]</code> <p>The set of edges in the circuit.</p> Note <p>The Greaterthan circuit does not specify token positions, so if <code>token_positions</code> is <code>True</code>, all token positions are included for the edges in the circuit.</p> Source code in <code>auto_circuit/metrics/official_circuits/circuits/greaterthan_official.py</code> <pre><code>def greaterthan_true_edges(\n    model: PatchableModel,\n    token_positions: bool = False,\n    word_idxs: Dict[str, int] = {},\n    seq_start_idx: int = 0,\n) -&gt; Set[Edge]:\n    \"\"\"\n    The Greaterthan circuit, discovered by\n    [Hanna et al. 2023](https://arxiv.org/abs/2305.00586).\n\n    Based on the [ACDC implementation](https://github.com/ArthurConmy/Automatic-Circuit-Discovery/blob/main/acdc/greaterthan/utils.py).\n\n    Args:\n        model: A patchable TransformerLens GPT-2 `HookedTransformer` model.\n        token_positions: Whether to distinguish between token positions when returning\n            the set of circuit edges. If `True`, we require that the `model` has\n            `seq_len` not `None` (ie. separate edges for each token position) and that\n            `word_idxs` is provided.\n        word_idxs: A dictionary defining the index of specific named tokens in the\n            circuit definition. For this circuit, token positions are not specified, so\n            this parameter is not used.\n        seq_start_idx: Offset to add to all of the token positions in `word_idxs`.\n            For this circuit, token positions are not specified, so this parameter is\n            not used.\n\n    Returns:\n        The set of edges in the circuit.\n\n    Note:\n        The Greaterthan circuit does not specify token positions, so if\n        `token_positions` is `True`, all token positions are included for the edges in\n        the circuit.\n    \"\"\"\n    assert model.cfg.model_name == \"gpt2\"\n    assert model.separate_qkv\n\n    edges_present: List[str] = []\n\n    # attach input\n    for GROUP in [\"ATTN_0.3_0.5\", \"ATTN_0.1\", \"EARLY_MLPS\"]:\n        for layer_idx, head_idx in CIRCUIT[GROUP]:\n            dest_nodes = idx_to_nodes(layer_idx, head_idx, src_nodes=False)\n\n            for node_name in dest_nodes:\n                edges_present.append(f\"Resid Start-&gt;{node_name}\")\n\n    # attach output\n    for GROUP in [\"MID_ATTNS\", \"LATE_MLPS\"]:\n        for layer_idx, head_idx in CIRCUIT[GROUP]:\n            src_nodes = idx_to_nodes(layer_idx, head_idx, src_nodes=True)\n            for node_name in src_nodes:\n                edges_present.append(f\"{node_name}-&gt;Resid End\")\n\n    # MLP groups are interconnected\n    for GROUP in [\"EARLY_MLPS\", \"LATE_MLPS\"]:\n        for src_layer, _ in CIRCUIT[GROUP]:\n            for dest_layer, _ in CIRCUIT[GROUP]:\n                if src_layer &gt;= dest_layer:\n                    continue\n                edges_present.append(f\"MLP {src_layer}-&gt;MLP {dest_layer}\")\n\n    # connected pairs\n    for GROUP1, GROUP2 in CONNECTED_PAIRS:\n        for src_layer, src_head in CIRCUIT[GROUP1]:\n            for dest_layer, dest_head in CIRCUIT[GROUP2]:\n                src_is_attn = src_head is not None\n                dest_is_mlp = dest_head is None\n                same_layer = src_layer == dest_layer\n                attn_to_mlp_on_same_layer = src_is_attn and dest_is_mlp and same_layer\n                if src_layer &gt;= dest_layer and not attn_to_mlp_on_same_layer:\n                    continue\n                for src_node_name in idx_to_nodes(src_layer, src_head, src_nodes=True):\n                    for dest_node_name in idx_to_nodes(\n                        dest_layer, dest_head, src_nodes=False\n                    ):\n                        edges_present.append(f\"{src_node_name}-&gt;{dest_node_name}\")\n\n    # Hanna et al have totally clean query inputs to MID_ATTNS heads\n    # this is A LOT of edges so we just add the MLP -&gt; MID_ATTNS Q edges\n\n    MAX_AMID_LAYER = max([layer_idx for layer_idx, _ in CIRCUIT[\"MID_ATTNS\"]])\n    # connect all MLPs before the MID_ATTNS heads\n    for mlp_sender_layer in range(0, MAX_AMID_LAYER):\n        for mlp_receiver_layer in range(1 + mlp_sender_layer, MAX_AMID_LAYER):\n            edges_present.append(f\"MLP {mlp_sender_layer}-&gt;MLP {mlp_receiver_layer}\")\n\n    # connect all early MLPs to MID_ATTNS heads' Q inputs\n    for layer_idx, head_idx in CIRCUIT[\"MID_ATTNS\"]:\n        for mlp_sender_layer in range(0, layer_idx):\n            edges_present.append(f\"MLP {mlp_sender_layer}-&gt;A{layer_idx}.{head_idx}.Q\")\n\n    true_edges: Set[Edge] = set()\n    for edge in model.edges:\n        if edge.name in edges_present:\n            true_edges.add(edge)\n    return true_edges\n</code></pre>"},{"location":"reference/metrics/official_circuits/circuits/ioi_official/","title":"Ioi official","text":""},{"location":"reference/metrics/official_circuits/circuits/ioi_official/#auto_circuit.metrics.official_circuits.circuits.ioi_official","title":"auto_circuit.metrics.official_circuits.circuits.ioi_official","text":""},{"location":"reference/metrics/official_circuits/circuits/ioi_official/#auto_circuit.metrics.official_circuits.circuits.ioi_official-classes","title":"Classes","text":""},{"location":"reference/metrics/official_circuits/circuits/ioi_official/#auto_circuit.metrics.official_circuits.circuits.ioi_official-functions","title":"Functions","text":""},{"location":"reference/metrics/official_circuits/circuits/ioi_official/#auto_circuit.metrics.official_circuits.circuits.ioi_official.ioi_head_based_official_edges","title":"ioi_head_based_official_edges","text":"<pre><code>ioi_head_based_official_edges(model: PatchableModel, token_positions: bool = False, word_idxs: Dict[str, int] = {}, seq_start_idx: int = 0) -&gt; Set[Edge]\n</code></pre> <p>Node-based circuit of the IOI attention heads.</p> <p>To measure the performance of their circuit, Wang et al. (2022) Mean Ablate the heads in the circuit, rather than Edge Ablating the specific edges they find (means calculated over ABC dataset). We include this variation to enable replication of these results.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>PatchableModel</code> <p>A patchable TransformerLens GPT-2 <code>HookedTransformer</code> model.</p> required <code>token_positions</code> <code>bool</code> <p>Whether to distinguish between token positions when returning the set of circuit edges. If <code>True</code>, we require that the <code>model</code> has <code>seq_len</code> not <code>None</code> (ie. separate edges for each token position) and that <code>word_idxs</code> is provided.</p> <code>False</code> <code>word_idxs</code> <code>Dict[str, int]</code> <p>A dictionary defining the index of specific named tokens in the circuit definition. For this circuit, the required tokens positions are:</p> <ul> <li><code>S1+1</code></li> <li><code>S2</code></li> <li><code>end</code></li> </ul> <code>{}</code> <code>seq_start_idx</code> <code>int</code> <p>Offset to add to all of the token positions in <code>word_idxs</code>. This is useful when using KV caching to skip the common prefix of the prompt.</p> <code>0</code> <p>Returns:</p> Type Description <code>Set[Edge]</code> <p>The set of edges in the circuit.</p> Source code in <code>auto_circuit/metrics/official_circuits/circuits/ioi_official.py</code> <pre><code>def ioi_head_based_official_edges(\n    model: PatchableModel,\n    token_positions: bool = False,\n    word_idxs: Dict[str, int] = {},\n    seq_start_idx: int = 0,\n) -&gt; Set[Edge]:\n    \"\"\"\n    Node-based circuit of the IOI attention heads.\n\n    To measure the performance of their circuit,\n    [Wang et al. (2022)](https://arxiv.org/abs/2211.00593) Mean Ablate the heads in the\n    circuit, rather than Edge Ablating the specific edges they find (means calculated\n    over ABC dataset). We include this variation to enable replication of these results.\n\n    Args:\n        model: A patchable TransformerLens GPT-2 `HookedTransformer` model.\n        token_positions: Whether to distinguish between token positions when returning\n            the set of circuit edges. If `True`, we require that the `model` has\n            `seq_len` not `None` (ie. separate edges for each token position) and that\n            `word_idxs` is provided.\n        word_idxs: A dictionary defining the index of specific named tokens in the\n            circuit definition. For this circuit, the required tokens positions are:\n            &lt;ul&gt;\n                &lt;li&gt;&lt;code&gt;S1+1&lt;/code&gt;&lt;/li&gt;\n                &lt;li&gt;&lt;code&gt;S2&lt;/code&gt;&lt;/li&gt;\n                &lt;li&gt;&lt;code&gt;end&lt;/code&gt;&lt;/li&gt;\n            &lt;/ul&gt;\n        seq_start_idx: Offset to add to all of the token positions in `word_idxs`.\n            This is useful when using KV caching to skip the common prefix of the\n            prompt.\n\n    Returns:\n        The set of edges in the circuit.\n    \"\"\"\n    assert model.cfg.model_name == \"gpt2\"\n    assert (model.seq_len is not None) == token_positions\n\n    S1_plus_1_tok_idx = word_idxs.get(\"S1+1\", 0)\n    S2_tok_idx = word_idxs.get(\"S2\", 0)\n    final_tok_idx = word_idxs.get(\"end\", 0)\n\n    if token_positions:\n        assert final_tok_idx &gt; 0, \"Must provide word_idxs if token_positions is True\"\n\n    CIRCUIT = {\n        \"name mover\": [(9, 9), (10, 0), (9, 6)],\n        \"backup name mover\": [\n            (10, 10),\n            (10, 6),\n            (10, 2),\n            (10, 1),\n            (11, 2),\n            (9, 7),\n            (9, 0),\n            (11, 9),\n        ],\n        \"negative name mover\": [(10, 7), (11, 10)],\n        \"s2 inhibition\": [(7, 3), (7, 9), (8, 6), (8, 10)],\n        \"induction\": [(5, 5), (5, 8), (5, 9), (6, 9)],\n        \"duplicate token\": [(0, 1), (0, 10), (3, 0)],\n        \"previous token\": [(2, 2), (4, 11)],\n    }\n\n    SEQ_POS_TO_KEEP = {\n        \"name mover\": final_tok_idx,\n        \"backup name mover\": final_tok_idx,\n        \"negative name mover\": final_tok_idx,\n        \"s2 inhibition\": final_tok_idx,\n        \"induction\": S2_tok_idx,\n        \"duplicate token\": S2_tok_idx,\n        \"previous token\": S1_plus_1_tok_idx,\n    }\n    heads_to_keep: Set[Tuple[str, Optional[int]]] = set()\n    for head_type, head_idxs in CIRCUIT.items():\n        head_type_seq_idx = SEQ_POS_TO_KEEP[head_type]\n        for head_lyr, head_idx in head_idxs:\n            head_name = f\"A{head_lyr}.{head_idx}\"\n            if token_positions:\n                heads_to_keep.add((head_name, head_type_seq_idx))\n            else:\n                heads_to_keep.add((head_name, None))\n\n    official_edges: Set[Edge] = set()\n    not_official_edges: Set[Edge] = set()\n    for edge in model.edges:\n        src_is_head = edge.src.head_idx is not None\n        if token_positions:\n            assert edge.seq_idx is not None\n            src_head_key = (edge.src.name, edge.seq_idx + seq_start_idx)\n        else:\n            src_head_key = (edge.src.name, None)\n\n        if src_is_head and src_head_key not in heads_to_keep:\n            not_official_edges.add(edge)\n            continue\n\n        official_edges.add(edge)\n    return official_edges\n</code></pre>"},{"location":"reference/metrics/official_circuits/circuits/ioi_official/#auto_circuit.metrics.official_circuits.circuits.ioi_official.ioi_true_edges","title":"ioi_true_edges","text":"<pre><code>ioi_true_edges(model: PatchableModel, token_positions: bool = False, word_idxs: Dict[str, int] = {}, seq_start_idx: int = 0) -&gt; Set[Edge]\n</code></pre> <p>The Indirect Object Identification (IOI) circuit, discovered by Wang et al. (2022).</p> <p>The exact set of edges was defined by Conmy et al. in the ACDC repo.</p> <p>The token positions are based on my reading of the paper.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>PatchableModel</code> <p>A patchable TransformerLens GPT-2 <code>HookedTransformer</code> model.</p> required <code>token_positions</code> <code>bool</code> <p>Whether to distinguish between token positions when returning the set of circuit edges. If <code>True</code>, we require that the <code>model</code> has <code>seq_len</code> not <code>None</code> (ie. separate edges for each token position) and that <code>word_idxs</code> is provided.</p> <code>False</code> <code>word_idxs</code> <code>Dict[str, int]</code> <p>A dictionary defining the index of specific named tokens in the circuit definition. For this circuit, the required tokens positions are:</p> <ul> <li><code>IO</code></li> <li><code>S1</code></li> <li><code>S1+1</code></li> <li><code>S2</code></li> <li><code>end</code></li> </ul> <code>{}</code> <code>seq_start_idx</code> <code>int</code> <p>Offset to add to all of the token positions in <code>word_idxs</code>. This is useful when using KV caching to skip the common prefix of the prompt.</p> <code>0</code> <p>Returns:</p> Type Description <code>Set[Edge]</code> <p>The set of edges in the circuit.</p> Source code in <code>auto_circuit/metrics/official_circuits/circuits/ioi_official.py</code> <pre><code>def ioi_true_edges(\n    model: PatchableModel,\n    token_positions: bool = False,\n    word_idxs: Dict[str, int] = {},\n    seq_start_idx: int = 0,\n) -&gt; Set[Edge]:\n    \"\"\"\n    The Indirect Object Identification (IOI) circuit, discovered by\n    [Wang et al. (2022)](https://arxiv.org/abs/2211.00593).\n\n    The exact set of edges was defined by Conmy et al. in the\n    [ACDC repo](https://github.com/ArthurConmy/Automatic-Circuit-Discovery/blob/main/acdc/ioi/utils.py).\n\n    The token positions are based on my reading of the paper.\n\n    Args:\n        model: A patchable TransformerLens GPT-2 `HookedTransformer` model.\n        token_positions: Whether to distinguish between token positions when returning\n            the set of circuit edges. If `True`, we require that the `model` has\n            `seq_len` not `None` (ie. separate edges for each token position) and that\n            `word_idxs` is provided.\n        word_idxs: A dictionary defining the index of specific named tokens in the\n            circuit definition. For this circuit, the required tokens positions are:\n            &lt;ul&gt;\n                &lt;li&gt;&lt;code&gt;IO&lt;/code&gt;&lt;/li&gt;\n                &lt;li&gt;&lt;code&gt;S1&lt;/code&gt;&lt;/li&gt;\n                &lt;li&gt;&lt;code&gt;S1+1&lt;/code&gt;&lt;/li&gt;\n                &lt;li&gt;&lt;code&gt;S2&lt;/code&gt;&lt;/li&gt;\n                &lt;li&gt;&lt;code&gt;end&lt;/code&gt;&lt;/li&gt;\n            &lt;/ul&gt;\n        seq_start_idx: Offset to add to all of the token positions in `word_idxs`.\n            This is useful when using KV caching to skip the common prefix of the\n            prompt.\n\n    Returns:\n        The set of edges in the circuit.\n    \"\"\"\n    assert model.cfg.model_name == \"gpt2\"\n    assert model.is_factorized, \"IOI edge based circuit requires factorized model\"\n    assert model.separate_qkv\n\n    io_tok_idx = word_idxs.get(\"IO\", 0)\n    s1_tok_idx = word_idxs.get(\"S1\", 0)\n    s1_plus_1_tok_idx = word_idxs.get(\"S1+1\", 0)\n    s2_tok_idx = word_idxs.get(\"S2\", 0)\n    final_tok_idx = word_idxs.get(\"end\", 0)\n\n    if token_positions:\n        assert final_tok_idx &gt; 0, \"Must provide word_idxs if token_positions is True\"\n\n    special_connections: List[Conn] = [\n        Conn(\n            \"INPUT\",\n            \"previous token\",\n            [(\"q\", s1_plus_1_tok_idx), (\"k\", s1_tok_idx), (\"v\", s1_tok_idx)],\n        ),\n        Conn(\n            \"INPUT\",\n            \"duplicate token\",\n            [(\"q\", s2_tok_idx), (\"k\", s1_tok_idx), (\"v\", s1_tok_idx)],\n        ),\n        Conn(\"INPUT\", \"s2 inhibition\", [(\"q\", final_tok_idx)]),\n        Conn(\"INPUT\", \"negative\", [(\"k\", io_tok_idx), (\"v\", io_tok_idx)]),\n        Conn(\"INPUT\", \"name mover\", [(\"k\", io_tok_idx), (\"v\", io_tok_idx)]),\n        Conn(\"INPUT\", \"backup name mover\", [(\"k\", io_tok_idx), (\"v\", io_tok_idx)]),\n        Conn(\n            \"previous token\",\n            \"induction\",\n            [(\"k\", s1_plus_1_tok_idx), (\"v\", s1_plus_1_tok_idx)],\n        ),\n        Conn(\"induction\", \"s2 inhibition\", [(\"k\", s2_tok_idx), (\"v\", s2_tok_idx)]),\n        Conn(\n            \"duplicate token\", \"s2 inhibition\", [(\"k\", s2_tok_idx), (\"v\", s2_tok_idx)]\n        ),\n        Conn(\"s2 inhibition\", \"negative\", [(\"q\", final_tok_idx)]),\n        Conn(\"s2 inhibition\", \"name mover\", [(\"q\", final_tok_idx)]),\n        Conn(\"s2 inhibition\", \"backup name mover\", [(\"q\", final_tok_idx)]),\n        Conn(\"negative\", \"OUTPUT\", [(None, final_tok_idx)]),\n        Conn(\"name mover\", \"OUTPUT\", [(None, final_tok_idx)]),\n        Conn(\"backup name mover\", \"OUTPUT\", [(None, final_tok_idx)]),\n    ]\n\n    edges_present: Dict[str, List[int]] = defaultdict(list)\n    for conn in special_connections:\n        edge_src_names, edge_dests = [], []\n        if conn.inp == \"INPUT\":\n            edge_src_names = [\"Resid Start\"]\n        else:\n            for (layer, head) in IOI_CIRCUIT[conn.inp]:\n                edge_src_names.append(f\"A{layer}.{head}\")\n        if conn.out == \"OUTPUT\":\n            assert len(conn.qkv) == 1\n            edge_dests.append((\"Resid End\", final_tok_idx))\n        else:\n            for (layer, head) in IOI_CIRCUIT[conn.out]:\n                for qkv in conn.qkv:\n                    assert qkv[0] is not None\n                    edge_dests.append((f\"A{layer}.{head}.{qkv[0].upper()}\", qkv[1]))\n\n        # Connect all MLPS in between heads in the circuit\n        # (in the IOI paper they allow activations to flow through MLPs,\n        # which is equalivent to including all MLPs in between two nodes.)\n        if conn.inp == \"INPUT\":\n            src_layer = 0\n        else:\n            src_layer = min([layer for (layer, _) in IOI_CIRCUIT[conn.inp]])\n\n        if conn.out == \"OUTPUT\":\n            dest_layer = conn.qkv[0][1]\n        else:\n            dest_layer = max([layer for (layer, _) in IOI_CIRCUIT[conn.out]])\n        dest_tok_idxs = [tok_idx for (_, tok_idx) in conn.qkv]\n\n        # Src layer is inclusive because MLP comes after ATTN\n        for layer in range(src_layer, dest_layer):\n            for tok_idx in dest_tok_idxs:\n                edge_src_names.append(f\"MLP {layer}\")\n                edge_dests.append((f\"MLP {layer}\", tok_idx))\n\n        for src_name in edge_src_names:\n            for dest_name, tok_pos in edge_dests:\n                edges_present[f\"{src_name}-&gt;{dest_name}\"].append(tok_pos)\n\n    true_edges: Set[Edge] = set()\n    for edge in model.edges:\n        if edge.name in edges_present.keys():\n            if token_positions:\n                assert edge.seq_idx is not None\n                if (edge.seq_idx + seq_start_idx) in edges_present[edge.name]:\n                    true_edges.add(edge)\n            else:\n                true_edges.add(edge)\n    return true_edges\n</code></pre>"},{"location":"reference/metrics/official_circuits/circuits/ioi_official/#auto_circuit.metrics.official_circuits.circuits.ioi_official.ioi_true_edges_mlp_0_only","title":"ioi_true_edges_mlp_0_only","text":"<pre><code>ioi_true_edges_mlp_0_only(model: PatchableModel, token_positions: bool = False, word_idxs: Dict[str, int] = {}, seq_start_idx: int = 0) -&gt; Set[Edge]\n</code></pre> <p>Wrapper for <code>ioi_true_edges</code> that removes all edges to or from all MLPs except MLP 0.</p> <p>Wang et al. (2022) consider MLPs to be part of the direct path between attention heads, so they implicitly include a large number of MLP edges in the circuit, but they did not study these interactions in detail and most of them are probably not important.</p> <p>Therefore we include this function as a very rough attempt at \"the IOI circuit with fewer unnecessary MLP edges\". We include just MLP 0 because it has been widely observed that MLP 0 tends to be the most important MLP layer in GPT-2.</p> Source code in <code>auto_circuit/metrics/official_circuits/circuits/ioi_official.py</code> <pre><code>def ioi_true_edges_mlp_0_only(\n    model: PatchableModel,\n    token_positions: bool = False,\n    word_idxs: Dict[str, int] = {},\n    seq_start_idx: int = 0,\n) -&gt; Set[Edge]:\n    \"\"\"\n    Wrapper for\n    [`ioi_true_edges`][auto_circuit.metrics.official_circuits.circuits.ioi_official.ioi_true_edges]\n    that removes all edges to or from all MLPs except MLP 0.\n\n    [Wang et al. (2022)](https://arxiv.org/abs/2211.00593) consider MLPs to be part of\n    the direct path between attention heads, so they implicitly include a large number\n    of MLP edges in the circuit, but they did not study these interactions in detail and\n    most of them are probably not important.\n\n    Therefore we include this function as a very rough attempt at \"the IOI circuit with\n    fewer unnecessary MLP edges\". We include just MLP 0 because it has been widely\n    observed that MLP 0 tends to be the most important MLP layer in GPT-2.\n    \"\"\"\n    ioi_edges = ioi_true_edges(model, token_positions, word_idxs, seq_start_idx)\n    minimal_ioi_edges = set()\n    for edge in ioi_edges:\n        if \"MLP 0\" in edge.name or \"MLP\" not in edge.name:\n            minimal_ioi_edges.add(edge)\n    return minimal_ioi_edges\n</code></pre>"},{"location":"reference/metrics/official_circuits/circuits/sports_players_official/","title":"Sports players official","text":""},{"location":"reference/metrics/official_circuits/circuits/sports_players_official/#auto_circuit.metrics.official_circuits.circuits.sports_players_official","title":"auto_circuit.metrics.official_circuits.circuits.sports_players_official","text":""},{"location":"reference/metrics/official_circuits/circuits/sports_players_official/#auto_circuit.metrics.official_circuits.circuits.sports_players_official-classes","title":"Classes","text":""},{"location":"reference/metrics/official_circuits/circuits/sports_players_official/#auto_circuit.metrics.official_circuits.circuits.sports_players_official-functions","title":"Functions","text":""},{"location":"reference/metrics/official_circuits/circuits/sports_players_official/#auto_circuit.metrics.official_circuits.circuits.sports_players_official.sports_players_probe_true_edges","title":"sports_players_probe_true_edges","text":"<pre><code>sports_players_probe_true_edges(model: PatchableModel, token_positions: bool = False, word_idxs: Dict[str, int] = {}, seq_start_idx: int = 0) -&gt; Set[Edge]\n</code></pre> <p>Wrapper for <code>sports_players_true_edges</code> that does not include the <code>extract_sport</code> section of the circuit. Instead, we extend the <code>lookup</code> MLP stack to the final layer of the model.</p> <p>This is included to make it easier to reproduce the probing results from the post which just probe the MLP stack and ignore the <code>extract_sport</code> section.</p> Source code in <code>auto_circuit/metrics/official_circuits/circuits/sports_players_official.py</code> <pre><code>def sports_players_probe_true_edges(\n    model: PatchableModel,\n    token_positions: bool = False,\n    word_idxs: Dict[str, int] = {},\n    seq_start_idx: int = 0,\n) -&gt; Set[Edge]:\n    \"\"\"\n    Wrapper for\n    [`sports_players_true_edges`][auto_circuit.metrics.official_circuits.circuits.sports_players_official.sports_players_true_edges]\n    that does not include the `extract_sport` section of the circuit. Instead, we extend\n    the `lookup` MLP stack to the final layer of the model.\n\n    This is included to make it easier to reproduce the probing results from the post\n    which just probe the MLP stack and ignore the `extract_sport` section.\n    \"\"\"\n    assert model.cfg.model_name == \"pythia-2.8b-deduped\"\n    assert model.is_factorized, \"Sports Players circuit requires factorized model\"\n    assert model.separate_qkv is False, \"Sports players doesn't support separate QKV\"\n\n    first_name_tok_idx = word_idxs.get(\"first_name_tok\", 0)\n    final_name_tok_idx = word_idxs.get(\"final_name_tok\", 0)\n    non_final_name_toks_idxs = list(range(first_name_tok_idx, final_name_tok_idx))\n    final_tok_idx = word_idxs.get(\"end\", 0)\n\n    if token_positions:\n        assert final_tok_idx &gt; 0, \"Must provide word_idxs if token_positions is True\"\n\n    # Many edge names in this dict will not exist. We filter them at the end.\n    edges_present: Dict[str, List[int]] = defaultdict(list)\n\n    # ------------------------- Concaternate Tokens and Lookup -------------------------\n    # We include every edge between nodes in the first 2 layers at the first two (out of\n    # three) name tokens, except MLP 1 since this can't affect the attention heads at\n    # the final name token, where the final representation of the name is output to the\n    # residual stream at the beginning of layer 2 (the third layer with 0 indexing).\n    #\n    # DeepMind did find a more detailed view of how the final resprentation is a linear\n    # combination of the token embeddings (+MLP 0 embedding). But this was for the case\n    # of 2 name tokens, whereas we are using 3 name tokens (because it provides more\n    # data points) so we use this coarser view.\n    n_heads = model.cfg.n_heads\n    heads_01 = [f\"A{layer}.{head}\" for layer in range(2) for head in range(n_heads)]\n\n    non_final_name_tok_nodes = [\"Resid Start\", \"MLP 0\"] + heads_01\n    for src_node in non_final_name_tok_nodes:\n        for dest_node in non_final_name_tok_nodes:\n            edges_present[f\"{src_node}-&gt;{dest_node}\"].extend(non_final_name_toks_idxs)\n\n    # We include every edge between [[all nodes in the first 2 layers] and [every MLP\n    # from layers 2 onwards]] at the last name token.\n    mlps = [f\"MLP {layer}\" for layer in range(0, model.cfg.n_layers)]\n    final_name_tok_nodes = [\"Resid Start\"] + heads_01 + mlps\n    for src_node in final_name_tok_nodes:\n        for dest_node in final_name_tok_nodes:\n            edges_present[f\"{src_node}-&gt;{dest_node}\"].append(final_name_tok_idx)\n\n    true_edges: Set[Edge] = set()\n    for edge in model.edges:\n        if edge.name in edges_present.keys():\n            if token_positions:\n                assert edge.seq_idx is not None\n                if (edge.seq_idx + seq_start_idx) in edges_present[edge.name]:\n                    true_edges.add(edge)\n            else:\n                true_edges.add(edge)\n    return true_edges\n</code></pre>"},{"location":"reference/metrics/official_circuits/circuits/sports_players_official/#auto_circuit.metrics.official_circuits.circuits.sports_players_official.sports_players_true_edges","title":"sports_players_true_edges","text":"<pre><code>sports_players_true_edges(model: PatchableModel, token_positions: bool = False, word_idxs: Dict[str, int] = {}, seq_start_idx: int = 0) -&gt; Set[Edge]\n</code></pre> <p>The full Sports Players circuit from input to output, as discovered by Rajamanoharan et al. (2023).</p> <p>Read the source code comments for precise details on our interpretation of the circuit. The focus of the paper was on probing for sports features, so the exact set of edges that constitute the circuit is somewhat ambiguous.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>PatchableModel</code> <p>A patchable TransformerLens Pythia 2.8B <code>HookedTransformer</code> model.</p> required <code>token_positions</code> <code>bool</code> <p>Whether to distinguish between token positions when returning the set of circuit edges. If <code>True</code>, we require that the <code>model</code> has <code>seq_len</code> not <code>None</code> (ie. separate edges for each token position) and that <code>word_idxs</code> is provided.</p> <code>False</code> <code>word_idxs</code> <code>Dict[str, int]</code> <p>A dictionary defining the index of specific named tokens in the circuit definition. For this circuit, the required tokens positions are:</p> <ul> <li><code>first_name_tok</code></li> <li><code>final_name_tok</code></li> <li><code>end</code></li> </ul> <code>{}</code> <code>seq_start_idx</code> <code>int</code> <p>Offset to add to all of the token positions in <code>word_idxs</code>. This is useful when using KV caching to skip the common prefix of the prompt.</p> <code>0</code> <p>Returns:</p> Type Description <code>Set[Edge]</code> <p>The set of edges in the circuit.</p> Source code in <code>auto_circuit/metrics/official_circuits/circuits/sports_players_official.py</code> <pre><code>def sports_players_true_edges(\n    model: PatchableModel,\n    token_positions: bool = False,\n    word_idxs: Dict[str, int] = {},\n    seq_start_idx: int = 0,\n) -&gt; Set[Edge]:\n    \"\"\"\n    The full Sports Players circuit from input to output, as discovered by\n    [Rajamanoharan et al. (2023)](https://www.alignmentforum.org/posts/3tqJ65kuTkBh8wrRH/).\n\n    Read the source code comments for precise details on our interpretation of the\n    circuit. The focus of the paper was on probing for sports features, so the exact\n    set of edges that constitute the circuit is somewhat ambiguous.\n\n    Args:\n        model: A patchable TransformerLens Pythia 2.8B `HookedTransformer` model.\n        token_positions: Whether to distinguish between token positions when returning\n            the set of circuit edges. If `True`, we require that the `model` has\n            `seq_len` not `None` (ie. separate edges for each token position) and that\n            `word_idxs` is provided.\n        word_idxs: A dictionary defining the index of specific named tokens in the\n            circuit definition. For this circuit, the required tokens positions are:\n            &lt;ul&gt;\n                &lt;li&gt;&lt;code&gt;first_name_tok&lt;/code&gt;&lt;/li&gt;\n                &lt;li&gt;&lt;code&gt;final_name_tok&lt;/code&gt;&lt;/li&gt;\n                &lt;li&gt;&lt;code&gt;end&lt;/code&gt;&lt;/li&gt;\n            &lt;/ul&gt;\n        seq_start_idx: Offset to add to all of the token positions in `word_idxs`.\n            This is useful when using KV caching to skip the common prefix of the\n            prompt.\n\n    Returns:\n        The set of edges in the circuit.\n    \"\"\"\n    assert model.cfg.model_name == \"pythia-2.8b-deduped\"\n    assert model.is_factorized, \"Sports Players circuit requires factorized model\"\n    assert model.separate_qkv is False, \"Sports players doesn't support separate QKV\"\n\n    first_name_tok_idx = word_idxs.get(\"first_name_tok\", 0)\n    final_name_tok_idx = word_idxs.get(\"final_name_tok\", 0)\n    non_final_name_toks_idxs = list(range(first_name_tok_idx, final_name_tok_idx))\n    final_tok_idx = word_idxs.get(\"end\", 0)\n\n    if token_positions:\n        assert final_tok_idx &gt; 0, \"Must provide word_idxs if token_positions is True\"\n\n    # Many edge names in this dict will not exist. We filter them at the end.\n    edges_present: Dict[str, List[int]] = defaultdict(list)\n\n    # ------------------------- Concaternate Tokens and Lookup -------------------------\n    # We include every edge between nodes in the first 2 layers at the first two (out of\n    # three) name tokens, except MLP 1 since this can't affect the attention heads at\n    # the final name token, where the final representation of the name is output to the\n    # residual stream at the beginning of layer 2 (the third layer with 0 indexing).\n    #\n    # DeepMind did find a more detailed view of how the final resprentation is a linear\n    # combination of the token embeddings (+MLP 0 embedding). But this was for the case\n    # of 2 name tokens, whereas we are using 3 name tokens (because it provides more\n    # data points) so we use this coarser view.\n    n_heads = model.cfg.n_heads\n    heads_01 = [f\"A{layer}.{head}\" for layer in range(2) for head in range(n_heads)]\n\n    non_final_name_tok_nodes = [\"Resid Start\", \"MLP 0\"] + heads_01\n    for src_node in non_final_name_tok_nodes:\n        for dest_node in non_final_name_tok_nodes:\n            edges_present[f\"{src_node}-&gt;{dest_node}\"].extend(non_final_name_toks_idxs)\n\n    # We include every edge between [[all nodes in the first 2 layers] and [every MLP\n    # from layers 2-8]] at the last name token.\n    mlps = [f\"MLP {layer}\" for layer in range(0, 16)]\n    final_name_tok_nodes = [\"Resid Start\"] + heads_01 + mlps\n    for src_node in final_name_tok_nodes:\n        for dest_node in final_name_tok_nodes:\n            edges_present[f\"{src_node}-&gt;{dest_node}\"].append(final_name_tok_idx)\n\n    # --------------------------------- Extract sport ----------------------------------\n    # Lookup to main attention head\n    main_attn_head = \"A16.20\"\n    for src_node in final_name_tok_nodes:\n        edges_present[f\"{src_node}-&gt;{main_attn_head}\"].append(final_name_tok_idx)\n\n    # V-composition from A16.20 to the other important attention heads\n    secondary_attn_heads = [\"A21.9\", \"A22.17\", \"A22.15\", \"A17.30\", \"A19.24\"]\n    for attn_head in secondary_attn_heads:\n        edges_present[f\"{main_attn_head}-&gt;{attn_head}\"].append(final_name_tok_idx)\n\n    # Attention heads to Resid End\n    for attn_head in [main_attn_head] + secondary_attn_heads:\n        edges_present[f\"{attn_head}-&gt;Resid End\"].append(final_tok_idx)\n\n    true_edges: Set[Edge] = set()\n    for edge in model.edges:\n        if edge.name in edges_present.keys():\n            if token_positions:\n                assert edge.seq_idx is not None\n                if (edge.seq_idx + seq_start_idx) in edges_present[edge.name]:\n                    true_edges.add(edge)\n            else:\n                true_edges.add(edge)\n    return true_edges\n</code></pre>"},{"location":"reference/metrics/official_circuits/circuits/tracr/reverse_official/","title":"Reverse official","text":""},{"location":"reference/metrics/official_circuits/circuits/tracr/reverse_official/#auto_circuit.metrics.official_circuits.circuits.tracr.reverse_official","title":"auto_circuit.metrics.official_circuits.circuits.tracr.reverse_official","text":""},{"location":"reference/metrics/official_circuits/circuits/tracr/reverse_official/#auto_circuit.metrics.official_circuits.circuits.tracr.reverse_official-classes","title":"Classes","text":""},{"location":"reference/metrics/official_circuits/circuits/tracr/reverse_official/#auto_circuit.metrics.official_circuits.circuits.tracr.reverse_official-functions","title":"Functions","text":""},{"location":"reference/metrics/official_circuits/circuits/tracr/reverse_official/#auto_circuit.metrics.official_circuits.circuits.tracr.reverse_official.tracr_reverse_acdc_edges","title":"tracr_reverse_acdc_edges","text":"<pre><code>tracr_reverse_acdc_edges(model: PatchableModel, token_positions: bool = False, word_idxs: Dict[str, int] = {}, seq_start_idx: int = 0) -&gt; Set[Edge]\n</code></pre> <p>The canonical circuit for tracr-reverse according to Conmy et al. (2023). Based on the ACDC repo.</p> <p>As discussed in Miller et al. (forthcoming), this circuit is (intended to be) the set of edges that must be preserved when Zero Ablation is used.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>PatchableModel</code> <p>A patchable TransformerLens tracr-reverse <code>HookedTransformer</code> model.</p> required <code>token_positions</code> <code>bool</code> <p>Whether to distinguish between token positions when returning the set of circuit edges. If <code>True</code>, we require that the <code>model</code> has <code>seq_len</code> not <code>None</code> (ie. separate edges for each token position) and that <code>word_idxs</code> is provided.</p> <code>False</code> <code>word_idxs</code> <code>Dict[str, int]</code> <p>A dictionary defining the index of specific named tokens in the circuit definition. This variable is not used in this circuit, instead we assume a sequence of length 6 (including BOS).</p> <code>{}</code> <code>seq_start_idx</code> <code>int</code> <p>Offset to add to all of the token positions in <code>word_idxs</code>. This is useful when using KV caching to skip the common prefix of the prompt.</p> <code>0</code> <p>Returns:</p> Type Description <code>Set[Edge]</code> <p>The set of edges in the circuit.</p> Note <p>The sequence positions assume prompts of length 6 (including BOS), as in tracr/tracr_reverse_len_5_prompts.json</p> Source code in <code>auto_circuit/metrics/official_circuits/circuits/tracr/reverse_official.py</code> <pre><code>def tracr_reverse_acdc_edges(\n    model: PatchableModel,\n    token_positions: bool = False,\n    word_idxs: Dict[str, int] = {},\n    seq_start_idx: int = 0,\n) -&gt; Set[Edge]:\n    \"\"\"\n    The canonical circuit for tracr-reverse according to\n    [Conmy et al. (2023)](https://arxiv.org/abs/2304.14997). Based on the\n    [ACDC repo](https://github.com/ArthurConmy/Automatic-Circuit-Discovery/blob/main/acdc/tracr_task/utils.py).\n\n    As discussed in Miller et al. (forthcoming), this circuit is (intended to be) the\n    set of edges that must be preserved when Zero Ablation is used.\n\n    Args:\n        model: A patchable TransformerLens tracr-reverse `HookedTransformer` model.\n        token_positions: Whether to distinguish between token positions when returning\n            the set of circuit edges. If `True`, we require that the `model` has\n            `seq_len` not `None` (ie. separate edges for each token position) and that\n            `word_idxs` is provided.\n        word_idxs: A dictionary defining the index of specific named tokens in the\n            circuit definition. This variable is not used in this circuit, instead we\n            assume a sequence of length 6 (including BOS).\n        seq_start_idx: Offset to add to all of the token positions in `word_idxs`.\n            This is useful when using KV caching to skip the common prefix of the\n            prompt.\n\n    Returns:\n        The set of edges in the circuit.\n\n    Note:\n        The sequence positions assume prompts of length 6 (including BOS), as in\n        tracr/tracr_reverse_len_5_prompts.json\n    \"\"\"\n    assert model.cfg.model_name == \"tracr-reverse\"\n    assert model.separate_qkv\n\n    # tok_seq_pos = [1, 2, 3, 4, 5]\n    tok_seq_pos = [1, 2, 4, 5]\n    edges_present: Dict[str, List[int]] = {}\n\n    edges_present[\"A3.0-&gt;Resid End\"] = tok_seq_pos\n    edges_present[\"MLP 2-&gt;A3.0.Q\"] = tok_seq_pos\n    edges_present[\"Resid Start-&gt;A3.0.K\"] = tok_seq_pos\n    edges_present[\"Resid Start-&gt;A3.0.V\"] = tok_seq_pos\n    edges_present[\"MLP 1-&gt;MLP 2\"] = tok_seq_pos\n    edges_present[\"MLP 0-&gt;MLP 1\"] = tok_seq_pos\n    edges_present[\"Resid Start-&gt;MLP 1\"] = tok_seq_pos\n    edges_present[\"A0.0-&gt;MLP 0\"] = tok_seq_pos\n    edges_present[\"Resid Start-&gt;MLP 0\"] = tok_seq_pos\n    edges_present[\"Resid Start-&gt;A0.0.V\"] = tok_seq_pos\n\n    true_edges: Set[Edge] = set()\n    for edge in model.edges:\n        if edge.name in edges_present.keys():\n            if token_positions:\n                assert edge.seq_idx is not None\n                if (edge.seq_idx + seq_start_idx) in edges_present[edge.name]:\n                    true_edges.add(edge)\n            else:\n                true_edges.add(edge)\n\n    return true_edges\n</code></pre>"},{"location":"reference/metrics/official_circuits/circuits/tracr/reverse_official/#auto_circuit.metrics.official_circuits.circuits.tracr.reverse_official.tracr_reverse_true_edges","title":"tracr_reverse_true_edges","text":"<pre><code>tracr_reverse_true_edges(model: PatchableModel, token_positions: bool = False, word_idxs: Dict[str, int] = {}, seq_start_idx: int = 0) -&gt; Set[Edge]\n</code></pre> <p>The canonical circuit for tracr-reverse according to Miller et al. (Forthcoming). As discussed in the paper, this circuit is the set of edges that must be preserved when Resample Ablation is used.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>PatchableModel</code> <p>A patchable TransformerLens tracr-reverse <code>HookedTransformer</code> model.</p> required <code>token_positions</code> <code>bool</code> <p>Whether to distinguish between token positions when returning the set of circuit edges. If <code>True</code>, we require that the <code>model</code> has <code>seq_len</code> not <code>None</code> (ie. separate edges for each token position) and that <code>word_idxs</code> is provided.</p> <code>False</code> <code>word_idxs</code> <code>Dict[str, int]</code> <p>A dictionary defining the index of specific named tokens in the circuit definition. This variable is not used in this circuit, instead we assume a sequence of length 6 (including BOS).</p> <code>{}</code> <code>seq_start_idx</code> <code>int</code> <p>Offset to add to all of the token positions in <code>word_idxs</code>. This is useful when using KV caching to skip the common prefix of the prompt.</p> <code>0</code> <p>Returns:</p> Type Description <code>Set[Edge]</code> <p>The set of edges in the circuit.</p> Note <p>The sequence positions assume prompts of length 6 (including BOS), as in tracr/tracr_reverse_len_5_prompts.json</p> Source code in <code>auto_circuit/metrics/official_circuits/circuits/tracr/reverse_official.py</code> <pre><code>def tracr_reverse_true_edges(\n    model: PatchableModel,\n    token_positions: bool = False,\n    word_idxs: Dict[str, int] = {},\n    seq_start_idx: int = 0,\n) -&gt; Set[Edge]:\n    \"\"\"\n    The canonical circuit for tracr-reverse according to Miller et al. (Forthcoming).\n    As discussed in the paper, this circuit is the set of edges that must be preserved\n    when Resample Ablation is used.\n\n    Args:\n        model: A patchable TransformerLens tracr-reverse `HookedTransformer` model.\n        token_positions: Whether to distinguish between token positions when returning\n            the set of circuit edges. If `True`, we require that the `model` has\n            `seq_len` not `None` (ie. separate edges for each token position) and that\n            `word_idxs` is provided.\n        word_idxs: A dictionary defining the index of specific named tokens in the\n            circuit definition. This variable is not used in this circuit, instead we\n            assume a sequence of length 6 (including BOS).\n        seq_start_idx: Offset to add to all of the token positions in `word_idxs`.\n            This is useful when using KV caching to skip the common prefix of the\n            prompt.\n\n    Returns:\n        The set of edges in the circuit.\n\n    Note:\n        The sequence positions assume prompts of length 6 (including BOS), as in\n        tracr/tracr_reverse_len_5_prompts.json\n    \"\"\"\n    assert model.cfg.model_name == \"tracr-reverse\"\n    assert model.separate_qkv\n\n    # tok_seq_pos = [1, 2, 3, 4, 5]\n    tok_seq_pos = [1, 2, 4, 5]\n    edges_present: Dict[str, List[int]] = {}\n    edges_present[\"Resid Start-&gt;A3.0.V\"] = tok_seq_pos\n    edges_present[\"A3.0-&gt;Resid End\"] = tok_seq_pos\n\n    true_edges: Set[Edge] = set()\n    for edge in model.edges:\n        if edge.name in edges_present.keys():\n            if token_positions:\n                assert edge.seq_idx is not None\n                if (edge.seq_idx + seq_start_idx) in edges_present[edge.name]:\n                    true_edges.add(edge)\n            else:\n                true_edges.add(edge)\n\n    return true_edges\n</code></pre>"},{"location":"reference/metrics/official_circuits/circuits/tracr/xproportion_official/","title":"Xproportion official","text":""},{"location":"reference/metrics/official_circuits/circuits/tracr/xproportion_official/#auto_circuit.metrics.official_circuits.circuits.tracr.xproportion_official","title":"auto_circuit.metrics.official_circuits.circuits.tracr.xproportion_official","text":""},{"location":"reference/metrics/official_circuits/circuits/tracr/xproportion_official/#auto_circuit.metrics.official_circuits.circuits.tracr.xproportion_official-classes","title":"Classes","text":""},{"location":"reference/metrics/official_circuits/circuits/tracr/xproportion_official/#auto_circuit.metrics.official_circuits.circuits.tracr.xproportion_official-functions","title":"Functions","text":""},{"location":"reference/metrics/official_circuits/circuits/tracr/xproportion_official/#auto_circuit.metrics.official_circuits.circuits.tracr.xproportion_official.tracr_xproportion_acdc_edges","title":"tracr_xproportion_acdc_edges","text":"<pre><code>tracr_xproportion_acdc_edges(model: PatchableModel, token_positions: bool = False, word_idxs: Dict[str, int] = {}, seq_start_idx: int = 0) -&gt; Set[Edge]\n</code></pre> <p>The canonical circuit for tracr-reverse according to Conmy et al. (2023). Based on the ACDC repo.</p> <p>As discussed in Miller et al. (forthcoming), this circuit is (intended to be) the set of edges that must be preserved when Zero Ablation is used.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>PatchableModel</code> <p>A patchable TransformerLens tracr-reverse <code>HookedTransformer</code> model.</p> required <code>token_positions</code> <code>bool</code> <p>Whether to distinguish between token positions when returning the set of circuit edges. If <code>True</code>, we require that the <code>model</code> has <code>seq_len</code> not <code>None</code> (ie. separate edges for each token position) and that <code>word_idxs</code> is provided.</p> <code>False</code> <code>word_idxs</code> <code>Dict[str, int]</code> <p>A dictionary defining the index of specific named tokens in the circuit definition. This variable is not used in this circuit, instead we assume a sequence of length 6 (including BOS).</p> <code>{}</code> <code>seq_start_idx</code> <code>int</code> <p>Offset to add to all of the token positions in <code>word_idxs</code>. This is useful when using KV caching to skip the common prefix of the prompt.</p> <code>0</code> <p>Returns:</p> Type Description <code>Set[Edge]</code> <p>The set of edges in the circuit.</p> Note <p>The sequence positions assume prompts of length 6 (including BOS), as in tracr/tracr_reverse_len_5_prompts.json</p> Source code in <code>auto_circuit/metrics/official_circuits/circuits/tracr/xproportion_official.py</code> <pre><code>def tracr_xproportion_acdc_edges(\n    model: PatchableModel,\n    token_positions: bool = False,\n    word_idxs: Dict[str, int] = {},\n    seq_start_idx: int = 0,\n) -&gt; Set[Edge]:\n    \"\"\"\n    The canonical circuit for tracr-reverse according to\n    [Conmy et al. (2023)](https://arxiv.org/abs/2304.14997). Based on the\n    [ACDC repo](https://github.com/ArthurConmy/Automatic-Circuit-Discovery/blob/main/acdc/tracr_task/utils.py).\n\n    As discussed in Miller et al. (forthcoming), this circuit is (intended to be) the\n    set of edges that must be preserved when Zero Ablation is used.\n\n    Args:\n        model: A patchable TransformerLens tracr-reverse `HookedTransformer` model.\n        token_positions: Whether to distinguish between token positions when returning\n            the set of circuit edges. If `True`, we require that the `model` has\n            `seq_len` not `None` (ie. separate edges for each token position) and that\n            `word_idxs` is provided.\n        word_idxs: A dictionary defining the index of specific named tokens in the\n            circuit definition. This variable is not used in this circuit, instead we\n            assume a sequence of length 6 (including BOS).\n        seq_start_idx: Offset to add to all of the token positions in `word_idxs`.\n            This is useful when using KV caching to skip the common prefix of the\n            prompt.\n\n    Returns:\n        The set of edges in the circuit.\n\n    Note:\n        The sequence positions assume prompts of length 6 (including BOS), as in\n        tracr/tracr_reverse_len_5_prompts.json\n    \"\"\"\n    assert model.cfg.model_name == \"tracr-xproportion\"\n    assert model.separate_qkv\n\n    tok_seq_pos = [1, 2, 3, 4, 5]\n    edges_present: Dict[str, List[int]] = {}\n\n    edges_present[\"A1.0-&gt;Resid End\"] = tok_seq_pos\n    edges_present[\"Resid Start-&gt;A1.0.Q\"] = tok_seq_pos\n    edges_present[\"Resid Start-&gt;A1.0.K\"] = tok_seq_pos\n    edges_present[\"MLP 0-&gt;A1.0.V\"] = tok_seq_pos\n    edges_present[\"Resid Start-&gt;MLP 0\"] = tok_seq_pos\n\n    true_edges: Set[Edge] = set()\n    for edge in model.edges:\n        if edge.name in edges_present.keys():\n            if token_positions:\n                assert edge.seq_idx is not None\n                if (edge.seq_idx + seq_start_idx) in edges_present[edge.name]:\n                    true_edges.add(edge)\n            else:\n                true_edges.add(edge)\n    return true_edges\n</code></pre>"},{"location":"reference/metrics/official_circuits/circuits/tracr/xproportion_official/#auto_circuit.metrics.official_circuits.circuits.tracr.xproportion_official.tracr_xproportion_official_edges","title":"tracr_xproportion_official_edges","text":"<pre><code>tracr_xproportion_official_edges(model: PatchableModel, token_positions: bool = False, word_idxs: Dict[str, int] = {}, seq_start_idx: int = 0) -&gt; Set[Edge]\n</code></pre> <p>The canonical circuit for tracr-reverse according to Miller et al. (Forthcoming). As discussed in the paper, this circuit is the set of edges that must be preserved when Resample Ablation is used.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>PatchableModel</code> <p>A patchable TransformerLens tracr-reverse <code>HookedTransformer</code> model.</p> required <code>token_positions</code> <code>bool</code> <p>Whether to distinguish between token positions when returning the set of circuit edges. If <code>True</code>, we require that the <code>model</code> has <code>seq_len</code> not <code>None</code> (ie. separate edges for each token position) and that <code>word_idxs</code> is provided.</p> <code>False</code> <code>word_idxs</code> <code>Dict[str, int]</code> <p>A dictionary defining the index of specific named tokens in the circuit definition. This variable is not used in this circuit, instead we assume a sequence of length 6 (including BOS).</p> <code>{}</code> <code>seq_start_idx</code> <code>int</code> <p>Offset to add to all of the token positions in <code>word_idxs</code>. This is useful when using KV caching to skip the common prefix of the prompt.</p> <code>0</code> <p>Returns:</p> Type Description <code>Set[Edge]</code> <p>The set of edges in the circuit.</p> Note <p>The sequence positions assume prompts of length 6 (including BOS), as in tracr/tracr_xproportion_len_5_prompts.json</p> Source code in <code>auto_circuit/metrics/official_circuits/circuits/tracr/xproportion_official.py</code> <pre><code>def tracr_xproportion_official_edges(\n    model: PatchableModel,\n    token_positions: bool = False,\n    word_idxs: Dict[str, int] = {},\n    seq_start_idx: int = 0,\n) -&gt; Set[Edge]:\n    \"\"\"\n    The canonical circuit for tracr-reverse according to Miller et al. (Forthcoming).\n    As discussed in the paper, this circuit is the set of edges that must be preserved\n    when Resample Ablation is used.\n\n    Args:\n        model: A patchable TransformerLens tracr-reverse `HookedTransformer` model.\n        token_positions: Whether to distinguish between token positions when returning\n            the set of circuit edges. If `True`, we require that the `model` has\n            `seq_len` not `None` (ie. separate edges for each token position) and that\n            `word_idxs` is provided.\n        word_idxs: A dictionary defining the index of specific named tokens in the\n            circuit definition. This variable is not used in this circuit, instead we\n            assume a sequence of length 6 (including BOS).\n        seq_start_idx: Offset to add to all of the token positions in `word_idxs`.\n            This is useful when using KV caching to skip the common prefix of the\n            prompt.\n\n    Returns:\n        The set of edges in the circuit.\n\n    Note:\n        The sequence positions assume prompts of length 6 (including BOS), as in\n        tracr/tracr_xproportion_len_5_prompts.json\n    \"\"\"\n    assert model.cfg.model_name == \"tracr-xproportion\"\n    assert model.separate_qkv\n\n    tok_seq_pos = [1, 2, 3, 4, 5]\n    edges_present: Dict[str, List[int]] = {}\n    edges_present[\"Resid Start-&gt;MLP 0\"] = tok_seq_pos\n    edges_present[\"MLP 0-&gt;A1.0.V\"] = tok_seq_pos\n    edges_present[\"A1.0-&gt;Resid End\"] = tok_seq_pos\n\n    true_edges: Set[Edge] = set()\n    for edge in model.edges:\n        if edge.name in edges_present.keys():\n            if token_positions:\n                assert edge.seq_idx is not None\n                if (edge.seq_idx + seq_start_idx) in edges_present[edge.name]:\n                    true_edges.add(edge)\n            else:\n                true_edges.add(edge)\n    return true_edges\n</code></pre>"},{"location":"reference/metrics/prune_metrics/answer_diff/","title":"Answer diff","text":""},{"location":"reference/metrics/prune_metrics/answer_diff/#auto_circuit.metrics.prune_metrics.answer_diff","title":"auto_circuit.metrics.prune_metrics.answer_diff","text":""},{"location":"reference/metrics/prune_metrics/answer_diff/#auto_circuit.metrics.prune_metrics.answer_diff-attributes","title":"Attributes","text":""},{"location":"reference/metrics/prune_metrics/answer_diff/#auto_circuit.metrics.prune_metrics.answer_diff-classes","title":"Classes","text":""},{"location":"reference/metrics/prune_metrics/answer_diff/#auto_circuit.metrics.prune_metrics.answer_diff-functions","title":"Functions","text":""},{"location":"reference/metrics/prune_metrics/answer_diff/#auto_circuit.metrics.prune_metrics.answer_diff.measure_answer_diff","title":"measure_answer_diff","text":"<pre><code>measure_answer_diff(model: PatchableModel, test_loader: PromptDataLoader, circuit_outs: CircuitOutputs, prob_func: Literal['log_softmax', 'softmax', 'logits'] = 'logits') -&gt; Measurements\n</code></pre> <p>The average difference in the logits (or some function of them) between the correct answers and the incorrect answers.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>PatchableModel</code> <p>Not used.</p> required <code>test_loader</code> <code>PromptDataLoader</code> <p>The dataloader on which the <code>circuit_outs</code> were calculated.</p> required <code>circuit_outs</code> <code>CircuitOutputs</code> <p>The outputs of the ablated model for each circuit size.</p> required <code>prob_func</code> <code>Literal['log_softmax', 'softmax', 'logits']</code> <p>The function to apply to the logits before calculating the answer difference.</p> <code>'logits'</code> <p>Returns:</p> Type Description <code>Measurements</code> <p>A list of tuples, where the first element is the number of edges pruned and the second element is the average answer difference for that number of edges.</p> Source code in <code>auto_circuit/metrics/prune_metrics/answer_diff.py</code> <pre><code>def measure_answer_diff(\n    model: PatchableModel,\n    test_loader: PromptDataLoader,\n    circuit_outs: CircuitOutputs,\n    prob_func: Literal[\"log_softmax\", \"softmax\", \"logits\"] = \"logits\",\n) -&gt; Measurements:\n    \"\"\"\n    The average difference in the logits (or some function of them) between the correct\n    answers and the incorrect answers.\n\n    Args:\n        model: Not used.\n        test_loader: The dataloader on which the `circuit_outs` were calculated.\n        circuit_outs: The outputs of the ablated model for each circuit size.\n        prob_func: The function to apply to the logits before calculating the answer\n            difference.\n\n    Returns:\n        A list of tuples, where the first element is the number of edges pruned and the\n            second element is the average answer difference for that number of edges.\n    \"\"\"\n    measurements = []\n    if prob_func == \"softmax\":\n        apply_prob_func = t.nn.functional.softmax\n    elif prob_func == \"log_softmax\":\n        apply_prob_func = t.nn.functional.log_softmax\n    else:\n        assert prob_func == \"logits\"\n        apply_prob_func = identity\n\n    for edge_count, batch_outs in (pruned_out_pbar := tqdm(circuit_outs.items())):\n        pruned_out_pbar.set_description_str(f\"Answer Diff for {edge_count} edges\")\n        avg_ans_diff = []\n        for batch in test_loader:\n            batch_probs = apply_prob_func(batch_outs[batch.key], dim=-1)\n            avg_ans_diff.append(batch_avg_answer_diff(batch_probs, batch))\n        # PromptDataLoaders have all batches the same size, so we mean the batch means\n        measurements.append((edge_count, t.stack(avg_ans_diff).mean().item()))\n    return measurements\n</code></pre>"},{"location":"reference/metrics/prune_metrics/answer_diff_percent/","title":"Answer diff percent","text":""},{"location":"reference/metrics/prune_metrics/answer_diff_percent/#auto_circuit.metrics.prune_metrics.answer_diff_percent","title":"auto_circuit.metrics.prune_metrics.answer_diff_percent","text":""},{"location":"reference/metrics/prune_metrics/answer_diff_percent/#auto_circuit.metrics.prune_metrics.answer_diff_percent-attributes","title":"Attributes","text":""},{"location":"reference/metrics/prune_metrics/answer_diff_percent/#auto_circuit.metrics.prune_metrics.answer_diff_percent-classes","title":"Classes","text":""},{"location":"reference/metrics/prune_metrics/answer_diff_percent/#auto_circuit.metrics.prune_metrics.answer_diff_percent-functions","title":"Functions","text":""},{"location":"reference/metrics/prune_metrics/answer_diff_percent/#auto_circuit.metrics.prune_metrics.answer_diff_percent.answer_diff_percent","title":"answer_diff_percent","text":"<pre><code>answer_diff_percent(model: PatchableModel, test_loader: PromptDataLoader, circuit_outs: CircuitOutputs, prob_func: Literal['log_softmax', 'softmax', 'logits'] = 'logits', diff_of_means: bool = True) -&gt; Tuple[Measurements, Measurements, List[Tuple[int, Tensor]]]\n</code></pre> <p>The average percentage of the difference in the logits (or some function of them) between the correct answers and the incorrect answers in the full model that is recovered by the circuit.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>PatchableModel</code> <p>The model on which <code>circuit_outs</code> was calculated.</p> required <code>test_loader</code> <code>PromptDataLoader</code> <p>The dataloader on which the <code>circuit_outs</code> was calculated.</p> required <code>circuit_outs</code> <code>CircuitOutputs</code> <p>The outputs of the ablated model for each circuit size.</p> required <code>prob_func</code> <code>Literal['log_softmax', 'softmax', 'logits']</code> <p>The function to apply to the logits before calculating the answer difference.</p> <code>'logits'</code> <code>diff_of_means</code> <code>bool</code> <p>Whether to calculate the difference of means (<code>True</code>) or the mean of differences (<code>False</code>). This is included because the IOI paper uses the difference of means.</p> <code>True</code> <p>Returns:</p> Type Description <code>Tuple[Measurements, Measurements, List[Tuple[int, Tensor]]]</code> <p>A tuple of three elements:</p> <ol> <li>         A list of tuples, where the first element is the number of edges         in the circuit and the second element is the average answer         percent for that number of edges.     </li> <li>         A list of tuples, where the first element is the number of edges         in the circuit and the second element is the standard deviation of         the answer percents for that number of edges.     </li> <li>         A list of tuples, where the first element is the number of edges         in the circuit and the second element is a tensor of the answer         percents for each individual input.     </li> </ol> Source code in <code>auto_circuit/metrics/prune_metrics/answer_diff_percent.py</code> <pre><code>def answer_diff_percent(\n    model: PatchableModel,\n    test_loader: PromptDataLoader,\n    circuit_outs: CircuitOutputs,\n    prob_func: Literal[\"log_softmax\", \"softmax\", \"logits\"] = \"logits\",\n    diff_of_means: bool = True,\n) -&gt; Tuple[Measurements, Measurements, List[Tuple[int, t.Tensor]]]:\n    \"\"\"\n    The average percentage of the difference in the logits (or some function of them)\n    between the correct answers and the incorrect answers in the full model that is\n    recovered by the circuit.\n\n    Args:\n        model: The model on which `circuit_outs` was calculated.\n        test_loader: The dataloader on which the `circuit_outs` was calculated.\n        circuit_outs: The outputs of the ablated model for each circuit size.\n        prob_func: The function to apply to the logits before calculating the answer\n            difference.\n        diff_of_means: Whether to calculate the difference of means (`True`) or the mean\n            of differences (`False`). This is included because the IOI paper uses the\n            difference of means.\n\n    Returns:\n        A tuple of three elements:\n            &lt;ol&gt;\n                &lt;li&gt;\n                    A list of tuples, where the first element is the number of edges\n                    in the circuit and the second element is the average answer\n                    percent for that number of edges.\n                &lt;/li&gt;\n                &lt;li&gt;\n                    A list of tuples, where the first element is the number of edges\n                    in the circuit and the second element is the standard deviation of\n                    the answer percents for that number of edges.\n                &lt;/li&gt;\n                &lt;li&gt;\n                    A list of tuples, where the first element is the number of edges\n                    in the circuit and the second element is a tensor of the answer\n                    percents for each individual input.\n                &lt;/li&gt;\n            &lt;/ol&gt;\n\n\n    \"\"\"\n    means: Measurements = []\n    standard_devs: Measurements = []\n    points: List[Tuple[int, t.Tensor]] = []\n    if prob_func == \"softmax\":\n        apply_prob_func = t.nn.functional.softmax\n    elif prob_func == \"log_softmax\":\n        apply_prob_func = t.nn.functional.log_softmax\n    else:\n        assert prob_func == \"logits\"\n        apply_prob_func = identity\n\n    batch_default_probs: Dict[BatchKey, t.Tensor] = {}\n    for batch in test_loader:\n        default_out = model(batch.clean)[model.out_slice]\n        batch_val = apply_prob_func(default_out, dim=-1)\n        batch_default_probs[batch.key] = batch_val\n\n    for edge_count, batch_outs in (pruned_out_pbar := tqdm(circuit_outs.items())):\n        pruned_out_pbar.set_description_str(f\"Answer Diff for {edge_count} edges\")\n        # PromptDataLoaders have all batches the same size, so we mean the batch means\n        if diff_of_means:\n            pred_answer_diffs, target_answer_diffs = [], []\n            for batch in test_loader:\n                circ_probs = apply_prob_func(batch_outs[batch.key], dim=-1)\n                default_probs = batch_default_probs[batch.key]\n                pred_answer_diffs.append(batch_avg_answer_diff(circ_probs, batch))\n                target_answer_diffs.append(batch_avg_answer_diff(default_probs, batch))\n            mean_pred_diff = t.stack(pred_answer_diffs).mean().item()\n            mean_target_diff = t.stack(target_answer_diffs).mean().item()\n            means.append((edge_count, (mean_pred_diff / mean_target_diff) * 100))\n            standard_devs.append((edge_count, 0.0))\n            points.append((edge_count, t.tensor([])))\n        else:\n            ans_diff_percents = []\n            for batch in test_loader:\n                circ_probs = apply_prob_func(batch_outs[batch.key], dim=-1)\n                default_probs = batch_default_probs[batch.key]\n                circ_perc = batch_answer_diff_percents(circ_probs, default_probs, batch)\n                ans_diff_percents.append(circ_perc)\n            ans_diff_percents = t.cat(ans_diff_percents, dim=0)\n            means.append((edge_count, ans_diff_percents.mean().item()))\n            standard_devs.append((edge_count, ans_diff_percents.std().item()))\n            points.append((edge_count, ans_diff_percents))\n    return means, standard_devs, points\n</code></pre>"},{"location":"reference/metrics/prune_metrics/answer_diff_percent/#auto_circuit.metrics.prune_metrics.answer_diff_percent.measure_answer_diff_percent","title":"measure_answer_diff_percent","text":"<pre><code>measure_answer_diff_percent(model: PatchableModel, test_loader: PromptDataLoader, circuit_outs: CircuitOutputs, prob_func: Literal['log_softmax', 'softmax', 'logits'] = 'logits', diff_of_means: bool = True) -&gt; Measurements\n</code></pre> <p>Wrapper of <code>answer_diff_percent</code> that returns only the average answer difference percentage (the first element of the tuple).</p> Source code in <code>auto_circuit/metrics/prune_metrics/answer_diff_percent.py</code> <pre><code>def measure_answer_diff_percent(\n    model: PatchableModel,\n    test_loader: PromptDataLoader,\n    circuit_outs: CircuitOutputs,\n    prob_func: Literal[\"log_softmax\", \"softmax\", \"logits\"] = \"logits\",\n    diff_of_means: bool = True,\n) -&gt; Measurements:\n    \"\"\"\n    Wrapper of\n    [`answer_diff_percent`][auto_circuit.metrics.prune_metrics.answer_diff_percent.answer_diff_percent]\n    that returns only the average answer difference\n    percentage (the first element of the tuple).\n    \"\"\"\n    return answer_diff_percent(\n        model, test_loader, circuit_outs, prob_func, diff_of_means\n    )[0]\n</code></pre>"},{"location":"reference/metrics/prune_metrics/answer_value/","title":"Answer value","text":""},{"location":"reference/metrics/prune_metrics/answer_value/#auto_circuit.metrics.prune_metrics.answer_value","title":"auto_circuit.metrics.prune_metrics.answer_value","text":""},{"location":"reference/metrics/prune_metrics/answer_value/#auto_circuit.metrics.prune_metrics.answer_value-attributes","title":"Attributes","text":""},{"location":"reference/metrics/prune_metrics/answer_value/#auto_circuit.metrics.prune_metrics.answer_value-classes","title":"Classes","text":""},{"location":"reference/metrics/prune_metrics/answer_value/#auto_circuit.metrics.prune_metrics.answer_value-functions","title":"Functions","text":""},{"location":"reference/metrics/prune_metrics/answer_value/#auto_circuit.metrics.prune_metrics.answer_value.measure_answer_val","title":"measure_answer_val","text":"<pre><code>measure_answer_val(model: PatchableModel, test_loader: PromptDataLoader, circuit_outs: CircuitOutputs, prob_func: Literal['log_softmax', 'softmax', 'logits'] = 'logits', wrong_answer: bool = False) -&gt; Measurements\n</code></pre> <p>The average value of the logits (or some function of them) for the correct answers.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>PatchableModel</code> <p>Not used.</p> required <code>test_loader</code> <code>PromptDataLoader</code> <p>The dataloader on which the <code>circuit_outs</code> were calculated.</p> required <code>circuit_outs</code> <code>CircuitOutputs</code> <p>The outputs of the ablated model for each circuit size.</p> required <code>prob_func</code> <code>Literal['log_softmax', 'softmax', 'logits']</code> <p>The function to apply to the logits before calculating the answer value.</p> <code>'logits'</code> <code>wrong_answer</code> <code>bool</code> <p>Whether to calculate the value for the wrong answers instead of the correct answers.</p> <code>False</code> <p>Returns:</p> Type Description <code>Measurements</code> <p>A list of tuples, where the first element is the number of edges pruned and the second element is the average answer value for that number of edges.</p> Source code in <code>auto_circuit/metrics/prune_metrics/answer_value.py</code> <pre><code>def measure_answer_val(\n    model: PatchableModel,\n    test_loader: PromptDataLoader,\n    circuit_outs: CircuitOutputs,\n    prob_func: Literal[\"log_softmax\", \"softmax\", \"logits\"] = \"logits\",\n    wrong_answer: bool = False,\n) -&gt; Measurements:\n    \"\"\"\n    The average value of the logits (or some function of them) for the correct answers.\n\n    Args:\n        model: Not used.\n        test_loader: The dataloader on which the `circuit_outs` were calculated.\n        circuit_outs: The outputs of the ablated model for each circuit size.\n        prob_func: The function to apply to the logits before calculating the answer\n            value.\n        wrong_answer: Whether to calculate the value for the wrong answers instead of\n            the correct answers.\n\n    Returns:\n        A list of tuples, where the first element is the number of edges pruned and the\n            second element is the average answer value for that number of edges.\n    \"\"\"\n    measurements = []\n    if prob_func == \"softmax\":\n        apply_prob_func = t.nn.functional.softmax\n    elif prob_func == \"log_softmax\":\n        apply_prob_func = t.nn.functional.log_softmax\n    else:\n        assert prob_func == \"logits\"\n        apply_prob_func = identity\n\n    for edge_count, pruned_out in (pruned_out_pbar := tqdm(circuit_outs.items())):\n        pruned_out_pbar.set_description_str(f\"Answer Value for {edge_count} edges\")\n        avg_ans_probs = []\n        for batch in test_loader:\n            batch_probs = apply_prob_func(pruned_out[batch.key], dim=-1)\n            avg_ans_probs.append(batch_avg_answer_val(batch_probs, batch, wrong_answer))\n        # PromptDataLoaders have all batches the same size, so we mean the batch means\n        measurements.append((edge_count, t.stack(avg_ans_probs).mean().item()))\n    return measurements\n</code></pre>"},{"location":"reference/metrics/prune_metrics/correct_answer_percent/","title":"Correct answer percent","text":""},{"location":"reference/metrics/prune_metrics/correct_answer_percent/#auto_circuit.metrics.prune_metrics.correct_answer_percent","title":"auto_circuit.metrics.prune_metrics.correct_answer_percent","text":""},{"location":"reference/metrics/prune_metrics/correct_answer_percent/#auto_circuit.metrics.prune_metrics.correct_answer_percent-attributes","title":"Attributes","text":""},{"location":"reference/metrics/prune_metrics/correct_answer_percent/#auto_circuit.metrics.prune_metrics.correct_answer_percent-classes","title":"Classes","text":""},{"location":"reference/metrics/prune_metrics/correct_answer_percent/#auto_circuit.metrics.prune_metrics.correct_answer_percent-functions","title":"Functions","text":""},{"location":"reference/metrics/prune_metrics/correct_answer_percent/#auto_circuit.metrics.prune_metrics.correct_answer_percent.measure_correct_ans_percent","title":"measure_correct_ans_percent","text":"<pre><code>measure_correct_ans_percent(model: PatchableModel, dataloader: PromptDataLoader, pruned_outs: CircuitOutputs, out_of_correct_and_incorrect_answers: bool = False) -&gt; Measurements\n</code></pre> <p>Percentage of outputs for which the correct answer has the highest logit.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>PatchableModel</code> <p>Not used.</p> required <code>dataloader</code> <code>PromptDataLoader</code> <p>The dataloader on which the <code>pruned_outs</code> were calculated.</p> required <code>pruned_outs</code> <code>CircuitOutputs</code> <p>The outputs of the ablated model for each circuit size.</p> required <code>out_of_correct_and_incorrect_answers</code> <code>bool</code> <p>Whether to calculate the proportion of prompts for which the correct answer has a higher logit than the incorrect answers (<code>True</code>). Otherwise, calculate the proportion of prompts for which the correct answer has the highest of all logits (<code>False</code>).</p> <p>This is useful when you are particularly interested in the counterfactual comparison to the corrupt prompts. For example, in the Sports Player post Rajamanoharan et al. (2023) look at the proportion of prompts for which the correct sport has a greater logit than the two other sports.</p> <code>False</code> Note <p>This function assumes that each prompt in <code>dataloader</code> has only one correct     answer. If not, an error will be raised.</p> Source code in <code>auto_circuit/metrics/prune_metrics/correct_answer_percent.py</code> <pre><code>def measure_correct_ans_percent(\n    model: PatchableModel,\n    dataloader: PromptDataLoader,\n    pruned_outs: CircuitOutputs,\n    out_of_correct_and_incorrect_answers: bool = False,\n) -&gt; Measurements:\n    \"\"\"\n    Percentage of outputs for which the correct answer has the highest logit.\n\n    Args:\n        model: Not used.\n        dataloader: The dataloader on which the `pruned_outs` were calculated.\n        pruned_outs: The outputs of the ablated model for each circuit size.\n        out_of_correct_and_incorrect_answers: Whether to calculate the proportion of\n            prompts for which the correct answer has a higher logit than the incorrect\n            answers (`True`). Otherwise, calculate the proportion of prompts for which\n            the correct answer has the highest of all logits (`False`).\n\n            This is useful when you are particularly interested in the counterfactual\n            comparison to the corrupt prompts. For example, in the Sports Player post\n            Rajamanoharan et al.\n            [(2023)](https://www.alignmentforum.org/posts/3tqJ65kuTkBh8wrRH/)\n            look at the proportion of prompts for which the correct sport has a\n            greater logit than the two other sports.\n\n\n    Note:\n        This function assumes that each prompt in `dataloader` has only one correct\n            answer. If not, an error will be raised.\n    \"\"\"\n    measurements = []\n    for edge_count, pruned_out in (pruned_out_pbar := tqdm(pruned_outs.items())):\n        pruned_out_pbar.set_description_str(f\"Correct Percent for {edge_count} edges\")\n        correct_proportions = []\n        for batch in dataloader:\n            assert isinstance(batch.answers, t.Tensor)\n            logits = pruned_out[batch.key]\n            if out_of_correct_and_incorrect_answers:\n                correct_proportn = correct_answer_greater_than_incorrect_proportion(\n                    logits, batch\n                )\n            else:\n                correct_proportn = correct_answer_proportion(logits, batch)\n            correct_proportions.append(correct_proportn)\n        # PromptDataLoaders have all batches the same size, so we mean the batch means\n        correct_proportion = t.stack(correct_proportions).float().mean().item() * 100\n        measurements.append((edge_count, correct_proportion))\n    return measurements\n</code></pre>"},{"location":"reference/metrics/prune_metrics/kl_div/","title":"Kl div","text":""},{"location":"reference/metrics/prune_metrics/kl_div/#auto_circuit.metrics.prune_metrics.kl_div","title":"auto_circuit.metrics.prune_metrics.kl_div","text":""},{"location":"reference/metrics/prune_metrics/kl_div/#auto_circuit.metrics.prune_metrics.kl_div-attributes","title":"Attributes","text":""},{"location":"reference/metrics/prune_metrics/kl_div/#auto_circuit.metrics.prune_metrics.kl_div-classes","title":"Classes","text":""},{"location":"reference/metrics/prune_metrics/kl_div/#auto_circuit.metrics.prune_metrics.kl_div-functions","title":"Functions","text":""},{"location":"reference/metrics/prune_metrics/kl_div/#auto_circuit.metrics.prune_metrics.kl_div.measure_kl_div","title":"measure_kl_div","text":"<pre><code>measure_kl_div(model: PatchableModel, dataloader: PromptDataLoader, circuit_outs: CircuitOutputs, compare_to_clean: bool = True) -&gt; Measurements\n</code></pre> <p>Average KL divergence between the full model and the circuits.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>PatchableModel</code> <p>The model on which <code>circuit_outs</code> was calculated.</p> required <code>dataloader</code> <code>PromptDataLoader</code> <p>The dataloader on which the <code>circuit_outs</code> was calculated.</p> required <code>circuit_outs</code> <code>CircuitOutputs</code> <p>The outputs of the ablated model for each circuit size.</p> required <code>compare_to_clean</code> <code>bool</code> <p>Whether to compare the circuit output to the full model on the clean (<code>True</code>) or corrupt (<code>False</code>) prompt.</p> <code>True</code> <p>Returns:</p> Type Description <code>Measurements</code> <p>A list of tuples, where the first element is the number of edges pruned and the second element is the average KL divergence for that number of edges.</p> Source code in <code>auto_circuit/metrics/prune_metrics/kl_div.py</code> <pre><code>def measure_kl_div(\n    model: PatchableModel,\n    dataloader: PromptDataLoader,\n    circuit_outs: CircuitOutputs,\n    compare_to_clean: bool = True,\n) -&gt; Measurements:\n    \"\"\"\n    Average KL divergence between the full model and the circuits.\n\n    Args:\n        model: The model on which `circuit_outs` was calculated.\n        dataloader: The dataloader on which the `circuit_outs` was calculated.\n        circuit_outs: The outputs of the ablated model for each circuit size.\n        compare_to_clean: Whether to compare the circuit output to the full model on the\n            clean (`True`) or corrupt (`False`) prompt.\n\n    Returns:\n        A list of tuples, where the first element is the number of edges pruned and the\n            second element is the average KL divergence for that number of edges.\n    \"\"\"\n    circuit_kl_divs: Measurements = []\n    default_logprobs: Dict[BatchKey, t.Tensor] = {}\n    with t.inference_mode():\n        for batch in dataloader:\n            default_batch = batch.clean if compare_to_clean else batch.corrupt\n            logits = model(default_batch)[model.out_slice]\n            default_logprobs[batch.key] = log_softmax(logits, dim=-1)\n\n    for edge_count, circuit_out in (pruned_out_pbar := tqdm(circuit_outs.items())):\n        pruned_out_pbar.set_description_str(f\"KL Div for {edge_count} edges\")\n        circuit_logprob_list: List[t.Tensor] = []\n        default_logprob_list: List[t.Tensor] = []\n        for batch in dataloader:\n            circuit_logprob_list.append(log_softmax(circuit_out[batch.key], dim=-1))\n            default_logprob_list.append(default_logprobs[batch.key])\n        kl = multibatch_kl_div(t.cat(circuit_logprob_list), t.cat(default_logprob_list))\n\n        # Numerical errors can cause tiny negative values in KL divergence\n        circuit_kl_divs.append((edge_count, max(kl.item(), 0)))\n    return circuit_kl_divs\n</code></pre>"},{"location":"reference/metrics/prune_metrics/measure_prune_metrics/","title":"Measure prune metrics","text":""},{"location":"reference/metrics/prune_metrics/measure_prune_metrics/#auto_circuit.metrics.prune_metrics.measure_prune_metrics","title":"auto_circuit.metrics.prune_metrics.measure_prune_metrics","text":""},{"location":"reference/metrics/prune_metrics/measure_prune_metrics/#auto_circuit.metrics.prune_metrics.measure_prune_metrics-attributes","title":"Attributes","text":""},{"location":"reference/metrics/prune_metrics/measure_prune_metrics/#auto_circuit.metrics.prune_metrics.measure_prune_metrics-classes","title":"Classes","text":""},{"location":"reference/metrics/prune_metrics/measure_prune_metrics/#auto_circuit.metrics.prune_metrics.measure_prune_metrics-functions","title":"Functions","text":""},{"location":"reference/metrics/prune_metrics/measure_prune_metrics/#auto_circuit.metrics.prune_metrics.measure_prune_metrics.measure_prune_metrics","title":"measure_prune_metrics","text":"<pre><code>measure_prune_metrics(ablation_types: List[AblationType], metrics: List[PruneMetric], task_prune_scores: TaskPruneScores, patch_type: PatchType, reverse_clean_corrupt: bool = False, test_edge_counts: Optional[List[int]] = None) -&gt; AblationMeasurements\n</code></pre> <p>Measure a set of circuit metrics for each <code>Task</code>s and each <code>PruneAlgos</code> in the given <code>task_prune_scores</code>.</p> <p>Parameters:</p> Name Type Description Default <code>ablation_types</code> <code>List[AblationType]</code> <p>The types of ablation to test.</p> required <code>metrics</code> <code>List[PruneMetric]</code> <p>The metrics to measure.</p> required <code>task_prune_scores</code> <code>TaskPruneScores</code> <p>The edge scores for each task and each algorithm.</p> required <code>patch_type</code> <code>PatchType</code> <p>Whether to ablate the circuit or the complement.</p> required <code>reverse_clean_corrupt</code> <code>bool</code> <p>Reverse clean and corrupt (for input and patches).</p> <code>False</code> <code>test_edge_counts</code> <code>Optional[List[int]]</code> <p>The set of [number of edges to prune] for each task and algorithm.</p> <code>None</code> <p>Returns:</p> Type Description <code>AblationMeasurements</code> <p>A nested dictionary of measurements for each ablation type, metric, task, and algorithm (in that order).</p> Source code in <code>auto_circuit/metrics/prune_metrics/measure_prune_metrics.py</code> <pre><code>def measure_prune_metrics(\n    ablation_types: List[AblationType],\n    metrics: List[PruneMetric],\n    task_prune_scores: TaskPruneScores,\n    patch_type: PatchType,\n    reverse_clean_corrupt: bool = False,\n    test_edge_counts: Optional[List[int]] = None,\n) -&gt; AblationMeasurements:\n    \"\"\"\n    Measure a set of circuit metrics for each\n    [`Task`s][auto_circuit.tasks.Task] and each\n    [`PruneAlgos`][auto_circuit.prune_algos.prune_algos.PruneAlgo] in the given\n    `task_prune_scores`.\n\n    Args:\n        ablation_types: The types of ablation to test.\n        metrics: The metrics to measure.\n        task_prune_scores: The edge scores for each task and each algorithm.\n        patch_type: Whether to ablate the circuit or the complement.\n        reverse_clean_corrupt: Reverse clean and corrupt (for input and patches).\n        test_edge_counts: The set of [number of edges to prune] for each task and\n            algorithm.\n\n    Returns:\n        A nested dictionary of measurements for each ablation type, metric, task, and\n            algorithm (in that order).\n    \"\"\"\n    measurements: AblationMeasurements = defaultdict(double_default_factory)\n    for task_key, algo_prune_scores in (task_pbar := tqdm(task_prune_scores.items())):\n        task = TASK_DICT[task_key]\n        task_pbar.set_description_str(f\"Task: {task.name}\")\n        test_loader = task.test_loader\n        for algo_key, prune_scores in (algo_pbar := tqdm(algo_prune_scores.items())):\n            algo = PRUNE_ALGO_DICT[algo_key]\n            algo_pbar.set_description_str(f\"Pruning with {algo.name}\")\n            default_edge_counts = edge_counts_util(\n                edges=task.model.edges,\n                test_counts=None,\n                prune_scores=prune_scores,\n                true_edge_count=task.true_edge_count,\n            )\n            for ablation_type in (ablation_pbar := tqdm(ablation_types)):\n                ablation_pbar.set_description_str(f\"Ablating with {ablation_type}\")\n                circuit_outs: CircuitOutputs = run_circuits(\n                    model=task.model,\n                    dataloader=test_loader,\n                    test_edge_counts=test_edge_counts or default_edge_counts,\n                    prune_scores=prune_scores,\n                    patch_type=patch_type,\n                    ablation_type=ablation_type,\n                    reverse_clean_corrupt=reverse_clean_corrupt,\n                )\n                for metric in (metric_pbar := tqdm(metrics)):\n                    metric_pbar.set_description_str(f\"Measuring {metric.name}\")\n                    measurement = metric.metric_func(\n                        task.model, task.test_loader, circuit_outs\n                    )\n                    measurements[ablation_type][metric.key][task.key][\n                        algo.key\n                    ] = measurement\n                del circuit_outs\n            t.cuda.empty_cache()\n    return measurements\n</code></pre>"},{"location":"reference/metrics/prune_metrics/measure_prune_metrics/#auto_circuit.metrics.prune_metrics.measure_prune_metrics.measurement_figs","title":"measurement_figs","text":"<pre><code>measurement_figs(measurements: AblationMeasurements, auc_plots: bool = False) -&gt; Tuple[Figure, ...]\n</code></pre> <p>Plot the measurements from <code>measure_prune_metrics</code> as a set of Plotly figures (one for each ablation type and metric).</p> <p>Optionally include average Area Under the Curve (AUC) plots for each metric.</p> <p>Parameters:</p> Name Type Description Default <code>measurements</code> <code>AblationMeasurements</code> <p>The measurements to plot.</p> required <code>auc_plots</code> <code>bool</code> <p>Whether to include the average AUC plots.</p> <code>False</code> <p>Returns:</p> Type Description <code>Tuple[Figure, ...]</code> <p>A tuple of Plotly figures.</p> Source code in <code>auto_circuit/metrics/prune_metrics/measure_prune_metrics.py</code> <pre><code>def measurement_figs(\n    measurements: AblationMeasurements, auc_plots: bool = False\n) -&gt; Tuple[go.Figure, ...]:\n    \"\"\"\n    Plot the measurements from\n    [`measure_prune_metrics`][auto_circuit.metrics.prune_metrics.measure_prune_metrics]\n    as a set of Plotly figures (one for each ablation type and metric).\n\n    Optionally include average Area Under the Curve (AUC) plots for each metric.\n\n    Args:\n        measurements: The measurements to plot.\n        auc_plots: Whether to include the average AUC plots.\n\n    Returns:\n        A tuple of Plotly figures.\n    \"\"\"\n    figs = []\n    for ablation_type, metric_measurements in measurements.items():\n        for metric_key, task_measurements in metric_measurements.items():\n            token_circuit = TASK_DICT[list(task_measurements.keys())[0]].token_circuit\n            if metric_key not in PRUNE_METRIC_DICT:\n                print(f\"Skipping unknown metric: {metric_key}\")\n                continue\n            metric = PRUNE_METRIC_DICT[metric_key]\n            data, y_max = [], 0.0\n            for task_key, algo_measurements in task_measurements.items():\n                task = TASK_DICT[task_key]\n                # Assert all tasks have the same token_circuit value\n                assert task.token_circuit == token_circuit\n\n                for algo_key, points in algo_measurements.items():\n                    algo = PRUNE_ALGO_DICT[algo_key]\n                    if len(points) &gt; 1:\n                        for x, y in points:\n                            data.append(\n                                {\n                                    \"Task\": task.name,\n                                    \"Algorithm\": algo.short_name,\n                                    \"X\": max(x, 0.5) if metric.log_x else x,\n                                    \"Y\": y\n                                    if metric.y_min is None\n                                    else max(y, metric.y_min),\n                                }\n                            )\n                            # !!!! Make multiple different ones if not sharing y-axis\n                            # Also, why are the x-values not quite right?\n                            y_max = max(y_max, y)\n            y_max = None if metric.y_min is None or not metric.y_axes_match else y_max\n            figs.append(\n                edge_patching_plot(\n                    data=data,\n                    task_measurements=task_measurements,\n                    ablation_type=ablation_type,\n                    metric_name=metric.name,\n                    log_x=metric.log_x,\n                    log_y=metric.log_y,\n                    y_axes_match=metric.y_axes_match,\n                    y_max=y_max,\n                    y_min=metric.y_min,\n                )\n            )\n            if auc_plots:\n                figs.append(\n                    average_auc_plot(\n                        task_measurements=task_measurements,\n                        log_x=metric.log_x,\n                        log_y=metric.log_y,\n                        y_min=metric.y_min,\n                        inverse=metric.lower_better,\n                    )\n                )\n    return tuple(figs)\n</code></pre>"},{"location":"reference/metrics/prune_metrics/prune_metrics/","title":"Prune metrics","text":""},{"location":"reference/metrics/prune_metrics/prune_metrics/#auto_circuit.metrics.prune_metrics.prune_metrics","title":"auto_circuit.metrics.prune_metrics.prune_metrics","text":""},{"location":"reference/metrics/prune_metrics/prune_metrics/#auto_circuit.metrics.prune_metrics.prune_metrics-attributes","title":"Attributes","text":""},{"location":"reference/metrics/prune_metrics/prune_metrics/#auto_circuit.metrics.prune_metrics.prune_metrics-classes","title":"Classes","text":""},{"location":"reference/metrics/prune_metrics/prune_metrics/#auto_circuit.metrics.prune_metrics.prune_metrics.PruneMetric","title":"PruneMetric  <code>dataclass</code>","text":"<pre><code>PruneMetric(key: PruneMetricKey, name: str, metric_func: Callable[[PatchableModel, PromptDataLoader, CircuitOutputs], Measurements], log_x: bool = False, log_y: bool = False, lower_better: bool = False, y_axes_match: bool = False, y_min: Optional[float] = None)\n</code></pre> <p>A metric of the output of a circuit on a task.</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>PruneMetricKey</code> <p>A unique identifier for the metric.</p> required <code>name</code> <code>str</code> <p>The name of the metric.</p> required <code>metric_func</code> <code>Callable[[PatchableModel, PromptDataLoader, CircuitOutputs], Measurements]</code> <p>A function that takes a model, a dataloader, and the outputs of the ablated model on the dataloader and returns a list of measurements.</p> required <code>log_x</code> <code>bool</code> <p>Whether to log the x-axis when plotting a graph of performance.</p> <code>False</code> <code>log_y</code> <code>bool</code> <p>Whether to log the y-axis when plotting a graph of performance.</p> <code>False</code> <code>lower_better</code> <code>bool</code> <p>Whether lower values are better on the y-axis.</p> <code>False</code> <code>y_axes_match</code> <code>bool</code> <p>Whether to use the same y-axis when plotting multiple tasks.</p> <code>False</code> <code>y_min</code> <code>Optional[float]</code> <p>The minimum value for the y-axis when plotting.</p> <code>None</code>"},{"location":"reference/metrics/prune_metrics/prune_metrics/#auto_circuit.metrics.prune_metrics.prune_metrics-functions","title":"Functions","text":""},{"location":"reference/metrics/prune_metrics/prune_metrics_plot/","title":"Prune metrics plot","text":""},{"location":"reference/metrics/prune_metrics/prune_metrics_plot/#auto_circuit.metrics.prune_metrics.prune_metrics_plot","title":"auto_circuit.metrics.prune_metrics.prune_metrics_plot","text":""},{"location":"reference/metrics/prune_metrics/prune_metrics_plot/#auto_circuit.metrics.prune_metrics.prune_metrics_plot-attributes","title":"Attributes","text":""},{"location":"reference/metrics/prune_metrics/prune_metrics_plot/#auto_circuit.metrics.prune_metrics.prune_metrics_plot-classes","title":"Classes","text":""},{"location":"reference/metrics/prune_metrics/prune_metrics_plot/#auto_circuit.metrics.prune_metrics.prune_metrics_plot-functions","title":"Functions","text":""},{"location":"reference/metrics/prune_metrics/prune_metrics_plot/#auto_circuit.metrics.prune_metrics.prune_metrics_plot.edge_patching_plot","title":"edge_patching_plot","text":"<pre><code>edge_patching_plot(data: List[Dict[str, Any]], task_measurements: TaskMeasurements, ablation_type: AblationType, metric_name: str, log_x: bool, log_y: bool, y_axes_match: bool, y_max: Optional[float], y_min: Optional[float]) -&gt; Figure\n</code></pre> <p>A figure showing the performance of the circuits produced by different <code>PruneAlgos</code> on different tasks. The x-axis is the number of edges in the circuit and the y-axis is the performance.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>List[Dict[str, Any]]</code> <p>A list of dictionaries in the following format:     <pre><code>{\n    \"Task\": str,\n    \"Algorithm\": str,\n    \"X\": Number,\n    \"Y\": Number,\n}</code></pre></p> required <code>task_measurements</code> <code>TaskMeasurements</code> <p>The measurements to plot (the same as <code>data</code> but in a different format).</p> required <code>ablation_type</code> <code>AblationType</code> <p>The type of ablation used to generate the data.</p> required <code>metric_name</code> <code>str</code> <p>The name of the metric which the data represents.</p> required <code>log_x</code> <code>bool</code> <p>Whether to log the x-axis.</p> required <code>log_y</code> <code>bool</code> <p>Whether to log the y-axis.</p> required <code>y_axes_match</code> <code>bool</code> <p>Whether to use the same y-axis for all tasks.</p> required <code>y_max</code> <code>Optional[float]</code> <p>The maximum value for the y-axis.</p> required <code>y_min</code> <code>Optional[float]</code> <p>The minimum value for the y-axis.</p> required <p>Returns:</p> Type Description <code>Figure</code> <p>A plotly figure.</p> Source code in <code>auto_circuit/metrics/prune_metrics/prune_metrics_plot.py</code> <pre><code>def edge_patching_plot(\n    data: List[Dict[str, Any]],\n    task_measurements: TaskMeasurements,\n    ablation_type: AblationType,\n    metric_name: str,\n    log_x: bool,\n    log_y: bool,\n    y_axes_match: bool,\n    y_max: Optional[float],\n    y_min: Optional[float],\n) -&gt; go.Figure:\n    \"\"\"\n    A figure showing the performance of the circuits produced by different\n    [`PruneAlgos`][auto_circuit.prune_algos.prune_algos.PruneAlgo] on different tasks.\n    The x-axis is the number of edges in the circuit and the y-axis is the performance.\n\n    Args:\n        data: A list of dictionaries in the following format:\n                &lt;pre&gt;&lt;code&gt;{\n                \"Task\": str,\n                \"Algorithm\": str,\n                \"X\": Number,\n                \"Y\": Number,\n            }&lt;/code&gt;&lt;/pre&gt;\n        task_measurements: The measurements to plot\n            (the same as `data` but in a different format).\n        ablation_type: The type of ablation used to generate the data.\n        metric_name: The name of the metric which the data represents.\n        log_x: Whether to log the x-axis.\n        log_y: Whether to log the y-axis.\n        y_axes_match: Whether to use the same y-axis for all tasks.\n        y_max: The maximum value for the y-axis.\n        y_min: The minimum value for the y-axis.\n\n    Returns:\n        A plotly figure.\n    \"\"\"\n    if len(data) &gt; 0:\n        data = sorted(data, key=lambda x: (x[\"Algorithm\"], x[\"Task\"]))\n        fig = px.line(\n            data,\n            x=\"X\",\n            y=\"Y\",\n            facet_col=\"Task\",\n            color=\"Algorithm\",\n            log_x=log_x,\n            log_y=log_y,\n            range_y=None if y_max is None else [y_min, y_max * 0.8],\n            # range_y=[-45, 120],\n            facet_col_spacing=0.03 if y_axes_match else 0.06,\n            markers=True,\n        )\n    else:\n        fig = subplots.make_subplots(rows=1, cols=len(task_measurements))\n\n    task_measurements = dict(sorted(task_measurements.items(), key=lambda x: x[0]))\n    for task_idx, algo_measurements in enumerate(task_measurements.values()):\n        for algo_key, measurements in algo_measurements.items():\n            algo = PRUNE_ALGO_DICT[algo_key]\n            pos = \"middle right\" if algo.short_name == \"GT\" else \"middle left\"\n            if len(measurements) == 1:\n                x, y = measurements[0]\n                fig.add_scattergl(\n                    row=1,\n                    col=task_idx + 1,\n                    x=[x],\n                    y=[y],\n                    mode=\"markers+text\",\n                    text=algo.short_name if algo.short_name else algo.name,\n                    textposition=pos,\n                    showlegend=task_idx == 0,\n                    marker=dict(color=\"black\", size=10, symbol=\"x-thin\"),\n                    marker_line_width=2,\n                    name=algo.short_name,\n                )\n\n    fig.update_layout(\n        # title=f\"{main_title}: {metric_name} vs. Patched Edges\",\n        yaxis_title=f\"{metric_name} ({ablation_type})\",\n        # yaxis_title=f\"{metric_name}\",\n        template=\"plotly\",\n        # width=335 * len(set([d[\"Task\"] for d in data])) + 280,\n        width=max(365 * len(set([d[\"Task\"] for d in data])) - 10, 500),\n        height=500,\n        legend=dict(\n            orientation=\"h\",\n            yanchor=\"bottom\",\n            y=-0.7,\n            xanchor=\"left\",\n            x=0.0,\n            entrywidthmode=\"fraction\",\n            entrywidth=0.25,\n        ),\n    )\n    fig.update_yaxes(matches=None, showticklabels=True) if not y_axes_match else None\n    fig.update_xaxes(matches=None, title=\"Circuit Edges\")\n    return fig\n</code></pre>"},{"location":"reference/model_utils/micro_model_utils/","title":"Micro model utils","text":""},{"location":"reference/model_utils/micro_model_utils/#auto_circuit.model_utils.micro_model_utils","title":"auto_circuit.model_utils.micro_model_utils","text":"<p>This module defines a tiny toy model used mostly for testing purposes.</p>"},{"location":"reference/model_utils/micro_model_utils/#auto_circuit.model_utils.micro_model_utils-classes","title":"Classes","text":""},{"location":"reference/model_utils/micro_model_utils/#auto_circuit.model_utils.micro_model_utils.Block","title":"Block","text":"<pre><code>Block()\n</code></pre> <p>             Bases: <code>Module</code></p> <p>Trivial linear layer with input and output of size 2.</p> Source code in <code>auto_circuit/model_utils/micro_model_utils.py</code> <pre><code>def __init__(self):\n    super().__init__()\n    self.head_inputs = t.nn.Identity()\n    self.head_outputs = t.nn.Identity()\n    self.weights = t.nn.Parameter(t.tensor([[1.0, 2.0], [3.0, 4.0]]))\n</code></pre>"},{"location":"reference/model_utils/micro_model_utils/#auto_circuit.model_utils.micro_model_utils.MicroModel","title":"MicroModel","text":"<pre><code>MicroModel(n_layers: int = 2)\n</code></pre> <p>             Bases: <code>Module</code></p> <p>A trivial model with two \"heads\" per layer that perform simple multiplication.</p> Source code in <code>auto_circuit/model_utils/micro_model_utils.py</code> <pre><code>def __init__(self, n_layers: int = 2):\n    super().__init__()\n    self.input = t.nn.Identity()\n    self.n_layers = n_layers\n    self.blocks, self.resids = t.nn.ModuleList(), t.nn.ModuleList()\n    for _ in range(n_layers):\n        self.blocks.append(Block())\n        self.resids.append(t.nn.Identity())\n    self.output = t.nn.Identity()\n</code></pre>"},{"location":"reference/model_utils/micro_model_utils/#auto_circuit.model_utils.micro_model_utils-functions","title":"Functions","text":""},{"location":"reference/model_utils/micro_model_utils/#auto_circuit.model_utils.micro_model_utils.factorized_dest_nodes","title":"factorized_dest_nodes","text":"<pre><code>factorized_dest_nodes(model: MicroModel) -&gt; Set[DestNode]\n</code></pre> <p>Get the destination part of each edge in the factorized graph, grouped by layer.</p> <p>Used by <code>graph_edges</code> in <code>patchable_model</code>.</p> Source code in <code>auto_circuit/model_utils/micro_model_utils.py</code> <pre><code>def factorized_dest_nodes(model: MicroModel) -&gt; Set[DestNode]:\n    \"\"\"\n    Get the destination part of each edge in the factorized graph, grouped by layer.\n\n    Used by [`graph_edges`][auto_circuit.utils.graph_utils.graph_edges] in\n    [`patchable_model`][auto_circuit.utils.graph_utils.patchable_model].\n    \"\"\"\n    nodes = set()\n    layers = count(1)\n    for layer_idx in range(model.n_layers):\n        layer = next(layers)\n        for elem in [0, 1]:\n            nodes.add(\n                DestNode(\n                    name=f\"B{layer_idx}.{elem}\",\n                    module_name=f\"blocks.{layer_idx}.head_inputs\",\n                    layer=layer,\n                    head_idx=elem,\n                    head_dim=2,\n                )\n            )\n    nodes.add(\n        DestNode(\n            name=\"Resid End\",\n            module_name=\"output\",\n            layer=next(layers),\n        )\n    )\n    return nodes\n</code></pre>"},{"location":"reference/model_utils/micro_model_utils/#auto_circuit.model_utils.micro_model_utils.factorized_src_nodes","title":"factorized_src_nodes","text":"<pre><code>factorized_src_nodes(model: MicroModel) -&gt; Set[SrcNode]\n</code></pre> <p>Get the source part of each edge in the factorized graph, grouped by layer.</p> <p>Used by <code>graph_edges</code> in <code>patchable_model</code>.</p> Source code in <code>auto_circuit/model_utils/micro_model_utils.py</code> <pre><code>def factorized_src_nodes(model: MicroModel) -&gt; Set[SrcNode]:\n    \"\"\"\n    Get the source part of each edge in the factorized graph, grouped by layer.\n\n    Used by [`graph_edges`][auto_circuit.utils.graph_utils.graph_edges] in\n    [`patchable_model`][auto_circuit.utils.graph_utils.patchable_model].\n    \"\"\"\n    nodes = set()\n    layers, idxs = count(), count()\n    nodes.add(\n        SrcNode(\n            name=\"Resid Start\",\n            module_name=\"input\",\n            layer=next(layers),\n            src_idx=next(idxs),\n        )\n    )\n    for layer_idx in range(model.n_layers):\n        layer = next(layers)\n        for elem in [0, 1]:\n            nodes.add(\n                SrcNode(\n                    name=f\"B{layer_idx}.{elem}\",\n                    module_name=f\"blocks.{layer_idx}.head_outputs\",\n                    layer=layer,\n                    src_idx=next(idxs),\n                    head_idx=elem,\n                    head_dim=2,\n                    weight=\"weights\",\n                    weight_head_dim=0,\n                )\n            )\n    return nodes\n</code></pre>"},{"location":"reference/model_utils/micro_model_utils/#auto_circuit.model_utils.micro_model_utils.simple_graph_nodes","title":"simple_graph_nodes","text":"<pre><code>simple_graph_nodes(model: MicroModel) -&gt; Tuple[Set[SrcNode], Set[DestNode]]\n</code></pre> <p>Get the nodes of the unfactorized graph.</p> <p><code>graph_edges</code> requires that all input <code>SrcNodes</code> are in the previous layer to the respective <code>DestNodes</code>.</p> Source code in <code>auto_circuit/model_utils/micro_model_utils.py</code> <pre><code>def simple_graph_nodes(model: MicroModel) -&gt; Tuple[Set[SrcNode], Set[DestNode]]:\n    \"\"\"\n    Get the nodes of the unfactorized graph.\n\n    [`graph_edges`][auto_circuit.utils.graph_utils.graph_edges] requires that all input\n    [`SrcNodes`][auto_circuit.types.SrcNode] are in the previous layer to the respective\n    [`DestNodes`][auto_circuit.types.DestNode].\n    \"\"\"\n    src_nodes, dest_nodes = set(), set()\n    layers, src_idxs = count(), count()\n    layer = next(layers)\n    for layer_idx in range(model.n_layers):\n        min_src_idx = next(src_idxs)\n        first_block = layer_idx == 0\n        src_nodes.add(\n            SrcNode(\n                name=\"Resid Start\" if first_block else f\"Resid Post {layer_idx -1}\",\n                module_name=\"input\" if first_block else f\"resids.{layer_idx - 1}\",\n                layer=layer,\n                src_idx=min_src_idx,\n            )\n        )\n        for elem in [0, 1]:\n            src_nodes.add(\n                SrcNode(\n                    name=f\"B{layer_idx}.{elem}\",\n                    module_name=f\"blocks.{layer_idx}.head_outputs\",\n                    layer=layer,\n                    src_idx=next(src_idxs),\n                    head_idx=elem,\n                    head_dim=2,\n                    weight=\"weights\",\n                    weight_head_dim=0,\n                )\n            )\n        last_block = layer_idx == model.n_layers - 1\n        layer = next(layers)\n        dest_nodes.add(\n            DestNode(\n                name=\"Resid End\" if last_block else f\"Resid Post {layer_idx}\",\n                module_name=\"output\" if last_block else f\"resids.{layer_idx}\",\n                layer=layer,\n                min_src_idx=min_src_idx,\n            )\n        )\n    return src_nodes, dest_nodes\n</code></pre>"},{"location":"reference/model_utils/tracr_model_utils/","title":"Tracr model utils","text":""},{"location":"reference/model_utils/tracr_model_utils/#auto_circuit.model_utils.tracr_model_utils","title":"auto_circuit.model_utils.tracr_model_utils","text":""},{"location":"reference/model_utils/tracr_model_utils/#auto_circuit.model_utils.tracr_model_utils-attributes","title":"Attributes","text":""},{"location":"reference/model_utils/tracr_model_utils/#auto_circuit.model_utils.tracr_model_utils.TRACR_TASK_KEY","title":"TRACR_TASK_KEY  <code>module-attribute</code>","text":"<pre><code>TRACR_TASK_KEY = Literal['reverse', 'xproportion']\n</code></pre> <p>Identifier of a Tracr model. Currently supported models:</p> <ul> <li><code>\"reverse\"</code></li> <li><code>\"xproportion\"</code></li> </ul>"},{"location":"reference/model_utils/tracr_model_utils/#auto_circuit.model_utils.tracr_model_utils-functions","title":"Functions","text":""},{"location":"reference/model_utils/tracr_model_utils/#auto_circuit.model_utils.tracr_model_utils.get_tracr_model","title":"get_tracr_model","text":"<pre><code>get_tracr_model(tracr_task_key: TRACR_TASK_KEY, device: str) -&gt; Tuple[HookedTransformer, AssembledTransformerModel]\n</code></pre> <p>Load the weights of a Tracr model and convert it to a HookedTransformer model.</p> <p>Adapted from Neel Nanda's TransformerLens port of tracr.</p> <p>Parameters:</p> Name Type Description Default <code>tracr_task_key</code> <code>TRACR_TASK_KEY</code> <p>Identifier of the Tracr model.</p> required <code>device</code> <code>str</code> <p>Device to load the model on.</p> required <p>Returns:</p> Type Description <code>Tuple[HookedTransformer, AssembledTransformerModel]</code> <p>A tuple of the HookedTransformer model and the original AssembledTransformerModel.</p> Source code in <code>auto_circuit/model_utils/tracr_model_utils.py</code> <pre><code>def get_tracr_model(\n    tracr_task_key: TRACR_TASK_KEY, device: str\n) -&gt; Tuple[HookedTransformer, AssembledTransformerModel]:\n    \"\"\"\n    Load the weights of a Tracr model and convert it to a HookedTransformer model.\n\n    Adapted from Neel Nanda's TransformerLens port of tracr.\n\n    Args:\n        tracr_task_key: Identifier of the Tracr model.\n        device: Device to load the model on.\n\n    Returns:\n        A tuple of the HookedTransformer model and the original\n            AssembledTransformerModel.\n    \"\"\"\n\n    def make_length():\n        all_true_selector = rasp.Select(rasp.tokens, rasp.tokens, rasp.Comparison.TRUE)\n        return rasp.SelectorWidth(all_true_selector)\n\n    if tracr_task_key == \"reverse\":\n        length = make_length()  # `length` is not a primitive in our implementation.\n        opp_index = length - rasp.indices - 1\n        flip = rasp.Select(rasp.indices, opp_index, rasp.Comparison.EQ)\n        reverse = rasp.Aggregate(flip, rasp.tokens)\n        model = compiling.compile_rasp_to_model(\n            reverse,\n            vocab=REVERSE_VOCAB,\n            max_seq_len=MAX_SEQ_LEN,\n            compiler_bos=BOS,\n        )\n    elif tracr_task_key == \"xproportion\":\n        model = compiling.compile_rasp_to_model(\n            make_frac_prevs(rasp.tokens == \"x\"),\n            vocab=XPROPORTION_VOCAB,\n            max_seq_len=MAX_SEQ_LEN,\n            compiler_bos=BOS,\n        )\n    else:\n        raise ValueError(f\"Unknown task {tracr_task_key}\")\n\n    # Extract the model config from the Tracr model, and create a blank\n    # HookedTransformer object\n\n    n_heads = model.model_config.num_heads\n    n_layers = model.model_config.num_layers\n    d_head = model.model_config.key_size\n    d_mlp = model.model_config.mlp_hidden_size\n    act_fn = \"relu\"\n    normalization_type = \"LN\" if model.model_config.layer_norm else None\n    attention_type = \"causal\" if model.model_config.causal else \"bidirectional\"\n\n    n_ctx = model.params[\"pos_embed\"][\"embeddings\"].shape[0]\n    # Equivalent to length of vocab, with BOS and PAD at the end\n    d_vocab = model.params[\"token_embed\"][\"embeddings\"].shape[0]\n    # Residual stream width, I don't know of an easy way to infer it from the above\n    # config.\n    d_model = model.params[\"token_embed\"][\"embeddings\"].shape[1]\n\n    if tracr_task_key == \"reverse\":\n        # Equivalent to length of vocab, WITHOUT BOS and PAD at the end because we never\n        # care about these outputs\n        d_vocab_out = model.params[\"token_embed\"][\"embeddings\"].shape[0] - 2\n    elif tracr_task_key == \"xproportion\":\n        # This task outputs a real number, so we only need the first residual dimension\n        d_vocab_out = 1\n\n    cfg = HookedTransformerConfig(\n        model_name=f\"tracr-{tracr_task_key}\",\n        n_layers=n_layers,\n        d_model=d_model,\n        d_head=d_head,\n        n_ctx=n_ctx,\n        d_vocab=d_vocab,\n        d_vocab_out=d_vocab_out,\n        d_mlp=d_mlp,\n        n_heads=n_heads,\n        act_fn=act_fn,\n        attention_dir=attention_type,\n        normalization_type=normalization_type,\n        use_attn_result=True,\n        use_split_qkv_input=True,\n        device=device,\n    )\n    tl_model = HookedTransformer(cfg)\n    if \"use_hook_mlp_in\" in tl_model.cfg.to_dict():\n        tl_model.set_use_hook_mlp_in(True)\n\n    # Extract the state dict, and do some reshaping so that everything has a n_heads\n    # dimension\n    sd = {}\n    sd[\"pos_embed.W_pos\"] = model.params[\"pos_embed\"][\"embeddings\"]\n    sd[\"embed.W_E\"] = model.params[\"token_embed\"][\"embeddings\"]\n    # Equivalent to max_seq_len plus one, for the BOS\n\n    # The unembed is just a projection onto the first few elements of the residual\n    # stream, these store output tokens\n    # This is a NumPy array, the rest are Jax Arrays, but w/e it's fine.\n    sd[\"unembed.W_U\"] = np.eye(d_model, d_vocab_out)\n\n    for lyr in range(n_layers):\n        sd[f\"blocks.{lyr}.attn.W_K\"] = einops.rearrange(\n            model.params[f\"transformer/layer_{lyr}/attn/key\"][\"w\"],\n            \"d_model (n_heads d_head) -&gt; n_heads d_model d_head\",\n            d_head=d_head,\n            n_heads=n_heads,\n        )\n        sd[f\"blocks.{lyr}.attn.b_K\"] = einops.rearrange(\n            model.params[f\"transformer/layer_{lyr}/attn/key\"][\"b\"],\n            \"(n_heads d_head) -&gt; n_heads d_head\",\n            d_head=d_head,\n            n_heads=n_heads,\n        )\n        sd[f\"blocks.{lyr}.attn.W_Q\"] = einops.rearrange(\n            model.params[f\"transformer/layer_{lyr}/attn/query\"][\"w\"],\n            \"d_model (n_heads d_head) -&gt; n_heads d_model d_head\",\n            d_head=d_head,\n            n_heads=n_heads,\n        )\n        sd[f\"blocks.{lyr}.attn.b_Q\"] = einops.rearrange(\n            model.params[f\"transformer/layer_{lyr}/attn/query\"][\"b\"],\n            \"(n_heads d_head) -&gt; n_heads d_head\",\n            d_head=d_head,\n            n_heads=n_heads,\n        )\n        sd[f\"blocks.{lyr}.attn.W_V\"] = einops.rearrange(\n            model.params[f\"transformer/layer_{lyr}/attn/value\"][\"w\"],\n            \"d_model (n_heads d_head) -&gt; n_heads d_model d_head\",\n            d_head=d_head,\n            n_heads=n_heads,\n        )\n        sd[f\"blocks.{lyr}.attn.b_V\"] = einops.rearrange(\n            model.params[f\"transformer/layer_{lyr}/attn/value\"][\"b\"],\n            \"(n_heads d_head) -&gt; n_heads d_head\",\n            d_head=d_head,\n            n_heads=n_heads,\n        )\n        sd[f\"blocks.{lyr}.attn.W_O\"] = einops.rearrange(\n            model.params[f\"transformer/layer_{lyr}/attn/linear\"][\"w\"],\n            \"(n_heads d_head) d_model -&gt; n_heads d_head d_model\",\n            d_head=d_head,\n            n_heads=n_heads,\n        )\n        sd[f\"blocks.{lyr}.attn.b_O\"] = model.params[\n            f\"transformer/layer_{lyr}/attn/linear\"\n        ][\"b\"]\n\n        sd[f\"blocks.{lyr}.mlp.W_in\"] = model.params[\n            f\"transformer/layer_{lyr}/mlp/linear_1\"\n        ][\"w\"]\n        sd[f\"blocks.{lyr}.mlp.b_in\"] = model.params[\n            f\"transformer/layer_{lyr}/mlp/linear_1\"\n        ][\"b\"]\n        sd[f\"blocks.{lyr}.mlp.W_out\"] = model.params[\n            f\"transformer/layer_{lyr}/mlp/linear_2\"\n        ][\"w\"]\n        sd[f\"blocks.{lyr}.mlp.b_out\"] = model.params[\n            f\"transformer/layer_{lyr}/mlp/linear_2\"\n        ][\"b\"]\n\n    # Convert weights to tensors and load into the tl_model\n\n    for k, v in sd.items():\n        # I cannot figure out a neater way to go from a Jax array to a numpy array lol\n        sd[k] = torch.tensor(np.array(v))\n\n    tl_model.load_state_dict(sd, strict=False)\n    return tl_model, model\n</code></pre>"},{"location":"reference/model_utils/transformer_lens_utils/","title":"Transformer lens utils","text":""},{"location":"reference/model_utils/transformer_lens_utils/#auto_circuit.model_utils.transformer_lens_utils","title":"auto_circuit.model_utils.transformer_lens_utils","text":""},{"location":"reference/model_utils/transformer_lens_utils/#auto_circuit.model_utils.transformer_lens_utils-classes","title":"Classes","text":""},{"location":"reference/model_utils/transformer_lens_utils/#auto_circuit.model_utils.transformer_lens_utils-functions","title":"Functions","text":""},{"location":"reference/model_utils/transformer_lens_utils/#auto_circuit.model_utils.transformer_lens_utils.factorized_dest_nodes","title":"factorized_dest_nodes","text":"<pre><code>factorized_dest_nodes(model: HookedTransformer, separate_qkv: bool) -&gt; Set[DestNode]\n</code></pre> <p>Get the destination part of each edge in the factorized graph, grouped by layer.</p> <p>Factorization introduced by Elhage et al. (2021). See Molina (2023) for a good explanation.</p> <p></p> Source code in <code>auto_circuit/model_utils/transformer_lens_utils.py</code> <pre><code>def factorized_dest_nodes(\n    model: tl.HookedTransformer, separate_qkv: bool\n) -&gt; Set[DestNode]:\n    \"\"\"\n    Get the destination part of each edge in the factorized graph, grouped by layer.\n\n    Factorization introduced by\n    [Elhage et al. (2021)](https://transformer-circuits.pub/2021/framework/index.html).\n    See [Molina (2023)](https://arxiv.org/pdf/2309.07315.pdf) for a good explanation.\n\n    ![](../../assets/Factorized_Transformer.png)\n    \"\"\"\n    if separate_qkv:\n        assert model.cfg.use_split_qkv_input  # Separate Q, K, V input for each head\n    else:\n        assert model.cfg.use_attn_in\n    if not model.cfg.attn_only:\n        assert model.cfg.use_hook_mlp_in  # Get MLP input BEFORE layernorm\n    layers = count(1)\n    nodes = set()\n    for block_idx in range(model.cfg.n_layers):\n        layer = next(layers)\n        for head_idx in range(model.cfg.n_heads):\n            if separate_qkv:\n                for letter in [\"Q\", \"K\", \"V\"]:\n                    nodes.add(\n                        DestNode(\n                            name=f\"A{block_idx}.{head_idx}.{letter}\",\n                            module_name=f\"blocks.{block_idx}.hook_{letter.lower()}_input\",\n                            layer=layer,\n                            head_dim=2,\n                            head_idx=head_idx,\n                            weight=f\"blocks.{block_idx}.attn.W_{letter}\",\n                            weight_head_dim=0,\n                        )\n                    )\n            else:\n                nodes.add(\n                    DestNode(\n                        name=f\"A{block_idx}.{head_idx}\",\n                        module_name=f\"blocks.{block_idx}.hook_attn_in\",\n                        layer=layer,\n                        head_dim=2,\n                        head_idx=head_idx,\n                        weight=f\"blocks.{block_idx}.attn.W_QKV\",\n                        weight_head_dim=0,\n                    )\n                )\n        if not model.cfg.attn_only:\n            nodes.add(\n                DestNode(\n                    name=f\"MLP {block_idx}\",\n                    module_name=f\"blocks.{block_idx}.hook_mlp_in\",\n                    layer=layer if model.cfg.parallel_attn_mlp else next(layers),\n                    weight=f\"blocks.{block_idx}.mlp.W_in\",\n                )\n            )\n    nodes.add(\n        DestNode(\n            name=\"Resid End\",\n            module_name=f\"blocks.{model.cfg.n_layers - 1}.hook_resid_post\",\n            layer=next(layers),\n            weight=\"unembed.W_U\",\n        )\n    )\n    return nodes\n</code></pre>"},{"location":"reference/model_utils/transformer_lens_utils/#auto_circuit.model_utils.transformer_lens_utils.factorized_src_nodes","title":"factorized_src_nodes","text":"<pre><code>factorized_src_nodes(model: HookedTransformer) -&gt; Set[SrcNode]\n</code></pre> <p>Get the source part of each edge in the factorized graph, grouped by layer.</p> <p>Factorization introduced by Elhage et al. (2021). See also Molina (2023) for a good explanation.</p> <p></p> Source code in <code>auto_circuit/model_utils/transformer_lens_utils.py</code> <pre><code>def factorized_src_nodes(model: tl.HookedTransformer) -&gt; Set[SrcNode]:\n    \"\"\"\n    Get the source part of each edge in the factorized graph, grouped by layer.\n\n    Factorization introduced by\n    [Elhage et al. (2021)](https://transformer-circuits.pub/2021/framework/index.html).\n    See also [Molina (2023)](https://arxiv.org/pdf/2309.07315.pdf) for a good\n    explanation.\n\n    ![](../../assets/Factorized_Transformer.png)\n    \"\"\"\n    assert model.cfg.use_attn_result  # Get attention head outputs separately\n    if not model.cfg.attn_only:\n        assert model.cfg.use_hook_mlp_in  # Get MLP input BEFORE layernorm\n    layers, idxs = count(), count()\n    nodes = set()\n    nodes.add(\n        SrcNode(\n            name=\"Resid Start\",\n            module_name=\"blocks.0.hook_resid_pre\",\n            layer=next(layers),\n            src_idx=next(idxs),\n            weight=\"embed.W_E\",\n        )\n    )\n\n    for block_idx in range(model.cfg.n_layers):\n        layer = next(layers)\n        for head_idx in range(model.cfg.n_heads):\n            nodes.add(\n                SrcNode(\n                    name=f\"A{block_idx}.{head_idx}\",\n                    module_name=f\"blocks.{block_idx}.attn.hook_result\",\n                    layer=layer,\n                    src_idx=next(idxs),\n                    head_dim=2,\n                    head_idx=head_idx,\n                    weight=f\"blocks.{block_idx}.attn.W_O\",\n                    weight_head_dim=0,\n                )\n            )\n        if not model.cfg.attn_only:\n            nodes.add(\n                SrcNode(\n                    name=f\"MLP {block_idx}\",\n                    module_name=f\"blocks.{block_idx}.hook_mlp_out\",\n                    layer=layer if model.cfg.parallel_attn_mlp else next(layers),\n                    src_idx=next(idxs),\n                    weight=f\"blocks.{block_idx}.mlp.W_out\",\n                )\n            )\n    return nodes\n</code></pre>"},{"location":"reference/model_utils/transformer_lens_utils/#auto_circuit.model_utils.transformer_lens_utils.simple_graph_nodes","title":"simple_graph_nodes","text":"<pre><code>simple_graph_nodes(model: HookedTransformer) -&gt; Tuple[Set[SrcNode], Set[DestNode]]\n</code></pre> <p>Get the nodes of the unfactorized graph.</p> <p><code>graph_edges</code> requires that all input <code>SrcNodes</code> are in the previous layer to the respective <code>DestNodes</code>.</p> <p></p> Source code in <code>auto_circuit/model_utils/transformer_lens_utils.py</code> <pre><code>def simple_graph_nodes(\n    model: tl.HookedTransformer,\n) -&gt; Tuple[Set[SrcNode], Set[DestNode]]:\n    \"\"\"\n    Get the nodes of the unfactorized graph.\n\n    [`graph_edges`][auto_circuit.utils.graph_utils.graph_edges] requires that all input\n    [`SrcNodes`][auto_circuit.types.SrcNode] are in the previous layer to the respective\n    [`DestNodes`][auto_circuit.types.DestNode].\n\n    ![](../../assets/Residual_Transformer.png)\n    \"\"\"\n    assert not model.cfg.parallel_attn_mlp\n    layers, src_idxs = count(), count()\n    src_nodes, dest_nodes = set(), set()\n    layer, min_src_idx = next(layers), next(src_idxs)\n    for block_idx in range(model.cfg.n_layers):\n        first_block = block_idx == 0\n        src_nodes.add(\n            SrcNode(\n                name=\"Resid Start\" if first_block else f\"Resid Post {block_idx -1}\",\n                module_name=\"blocks.0.hook_resid_pre\"\n                if first_block\n                else f\"blocks.{block_idx - 1}.hook_resid_post\",\n                layer=layer,\n                src_idx=min_src_idx,\n            )\n        )\n        for head_idx in range(model.cfg.n_heads):\n            src_nodes.add(\n                SrcNode(\n                    name=f\"A{block_idx}.{head_idx}\",\n                    module_name=f\"blocks.{block_idx}.attn.hook_result\",\n                    layer=layer,\n                    src_idx=next(src_idxs),\n                    head_idx=head_idx,\n                    head_dim=2,\n                    weight=f\"blocks.{block_idx}.attn.W_O\",\n                    weight_head_dim=0,\n                )\n            )\n        if not (model.cfg.attn_only or model.cfg.parallel_attn_mlp):\n            layer = next(layers)\n            dest_nodes.add(\n                DestNode(\n                    name=f\"Resid Mid {block_idx}\",\n                    module_name=f\"blocks.{block_idx}.hook_resid_mid\",\n                    layer=layer,\n                    min_src_idx=min_src_idx,\n                )\n            )\n            min_src_idx = next(src_idxs)\n            src_nodes.add(\n                SrcNode(\n                    name=f\"Resid Mid {block_idx}\",\n                    module_name=f\"blocks.{block_idx}.hook_resid_mid\",\n                    layer=layer,\n                    src_idx=min_src_idx,\n                )\n            )\n        if not model.cfg.attn_only:\n            src_nodes.add(\n                SrcNode(\n                    name=f\"MLP {block_idx}\",\n                    module_name=f\"blocks.{block_idx}.hook_mlp_out\",\n                    layer=layer,\n                    src_idx=next(src_idxs),\n                    weight=f\"blocks.{block_idx}.mlp.W_out\",\n                )\n            )\n        last_block = block_idx + 1 == model.cfg.n_layers\n        layer = next(layers)\n        dest_nodes.add(\n            DestNode(\n                name=\"Resid End\" if last_block else f\"Resid Post {block_idx}\",\n                module_name=f\"blocks.{block_idx}.hook_resid_post\",\n                layer=layer,\n                min_src_idx=min_src_idx,\n            )\n        )\n        min_src_idx = next(src_idxs)\n    return src_nodes, dest_nodes\n</code></pre>"},{"location":"reference/model_utils/sparse_autoencoders/autoencoder_training/","title":"Autoencoder training","text":""},{"location":"reference/model_utils/sparse_autoencoders/autoencoder_training/#auto_circuit.model_utils.sparse_autoencoders.autoencoder_training","title":"auto_circuit.model_utils.sparse_autoencoders.autoencoder_training","text":""},{"location":"reference/model_utils/sparse_autoencoders/autoencoder_training/#auto_circuit.model_utils.sparse_autoencoders.autoencoder_training-attributes","title":"Attributes","text":""},{"location":"reference/model_utils/sparse_autoencoders/autoencoder_training/#auto_circuit.model_utils.sparse_autoencoders.autoencoder_training-classes","title":"Classes","text":""},{"location":"reference/model_utils/sparse_autoencoders/autoencoder_training/#auto_circuit.model_utils.sparse_autoencoders.autoencoder_training-functions","title":"Functions","text":""},{"location":"reference/model_utils/sparse_autoencoders/autoencoder_transformer/","title":"Autoencoder transformer","text":""},{"location":"reference/model_utils/sparse_autoencoders/autoencoder_transformer/#auto_circuit.model_utils.sparse_autoencoders.autoencoder_transformer","title":"auto_circuit.model_utils.sparse_autoencoders.autoencoder_transformer","text":"<p>A transformer model that patches in sparse autoencoder reconstructions at each layer. Work in progress. Error nodes not implemented.</p>"},{"location":"reference/model_utils/sparse_autoencoders/autoencoder_transformer/#auto_circuit.model_utils.sparse_autoencoders.autoencoder_transformer-attributes","title":"Attributes","text":""},{"location":"reference/model_utils/sparse_autoencoders/autoencoder_transformer/#auto_circuit.model_utils.sparse_autoencoders.autoencoder_transformer-classes","title":"Classes","text":""},{"location":"reference/model_utils/sparse_autoencoders/autoencoder_transformer/#auto_circuit.model_utils.sparse_autoencoders.autoencoder_transformer.AutoencoderTransformer","title":"AutoencoderTransformer","text":"<pre><code>AutoencoderTransformer(wrapped_model: Module, saes: List[SparseAutoencoder])\n</code></pre> <p>             Bases: <code>Module</code></p> Source code in <code>auto_circuit/model_utils/sparse_autoencoders/autoencoder_transformer.py</code> <pre><code>def __init__(self, wrapped_model: t.nn.Module, saes: List[SparseAutoencoder]):\n    super().__init__()\n    self.sparse_autoencoders = saes\n\n    if isinstance(wrapped_model, PatchableModel):\n        self.wrapped_model = wrapped_model.wrapped_model\n    else:\n        self.wrapped_model = wrapped_model\n</code></pre>"},{"location":"reference/model_utils/sparse_autoencoders/autoencoder_transformer/#auto_circuit.model_utils.sparse_autoencoders.autoencoder_transformer-functions","title":"Functions","text":""},{"location":"reference/model_utils/sparse_autoencoders/autoencoder_transformer/#auto_circuit.model_utils.sparse_autoencoders.autoencoder_transformer.factorized_dest_nodes","title":"factorized_dest_nodes","text":"<pre><code>factorized_dest_nodes(model: AutoencoderTransformer, separate_qkv: bool) -&gt; Set[DestNode]\n</code></pre> <p>Get the destination part of each edge in the factorized graph, grouped by layer. Graph is factorized following the Mathematical Framework paper.</p> Source code in <code>auto_circuit/model_utils/sparse_autoencoders/autoencoder_transformer.py</code> <pre><code>def factorized_dest_nodes(\n    model: AutoencoderTransformer, separate_qkv: bool\n) -&gt; Set[DestNode]:\n    \"\"\"Get the destination part of each edge in the factorized graph, grouped by layer.\n    Graph is factorized following the Mathematical Framework paper.\"\"\"\n    if separate_qkv:\n        assert model.cfg.use_split_qkv_input  # Separate Q, K, V input for each head\n    else:\n        assert model.cfg.use_attn_in\n    if not model.cfg.attn_only:\n        assert model.cfg.use_hook_mlp_in  # Get MLP input BEFORE layernorm\n    layers = count(1)\n    nodes = set()\n    for block_idx in range(model.cfg.n_layers):\n        layer = next(layers)\n        for head_idx in range(model.cfg.n_heads):\n            if separate_qkv:\n                for letter in [\"Q\", \"K\", \"V\"]:\n                    nodes.add(\n                        DestNode(\n                            name=f\"A{block_idx}.{head_idx}.{letter}\",\n                            module_name=f\"blocks.{block_idx}.hook_{letter.lower()}_input\",\n                            layer=layer,\n                            head_dim=2,\n                            head_idx=head_idx,\n                            weight=f\"blocks.{block_idx}.attn.W_{letter}\",\n                            weight_head_dim=0,\n                        )\n                    )\n            else:\n                nodes.add(\n                    DestNode(\n                        name=f\"A{block_idx}.{head_idx}\",\n                        module_name=f\"blocks.{block_idx}.hook_attn_in\",\n                        layer=layer,\n                        head_dim=2,\n                        head_idx=head_idx,\n                        weight=f\"blocks.{block_idx}.attn.W_QKV\",\n                        weight_head_dim=0,\n                    )\n                )\n        nodes.add(\n            DestNode(\n                name=f\"MLP {block_idx}\",\n                module_name=f\"blocks.{block_idx}.hook_mlp_in\",\n                layer=layer if model.cfg.parallel_attn_mlp else next(layers),\n                weight=f\"blocks.{block_idx}.mlp.W_in\",\n            )\n        )\n    nodes.add(\n        DestNode(\n            name=\"Resid End\",\n            module_name=f\"blocks.{model.cfg.n_layers - 1}.hook_resid_post\",\n            layer=next(layers),\n            weight=\"unembed.W_U\",\n        )\n    )\n    return nodes\n</code></pre>"},{"location":"reference/model_utils/sparse_autoencoders/autoencoder_transformer/#auto_circuit.model_utils.sparse_autoencoders.autoencoder_transformer.factorized_src_nodes","title":"factorized_src_nodes","text":"<pre><code>factorized_src_nodes(model: AutoencoderTransformer) -&gt; Set[SrcNode]\n</code></pre> <p>Get the source part of each edge in the factorized graph, grouped by layer. Graph is factorized following the Mathematical Framework paper.</p> Source code in <code>auto_circuit/model_utils/sparse_autoencoders/autoencoder_transformer.py</code> <pre><code>def factorized_src_nodes(model: AutoencoderTransformer) -&gt; Set[SrcNode]:\n    \"\"\"Get the source part of each edge in the factorized graph, grouped by layer.\n    Graph is factorized following the Mathematical Framework paper.\"\"\"\n    assert model.cfg.use_attn_result  # Get attention head outputs separately\n    assert model.cfg.use_attn_in  # Get attention head inputs separately\n    assert model.cfg.use_split_qkv_input  # Separate Q, K, V input for each head\n    if not model.cfg.attn_only:\n        assert model.cfg.use_hook_mlp_in  # Get MLP input BEFORE layernorm\n    assert not model.cfg.attn_only\n\n    layers, idxs = count(), count()\n    nodes = set()\n    nodes.add(\n        SrcNode(\n            name=\"Resid Start\",\n            module_name=\"blocks.0.hook_resid_pre\",\n            layer=next(layers),\n            src_idx=next(idxs),\n            weight=\"embed.W_E\",\n        )\n    )\n\n    for block_idx in range(model.cfg.n_layers):\n        layer = next(layers)\n        for head_idx in range(model.cfg.n_heads):\n            nodes.add(\n                SrcNode(\n                    name=f\"A{block_idx}.{head_idx}\",\n                    module_name=f\"blocks.{block_idx}.attn.hook_result\",\n                    layer=layer,\n                    src_idx=next(idxs),\n                    head_dim=2,\n                    head_idx=head_idx,\n                    weight=f\"blocks.{block_idx}.attn.W_O\",\n                    weight_head_dim=0,\n                )\n            )\n        layer = layer if model.cfg.parallel_attn_mlp else next(layers)\n        for latent_idx in range(model.blocks[block_idx].hook_mlp_out.n_latents):\n            nodes.add(\n                SrcNode(\n                    name=f\"MLP {block_idx} Latent {latent_idx}\",\n                    module_name=f\"blocks.{block_idx}.hook_mlp_out.latent_outs\",\n                    layer=layer,\n                    src_idx=next(idxs),\n                    head_dim=2,\n                    head_idx=latent_idx,\n                    weight=f\"blocks.{block_idx}.hook_mlp_out.decoder.weight\",\n                    weight_head_dim=0,\n                )\n            )\n    return nodes\n</code></pre>"},{"location":"reference/model_utils/sparse_autoencoders/autoencoder_transformer/#auto_circuit.model_utils.sparse_autoencoders.autoencoder_transformer.sae_model","title":"sae_model","text":"<pre><code>sae_model(model: HookedTransformer, sae_input: AutoencoderInput, load_pretrained: bool, n_latents: Optional[int] = None, pythia_size: Optional[str] = None, new_instance: bool = True) -&gt; AutoencoderTransformer\n</code></pre> <p>Inject <code>SparseAutoencoder</code> wrappers into a transformer model.</p> Source code in <code>auto_circuit/model_utils/sparse_autoencoders/autoencoder_transformer.py</code> <pre><code>def sae_model(\n    model: HookedTransformer,\n    sae_input: AutoencoderInput,\n    load_pretrained: bool,\n    n_latents: Optional[int] = None,\n    pythia_size: Optional[str] = None,\n    new_instance: bool = True,\n) -&gt; AutoencoderTransformer:\n    \"\"\"\n    Inject\n    [`SparseAutoencoder`][auto_circuit.model_utils.sparse_autoencoders.sparse_autoencoder.SparseAutoencoder]\n    wrappers into a transformer model.\n    \"\"\"\n    if new_instance:\n        model = deepcopy(model)\n    sparse_autoencoders: List[SparseAutoencoder] = []\n    for layer_idx in range(model.cfg.n_layers):\n        if sae_input == \"mlp_post_act\":\n            hook_point = model.blocks[layer_idx].mlp.hook_post\n            hook_module = model.blocks[layer_idx].mlp\n            hook_name = \"hook_post\"\n        else:\n            assert sae_input == \"resid_delta_mlp\"\n            hook_point = model.blocks[layer_idx].hook_mlp_out\n            hook_module = model.blocks[layer_idx]\n            hook_name = \"hook_mlp_out\"\n        if load_pretrained:\n            assert pythia_size is not None\n            sae = load_autoencoder(hook_point, model, layer_idx, sae_input, pythia_size)\n        else:\n            assert n_latents is not None\n            sae = SparseAutoencoder(hook_point, n_latents, model.cfg.d_model)\n        sae.to(model.cfg.device)\n        setattr(hook_module, hook_name, sae)\n        sparse_autoencoders.append(sae)\n    return AutoencoderTransformer(model, sparse_autoencoders)\n</code></pre>"},{"location":"reference/model_utils/sparse_autoencoders/sparse_autoencoder/","title":"Sparse autoencoder","text":""},{"location":"reference/model_utils/sparse_autoencoders/sparse_autoencoder/#auto_circuit.model_utils.sparse_autoencoders.sparse_autoencoder","title":"auto_circuit.model_utils.sparse_autoencoders.sparse_autoencoder","text":""},{"location":"reference/model_utils/sparse_autoencoders/sparse_autoencoder/#auto_circuit.model_utils.sparse_autoencoders.sparse_autoencoder-attributes","title":"Attributes","text":""},{"location":"reference/model_utils/sparse_autoencoders/sparse_autoencoder/#auto_circuit.model_utils.sparse_autoencoders.sparse_autoencoder-classes","title":"Classes","text":""},{"location":"reference/model_utils/sparse_autoencoders/sparse_autoencoder/#auto_circuit.model_utils.sparse_autoencoders.sparse_autoencoder.SparseAutoencoder","title":"SparseAutoencoder","text":"<pre><code>SparseAutoencoder(wrapped_hook: HookPoint, n_latents: int, n_inputs: int)\n</code></pre> <p>             Bases: <code>Module</code></p> <p>A Sparse Autoencoder wrapper module.</p> <p>Takes some input, passes it through the autoencoder and passes the reconstructed input to the wrapped hook.</p> Implements <p>latents = ReLU(encoder(x - bias) + latent_bias) recons = decoder(latents) + bias</p> <p>:param wrapped_hook: the wrapped transformer_lens hook that caches the SAE input :param n_latents: dimension of the autoencoder latent :param n_inputs: dimensionality of the input (e.g residual stream, MLP neurons)</p> Source code in <code>auto_circuit/model_utils/sparse_autoencoders/sparse_autoencoder.py</code> <pre><code>def __init__(self, wrapped_hook: HookPoint, n_latents: int, n_inputs: int) -&gt; None:\n    \"\"\"\n    :param wrapped_hook: the wrapped transformer_lens hook that caches the SAE input\n    :param n_latents: dimension of the autoencoder latent\n    :param n_inputs: dimensionality of the input (e.g residual stream, MLP neurons)\n    \"\"\"\n    super().__init__()\n    self.wrapped_hook: HookPoint = wrapped_hook\n    self.latent_outs: HookPoint = HookPoint()\n    # Weights start the same at each position. They're only different after pruning.\n    self.init_params(n_latents, n_inputs)\n    self.reset_activated_latents()\n</code></pre>"},{"location":"reference/model_utils/sparse_autoencoders/sparse_autoencoder/#auto_circuit.model_utils.sparse_autoencoders.sparse_autoencoder.SparseAutoencoder-functions","title":"Functions","text":""},{"location":"reference/model_utils/sparse_autoencoders/sparse_autoencoder/#auto_circuit.model_utils.sparse_autoencoders.sparse_autoencoder.SparseAutoencoder.decode","title":"decode","text":"<pre><code>decode(x: Tensor) -&gt; Tensor\n</code></pre> <p>:param x: autoencoder x (shape: [..., [seq], n_latents]) :return: reconstructed data (shape: [..., [seq], n_inputs])</p> Source code in <code>auto_circuit/model_utils/sparse_autoencoders/sparse_autoencoder.py</code> <pre><code>def decode(self, x: t.Tensor) -&gt; t.Tensor:\n    \"\"\"\n    :param x: autoencoder x (shape: [..., [seq], n_latents])\n    :return: reconstructed data (shape: [..., [seq], n_inputs])\n    \"\"\"\n    ein_str = \"... l, ... d l -&gt; ... l d\"\n    latent_outs = self.latent_outs(einsum(x, self.decode_weight, ein_str))\n    return latent_outs.sum(dim=-2) + self.bias\n</code></pre>"},{"location":"reference/model_utils/sparse_autoencoders/sparse_autoencoder/#auto_circuit.model_utils.sparse_autoencoders.sparse_autoencoder.SparseAutoencoder.encode","title":"encode","text":"<pre><code>encode(x: Tensor) -&gt; Tensor\n</code></pre> <p>:param x: input data (shape: [..., [seq], n_inputs]) :return: autoencoder latents (shape: [..., [seq], n_latents])</p> Source code in <code>auto_circuit/model_utils/sparse_autoencoders/sparse_autoencoder.py</code> <pre><code>def encode(self, x: t.Tensor) -&gt; t.Tensor:\n    \"\"\"\n    :param x: input data (shape: [..., [seq], n_inputs])\n    :return: autoencoder latents (shape: [..., [seq], n_latents])\n    \"\"\"\n    encoded = einsum(x - self.bias, self.encode_weight, \"... d, ... l d -&gt; ... l\")\n    latents_pre_act = encoded + self.latent_bias\n    return t.nn.functional.relu(latents_pre_act)\n</code></pre>"},{"location":"reference/model_utils/sparse_autoencoders/sparse_autoencoder/#auto_circuit.model_utils.sparse_autoencoders.sparse_autoencoder.SparseAutoencoder.forward","title":"forward","text":"<pre><code>forward(x: Tensor) -&gt; Tensor\n</code></pre> <p>:param x: input data (shape: [..., n_inputs]) :return:  reconstructed data (shape: [..., n_inputs])</p> Source code in <code>auto_circuit/model_utils/sparse_autoencoders/sparse_autoencoder.py</code> <pre><code>def forward(self, x: t.Tensor) -&gt; t.Tensor:\n    \"\"\"\n    :param x: input data (shape: [..., n_inputs])\n    :return:  reconstructed data (shape: [..., n_inputs])\n    \"\"\"\n    x = self.wrapped_hook(x)\n    latents = self.encode(x)\n    self.latent_total_act += latents.sum_to_size(self.latent_total_act.shape)\n    recons = self.decode(latents)\n    return recons\n</code></pre>"},{"location":"reference/model_utils/sparse_autoencoders/sparse_autoencoder/#auto_circuit.model_utils.sparse_autoencoders.sparse_autoencoder-functions","title":"Functions","text":""},{"location":"reference/model_utils/task_projectors/projector_transformer/","title":"Projector transformer","text":""},{"location":"reference/model_utils/task_projectors/projector_transformer/#auto_circuit.model_utils.task_projectors.projector_transformer","title":"auto_circuit.model_utils.task_projectors.projector_transformer","text":""},{"location":"reference/model_utils/task_projectors/projector_transformer/#auto_circuit.model_utils.task_projectors.projector_transformer-attributes","title":"Attributes","text":""},{"location":"reference/model_utils/task_projectors/projector_transformer/#auto_circuit.model_utils.task_projectors.projector_transformer-classes","title":"Classes","text":""},{"location":"reference/model_utils/task_projectors/projector_transformer/#auto_circuit.model_utils.task_projectors.projector_transformer-functions","title":"Functions","text":""},{"location":"reference/model_utils/task_projectors/task_projector/","title":"Task projector","text":""},{"location":"reference/model_utils/task_projectors/task_projector/#auto_circuit.model_utils.task_projectors.task_projector","title":"auto_circuit.model_utils.task_projectors.task_projector","text":"<p>This was a weird idea I has where you learn a projection at each layer of a transformer that tries to remove as many directions as possible.</p>"},{"location":"reference/model_utils/task_projectors/task_projector/#auto_circuit.model_utils.task_projectors.task_projector-attributes","title":"Attributes","text":""},{"location":"reference/model_utils/task_projectors/task_projector/#auto_circuit.model_utils.task_projectors.task_projector-classes","title":"Classes","text":""},{"location":"reference/model_utils/task_projectors/task_projector/#auto_circuit.model_utils.task_projectors.task_projector.TaskProjector","title":"TaskProjector","text":"<pre><code>TaskProjector(wrapped_hook: HookPoint, n_inputs: int, seq_idxs: Optional[List[int]] = None, mask_fn: MaskFn = None, layernorm: bool = False)\n</code></pre> <p>             Bases: <code>Module</code></p> <p>Task Projector</p> Implements <p>latents = ReLU(encoder(x - bias) + latent_bias) recons = decoder(latents) + bias</p> <p>:param wrapped_hook: the wrapped transformer_lens hook that caches the SAE input :param n_inputs: dimensionality of the input (e.g residual stream, MLP neurons)</p> Source code in <code>auto_circuit/model_utils/task_projectors/task_projector.py</code> <pre><code>def __init__(\n    self,\n    wrapped_hook: HookPoint,\n    n_inputs: int,\n    seq_idxs: Optional[List[int]] = None,\n    mask_fn: MaskFn = None,\n    layernorm: bool = False,\n) -&gt; None:\n    \"\"\"\n    :param wrapped_hook: the wrapped transformer_lens hook that caches the SAE input\n    :param n_inputs: dimensionality of the input (e.g residual stream, MLP neurons)\n    \"\"\"\n    super().__init__()\n    self.wrapped_hook: HookPoint = wrapped_hook\n    self.init_params(n_inputs, seq_idxs)\n    self.mask_fn: MaskFn = mask_fn\n    self.layernorm: bool = layernorm\n</code></pre>"},{"location":"reference/model_utils/task_projectors/task_projector/#auto_circuit.model_utils.task_projectors.task_projector.TaskProjector-functions","title":"Functions","text":""},{"location":"reference/model_utils/task_projectors/task_projector/#auto_circuit.model_utils.task_projectors.task_projector.TaskProjector.decode","title":"decode","text":"<pre><code>decode(x: Tensor) -&gt; Tensor\n</code></pre> <p>:param x: rotated data (shape: [..., [seq], n_inputs]) :return: unrotated data (shape: [..., [seq], n_inputs])</p> Source code in <code>auto_circuit/model_utils/task_projectors/task_projector.py</code> <pre><code>def decode(self, x: t.Tensor) -&gt; t.Tensor:\n    \"\"\"\n    :param x: rotated data (shape: [..., [seq], n_inputs])\n    :return: unrotated data (shape: [..., [seq], n_inputs])\n    \"\"\"\n    return self.rotation.inverse(x)\n</code></pre>"},{"location":"reference/model_utils/task_projectors/task_projector/#auto_circuit.model_utils.task_projectors.task_projector.TaskProjector.encode","title":"encode","text":"<pre><code>encode(x: Tensor) -&gt; Tensor\n</code></pre> <p>:param x: input data (shape: [..., [seq], n_inputs]) :return: projected rotated data (shape: [..., [seq], n_inputs])</p> Source code in <code>auto_circuit/model_utils/task_projectors/task_projector.py</code> <pre><code>def encode(self, x: t.Tensor) -&gt; t.Tensor:\n    \"\"\"\n    :param x: input data (shape: [..., [seq], n_inputs])\n    :return: projected rotated data (shape: [..., [seq], n_inputs])\n    \"\"\"\n    rotated = self.rotation(x)\n    einstr = []\n    if self.mask_fn == \"hard_concrete\":\n        mask_weights = sample_hard_concrete(self.dim_weights, x.size(0))\n        einstr.append(\"batch\")\n    elif self.mask_fn == \"sigmoid\":\n        mask_weights = t.sigmoid(self.dim_weights)\n    else:\n        assert self.mask_fn is None\n        mask_weights = self.dim_weights\n\n    if self.seq_len is not None:\n        einstr.append(\"seq\")\n\n    einstr = \" \".join(einstr) + \" d, batch seq d -&gt; batch seq d\"\n    masked_rotated = einsum(mask_weights, rotated, einstr)\n    return masked_rotated + self.bias\n</code></pre>"},{"location":"reference/model_utils/task_projectors/task_projector/#auto_circuit.model_utils.task_projectors.task_projector.TaskProjector.forward","title":"forward","text":"<pre><code>forward(x: Tensor) -&gt; Tensor\n</code></pre> <p>:param x: input data (shape: [..., n_inputs]) :return:  projected data (shape: [..., n_inputs])</p> Source code in <code>auto_circuit/model_utils/task_projectors/task_projector.py</code> <pre><code>def forward(self, x: t.Tensor) -&gt; t.Tensor:\n    \"\"\"\n    :param x: input data (shape: [..., n_inputs])\n    :return:  projected data (shape: [..., n_inputs])\n    \"\"\"\n    # x = self.wrapped_hook(x)\n    # projected_rotated = self.encode(x)\n    # projected_unrotated = self.decode(projected_rotated)\n    # return projected_unrotated\n    head_size = None\n    if head_dim := (x.ndim == 4):\n        head_size = x.shape[2]\n        x = x[:, :, 0]\n    if self.layernorm:\n        x = t.nn.functional.layer_norm(x, x.shape[-1:])\n    if self.seq_idxs is not None:\n        projected = (self.linear @ x[:, self.seq_idxs].unsqueeze(-1)).squeeze(-1)\n        out = x.clone()\n        out[:, self.seq_idxs] = projected + self.bias\n    else:\n        out = (self.linear @ x.unsqueeze(-1)).squeeze(-1) + self.bias\n    if head_dim:\n        assert head_size is not None\n        out = out.unsqueeze(2).repeat(1, 1, head_size, 1)\n    return out\n</code></pre>"},{"location":"reference/model_utils/task_projectors/task_projector/#auto_circuit.model_utils.task_projectors.task_projector-functions","title":"Functions","text":""},{"location":"reference/model_utils/task_projectors/task_projector_training/","title":"Task projector training","text":""},{"location":"reference/model_utils/task_projectors/task_projector_training/#auto_circuit.model_utils.task_projectors.task_projector_training","title":"auto_circuit.model_utils.task_projectors.task_projector_training","text":""},{"location":"reference/model_utils/task_projectors/task_projector_training/#auto_circuit.model_utils.task_projectors.task_projector_training-attributes","title":"Attributes","text":""},{"location":"reference/model_utils/task_projectors/task_projector_training/#auto_circuit.model_utils.task_projectors.task_projector_training-classes","title":"Classes","text":""},{"location":"reference/model_utils/task_projectors/task_projector_training/#auto_circuit.model_utils.task_projectors.task_projector_training-functions","title":"Functions","text":""},{"location":"reference/prune_algos/ACDC/","title":"ACDC","text":""},{"location":"reference/prune_algos/ACDC/#auto_circuit.prune_algos.ACDC","title":"auto_circuit.prune_algos.ACDC","text":""},{"location":"reference/prune_algos/ACDC/#auto_circuit.prune_algos.ACDC-attributes","title":"Attributes","text":""},{"location":"reference/prune_algos/ACDC/#auto_circuit.prune_algos.ACDC-classes","title":"Classes","text":""},{"location":"reference/prune_algos/ACDC/#auto_circuit.prune_algos.ACDC-functions","title":"Functions","text":""},{"location":"reference/prune_algos/ACDC/#auto_circuit.prune_algos.ACDC.acdc_prune_scores","title":"acdc_prune_scores","text":"<pre><code>acdc_prune_scores(model: PatchableModel, dataloader: PromptDataLoader, official_edges: Optional[Set[Edge]], tao_exps: List[int] = list(range(-5, -1)), tao_bases: List[int] = [1, 3, 5, 7, 9], faithfulness_target: Literal['kl_div', 'mse'] = 'kl_div', test_mode: bool = False, run_circuits_ref: Optional[Callable[..., CircuitOutputs]] = None, show_graphs: bool = False, draw_seq_graph_ref: Optional[Callable[..., Figure]] = None) -&gt; PruneScores\n</code></pre> <p>Run the ACDC algorithm from the paper \"Towards Automated Circuit Discovery for Mechanistic Interpretability\" (Conmy et al. (2023)).</p> <p>The algorithm does not assign scores to each edge, instead it finds the edges to be pruned given a certain value of tao. So we run the algorithm for several values of tao (each combination of <code>tao_exps</code> and <code>tao_bases</code>) and give equal scores to all edges that are pruned for a given tao. Then we use test_edge_counts to pass edge counts to run_circuits such that all edges with the same score are pruned together.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>PatchableModel</code> <p>The model to find the circuit for.</p> required <code>dataloader</code> <code>PromptDataLoader</code> <p>The dataloader to use for input and patches.</p> required <code>official_edges</code> <code>Optional[Set[Edge]]</code> <p>Not used.</p> required <code>tao_exps</code> <code>List[int]</code> <p>The exponents to use for the set of tao values.</p> <code>list(range(-5, -1))</code> <code>tao_bases</code> <code>List[int]</code> <p>The bases to use for the set of tao values.</p> <code>[1, 3, 5, 7, 9]</code> <code>faithfulness_target</code> <code>Literal['kl_div', 'mse']</code> <p>The faithfulness metric to optimize the circuit for.</p> <code>'kl_div'</code> <code>test_mode</code> <code>bool</code> <p>Run the model in test mode. This mode computes the output of each ablation again using the slower <code>run_circuits</code> function and checks that the result is the same.</p> <code>False</code> <code>run_circuits_ref</code> <code>Optional[Callable[..., CircuitOutputs]]</code> <p>Reference to the function <code>run_circuits</code> to use in test mode. Required to avoid a circular import.</p> <code>None</code> <code>show_graphs</code> <code>bool</code> <p>Whether to visualize the model activations during training using <code>draw_seq_graph</code>. Very slow on all but the smallest models.</p> <code>False</code> <code>draw_seq_graph_ref</code> <code>Optional[Callable[..., Figure]]</code> <p>Reference to the function <code>draw_seq_graph</code> to use in test mode. Required to avoid a circular import.</p> <code>None</code> <p>Returns:</p> Type Description <code>PruneScores</code> <p>An ordering of the edges by importance to the task. Importance is equal to the absolute value of the score assigned to the edge.</p> Note <p>Only the first batch of the <code>dataloader</code> is used.</p> Source code in <code>auto_circuit/prune_algos/ACDC.py</code> <pre><code>def acdc_prune_scores(\n    model: PatchableModel,\n    dataloader: PromptDataLoader,\n    official_edges: Optional[Set[Edge]],\n    tao_exps: List[int] = list(range(-5, -1)),\n    tao_bases: List[int] = [1, 3, 5, 7, 9],\n    faithfulness_target: Literal[\"kl_div\", \"mse\"] = \"kl_div\",\n    test_mode: bool = False,\n    run_circuits_ref: Optional[Callable[..., CircuitOutputs]] = None,\n    show_graphs: bool = False,\n    draw_seq_graph_ref: Optional[Callable[..., go.Figure]] = None,\n) -&gt; PruneScores:\n    \"\"\"\n    Run the ACDC algorithm from the paper \"Towards Automated Circuit Discovery for\n    Mechanistic Interpretability\"\n    ([Conmy et al. (2023)](https://arxiv.org/abs/2304.14997)).\n\n    The algorithm does not assign scores to each edge, instead it finds the edges to be\n    pruned given a certain value of tao. So we run the algorithm for several values of\n    tao (each combination of `tao_exps` and `tao_bases`) and give equal scores to all\n    edges that are pruned for a given tao. Then we use test_edge_counts to pass edge\n    counts to run_circuits such that all edges with the same score are pruned together.\n\n    Args:\n        model: The model to find the circuit for.\n        dataloader: The dataloader to use for input and patches.\n        official_edges: Not used.\n        tao_exps: The exponents to use for the set of tao values.\n        tao_bases: The bases to use for the set of tao values.\n        faithfulness_target: The faithfulness metric to optimize the circuit for.\n        test_mode: Run the model in test mode. This mode computes the output of each\n            ablation again using the slower\n            [`run_circuits`][auto_circuit.prune.run_circuits] function and checks that\n            the result is the same.\n        run_circuits_ref: Reference to the function\n            [`run_circuits`][auto_circuit.prune.run_circuits] to use in test mode.\n            Required to avoid a circular import.\n        show_graphs: Whether to visualize the model activations during training using\n            [`draw_seq_graph`][auto_circuit.visualize.draw_seq_graph]. Very slow on all\n            but the smallest models.\n        draw_seq_graph_ref: Reference to the function\n            [`draw_seq_graph`][auto_circuit.visualize.draw_seq_graph] to use in test\n            mode. Required to avoid a circular import.\n\n    Returns:\n        An ordering of the edges by importance to the task. Importance is equal to the\n            absolute value of the score assigned to the edge.\n\n    Note:\n        Only the first batch of the `dataloader` is used.\n    \"\"\"\n    model = model\n    test_model = deepcopy(model) if test_mode else None\n    out_slice = model.out_slice\n    edges: OrderedSet[Edge] = OrderedSet(\n        sorted(model.edges, key=lambda x: x.dest.layer, reverse=True)\n    )\n\n    prune_scores = model.new_prune_scores(init_val=t.inf)\n    for tao in (\n        pbar_tao := tqdm([a * 10**b for a, b in product(tao_bases, tao_exps)])\n    ):\n        pbar_tao.set_description_str(\"ACDC \\u03C4={:.7f}\".format(tao), refresh=True)\n\n        train_batch = next(iter(dataloader))\n        clean_batch, corrupt_batch = train_batch.clean, train_batch.corrupt\n\n        patch_outs_tensor = src_ablations(model, corrupt_batch)\n        src_outs_tensor = src_ablations(model, clean_batch)\n\n        with t.inference_mode():\n            clean_out = model(clean_batch)[out_slice]\n            toks, short_embd, attn_mask, resids = None, None, None, []\n            if model.is_transformer:\n                _, toks, short_embd, attn_mask = model.input_to_embed(clean_batch)\n                _, cache = model.run_with_cache(clean_batch)\n                n_layers = range(model.cfg.n_layers)\n                resids = [cache[f\"blocks.{i}.hook_resid_pre\"].clone() for i in n_layers]\n                del cache\n\n        clean_logprobs = t.nn.functional.log_softmax(clean_out, dim=-1)\n\n        prev_faith = 0.0\n        removed_edges: OrderedSet[Edge] = OrderedSet([])\n\n        set_all_masks(model, val=0.0)\n        # We set curr_src_outs manually so we can skip layers before the current edge.\n        with patch_mode(model, patch_outs_tensor, curr_src_outs=src_outs_tensor):\n            for edge_idx, edge in enumerate((pbar_edge := tqdm(edges))):\n                rmvd, left = len(removed_edges), edge_idx + 1 - len(removed_edges)\n                desc = f\"Removed: {rmvd}, Left: {left}, Current:'{edge}'\"\n                pbar_edge.set_description_str(desc, refresh=False)\n\n                edge.patch_mask(model).data[edge.patch_idx] = 1.0\n                with t.inference_mode():\n                    if model.is_transformer:\n                        start_layer = int(edge.dest.module_name.split(\".\")[1])\n                        out = model(\n                            resids[start_layer],\n                            start_at_layer=start_layer,\n                            tokens=toks,\n                            shortformer_pos_embed=short_embd,\n                            attention_mask=attn_mask,\n                        )[out_slice]\n                    else:\n                        out = model(clean_batch)[out_slice]\n\n                if test_mode:\n                    assert test_model is not None and run_circuits_ref is not None\n                    render = show_graphs and (random() &lt; 0.02 or len(edges) &lt; 20)\n\n                    print(\"ACDC model, with out=\", out) if render else None\n                    if render:\n                        assert draw_seq_graph_ref is not None\n                        draw_seq_graph_ref(model, None, True, True)\n\n                    print(\"Test mode: running pruned model\") if render else None\n                    p = dict([(m, (s == t.inf) * 1.0) for m, s in prune_scores.items()])\n                    p[edge.dest.module_name][edge.patch_idx] = 0.0  # Not in circuit\n                    n_edge = edge_counts_util(set(edges), None, p, zero_edges=False)[0]\n                    test_out = run_circuits_ref(\n                        model=test_model,\n                        dataloader=dataloader,\n                        test_edge_counts=[n_edge],\n                        prune_scores=p,\n                        patch_type=PatchType.TREE_PATCH,\n                        render_graph=render,\n                    )[n_edge][train_batch.key]\n                    print(\"Test_out:\", test_out) if render else None\n                    assert t.allclose(out, test_out, atol=1e-3)\n\n                if faithfulness_target == \"kl_div\":\n                    out_logprobs = log_softmax(out, dim=-1)\n                    faith = multibatch_kl_div(out_logprobs, clean_logprobs).item()\n                elif faithfulness_target == \"mse\":\n                    faith = mse_loss(out, clean_out).item()\n\n                if faith - prev_faith &lt; tao:  # Edge is unimportant\n                    removed_edges.add(edge)\n                    curr = edge.prune_score(prune_scores)\n                    prune_scores[edge.dest.module_name][edge.patch_idx] = min(tao, curr)\n                    prev_faith = faith\n                else:  # Edge is important - don't patch it\n                    edge.patch_mask(model).data[edge.patch_idx] = 0.0\n    return prune_scores\n</code></pre>"},{"location":"reference/prune_algos/activation_magnitude/","title":"Activation magnitude","text":""},{"location":"reference/prune_algos/activation_magnitude/#auto_circuit.prune_algos.activation_magnitude","title":"auto_circuit.prune_algos.activation_magnitude","text":""},{"location":"reference/prune_algos/activation_magnitude/#auto_circuit.prune_algos.activation_magnitude-attributes","title":"Attributes","text":""},{"location":"reference/prune_algos/activation_magnitude/#auto_circuit.prune_algos.activation_magnitude-classes","title":"Classes","text":""},{"location":"reference/prune_algos/activation_magnitude/#auto_circuit.prune_algos.activation_magnitude-functions","title":"Functions","text":""},{"location":"reference/prune_algos/activation_magnitude/#auto_circuit.prune_algos.activation_magnitude.activation_magnitude_prune_scores","title":"activation_magnitude_prune_scores","text":"<pre><code>activation_magnitude_prune_scores(model: PatchableModel, dataloader: PromptDataLoader, official_edges: Optional[Set[Edge]]) -&gt; PruneScores\n</code></pre> <p>Simple baseline circuit discovery algorithm. Prune scores are the mean activation magnitude of each edge.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>PatchableModel</code> <p>The model to find the circuit for.</p> required <code>dataloader</code> <code>PromptDataLoader</code> <p>The dataloader to use for input.</p> required <code>official_edges</code> <code>Optional[Set[Edge]]</code> <p>Not used.</p> required <p>Returns:</p> Type Description <code>PruneScores</code> <p>An ordering of the edges by importance to the task. Importance is equal to the absolute value of the score assigned to the edge.</p> Source code in <code>auto_circuit/prune_algos/activation_magnitude.py</code> <pre><code>def activation_magnitude_prune_scores(\n    model: PatchableModel,\n    dataloader: PromptDataLoader,\n    official_edges: Optional[Set[Edge]],\n) -&gt; PruneScores:\n    \"\"\"\n    Simple baseline circuit discovery algorithm. Prune scores are the mean activation\n    magnitude of each edge.\n\n    Args:\n        model: The model to find the circuit for.\n        dataloader: The dataloader to use for input.\n        official_edges: Not used.\n\n    Returns:\n        An ordering of the edges by importance to the task. Importance is equal to the\n            absolute value of the score assigned to the edge.\n    \"\"\"\n    prune_scores = model.new_prune_scores()\n    n_batches = len(dataloader)\n    with t.inference_mode():\n        for batch in dataloader:\n            src_outs = src_ablations(model, batch.clean, AblationType.RESAMPLE)\n            src_out_means = src_outs.mean(dim=list(range(1, src_outs.ndim)))\n            # prune_scores shape = seq_shape + head_shape + [prev_src_count]\n            for mod, ps in prune_scores.items():\n                n_srcs = ps.size(-1)\n                edge_acts = src_out_means[:n_srcs]\n                if ps.ndim &gt;= 2:\n                    edge_acts = edge_acts.unsqueeze(0).repeat(ps.shape[-2], 1)\n                if ps.ndim &gt;= 3:\n                    edge_acts = edge_acts.unsqueeze(0).repeat(ps.shape[-3], 1, 1)\n                prune_scores[mod] += edge_acts.abs() / n_batches\n    return prune_scores\n</code></pre>"},{"location":"reference/prune_algos/circuit_probing/","title":"Circuit probing","text":""},{"location":"reference/prune_algos/circuit_probing/#auto_circuit.prune_algos.circuit_probing","title":"auto_circuit.prune_algos.circuit_probing","text":""},{"location":"reference/prune_algos/circuit_probing/#auto_circuit.prune_algos.circuit_probing-attributes","title":"Attributes","text":""},{"location":"reference/prune_algos/circuit_probing/#auto_circuit.prune_algos.circuit_probing-classes","title":"Classes","text":""},{"location":"reference/prune_algos/circuit_probing/#auto_circuit.prune_algos.circuit_probing-functions","title":"Functions","text":""},{"location":"reference/prune_algos/circuit_probing/#auto_circuit.prune_algos.circuit_probing.circuit_probing_prune_scores","title":"circuit_probing_prune_scores","text":"<pre><code>circuit_probing_prune_scores(model: PatchableModel, dataloader: PromptDataLoader, official_edges: Optional[Set[Edge]], learning_rate: float = 0.1, epochs: int = 20, regularize_lambda: float = 10, mask_fn: MaskFn = 'hard_concrete', dropout_p: float = 0.0, init_val: float = -init_mask_val, show_train_graph: bool = False, circuit_sizes: List[int | Literal['true_size']] = ['true_size'], tree_optimisation: bool = False, avoid_edges: Optional[Set[Edge]] = None, avoid_lambda: float = 1.0, faithfulness_target: SP_FAITHFULNESS_TARGET = 'kl_div', validation_dataloader: Optional[PromptDataLoader] = None) -&gt; PruneScores\n</code></pre> <p>Wrapper of Subnetwork Probing that searches for circuits of different sizes and assigns scores to the edges according to the size of the smallest circuit that they are part of. Smaller circuits have higher scores because they contain more important edges. Edges not in any circuit are assigned a score of <code>0</code>.</p> <p>Parameters:</p> Name Type Description Default <code>circuit_sizes</code> <code>List[int | Literal['true_size']]</code> <p>List of circuit sizes to probe. If <code>\"true_size\"</code> is in the list, then we include the size of <code>official_edges</code> in the list. If <code>official_edges</code> is <code>None</code>, then we raise an error.</p> <code>['true_size']</code> Source code in <code>auto_circuit/prune_algos/circuit_probing.py</code> <pre><code>def circuit_probing_prune_scores(\n    model: PatchableModel,\n    dataloader: PromptDataLoader,\n    official_edges: Optional[Set[Edge]],\n    learning_rate: float = 0.1,\n    epochs: int = 20,\n    regularize_lambda: float = 10,\n    mask_fn: MaskFn = \"hard_concrete\",\n    dropout_p: float = 0.0,\n    init_val: float = -init_mask_val,\n    show_train_graph: bool = False,\n    circuit_sizes: List[int | Literal[\"true_size\"]] = [\"true_size\"],\n    tree_optimisation: bool = False,\n    avoid_edges: Optional[Set[Edge]] = None,\n    avoid_lambda: float = 1.0,\n    faithfulness_target: SP_FAITHFULNESS_TARGET = \"kl_div\",\n    validation_dataloader: Optional[PromptDataLoader] = None,\n) -&gt; PruneScores:\n    \"\"\"\n    Wrapper of\n    [Subnetwork Probing][auto_circuit.prune_algos.subnetwork_probing.subnetwork_probing_prune_scores]\n    that searches for circuits of different sizes and assigns scores to the edges\n    according to the size of the smallest circuit that they are part of. Smaller\n    circuits have higher scores because they contain more important edges. Edges not in\n    any circuit are assigned a score of `0`.\n\n    Args:\n        circuit_sizes: List of circuit sizes to probe. If `\"true_size\"` is in the list,\n            then we include the size of `official_edges` in the list. If\n            `official_edges` is `None`, then we raise an error.\n    \"\"\"  # noqa: W505, E501\n\n    sizes = []\n    for size in circuit_sizes:\n        if size == \"true_size\":\n            assert official_edges is not None\n            size = len(official_edges)\n        assert size &gt; 0\n        sizes.append(size)\n    assert len(set(sizes)) == len(sizes)\n    assert len(sizes) == len(circuit_sizes)\n    sorted_circuit_sizes = sorted(sizes)\n\n    prune_scores = model.new_prune_scores()\n\n    # Iterate over the circuit sizes in ascending order\n    for size_idx, size in enumerate((size_pbar := tqdm(sorted_circuit_sizes))):\n        size_pbar.set_description(f\"Circuit Probing Size {size}\")\n        assert (isinstance(size, int) and size &gt; 0) or size is None\n        new_prune_scores: PruneScores = subnetwork_probing_prune_scores(\n            model=model,\n            dataloader=dataloader,\n            official_edges=official_edges,\n            learning_rate=learning_rate,\n            epochs=epochs,\n            regularize_lambda=regularize_lambda,\n            mask_fn=mask_fn,\n            dropout_p=dropout_p,\n            init_val=init_val,\n            show_train_graph=show_train_graph,\n            circuit_size=size,\n            tree_optimisation=tree_optimisation,\n            avoid_edges=avoid_edges,\n            avoid_lambda=avoid_lambda,\n            faithfulness_target=faithfulness_target,\n            validation_dataloader=validation_dataloader,\n        )\n        assert all([t.all(ps &gt;= 0) for ps in new_prune_scores.values()])\n        threshold = prune_scores_threshold(new_prune_scores, size)\n        score = len(sorted_circuit_sizes) - size_idx\n        for mod, new_ps in new_prune_scores.items():\n            curr_ps = prune_scores[mod]\n            # Smaller circuits have higher scores. Bigger circuits don't overwrite\n            new_circuit = (new_ps &gt;= threshold) &amp; (curr_ps == 0)\n            prune_scores[mod] = t.where(new_circuit, score, curr_ps)\n    return prune_scores\n</code></pre>"},{"location":"reference/prune_algos/edge_attribution_patching/","title":"Edge attribution patching","text":""},{"location":"reference/prune_algos/edge_attribution_patching/#auto_circuit.prune_algos.edge_attribution_patching","title":"auto_circuit.prune_algos.edge_attribution_patching","text":""},{"location":"reference/prune_algos/edge_attribution_patching/#auto_circuit.prune_algos.edge_attribution_patching-attributes","title":"Attributes","text":""},{"location":"reference/prune_algos/edge_attribution_patching/#auto_circuit.prune_algos.edge_attribution_patching-classes","title":"Classes","text":""},{"location":"reference/prune_algos/edge_attribution_patching/#auto_circuit.prune_algos.edge_attribution_patching-functions","title":"Functions","text":""},{"location":"reference/prune_algos/edge_attribution_patching/#auto_circuit.prune_algos.edge_attribution_patching.edge_attribution_patching_prune_scores","title":"edge_attribution_patching_prune_scores","text":"<pre><code>edge_attribution_patching_prune_scores(model: PatchableModel, dataloader: PromptDataLoader, official_edges: Optional[Set[Edge]], answer_diff: bool = True) -&gt; PruneScores\n</code></pre> <p>Prune scores by Edge Attribution patching.</p> <p>This is an exact replication of the technique introduced in \"Attribution Patching Outperforms Automated Circuit Discovery\" (Syed et al. (2023)), as implemented in their codebase.</p> <p>It is equivalent to <code>mask_gradient_prune_scores</code> with <code>grad_function=\"logit\"</code> and <code>mask_val=0.0</code>. We verify that the output is exactly the same in <code>test_edge_attribution_patching.py</code>. This implementation is much slower, so we don't use it in practice, but it's useful for validating the correctness of the fast implementation.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>PatchableModel</code> <p>The model to find the circuit for.</p> required <code>dataloader</code> <code>PromptDataLoader</code> <p>The dataloader to use for input and ablation.</p> required <code>official_edges</code> <code>Optional[Set[Edge]]</code> <p>Not used.</p> required <p>Returns:</p> Type Description <code>PruneScores</code> <p>An ordering of the edges by importance to the task. Importance is equal to the absolute value of the score assigned to the edge.</p> Note <p>The implementation here uses <code>clean_act - corrupt_act</code>, as described in the paper, rather than <code>corrupt_act - clean_act</code>, as in author's implementation. It doesn't matter either way as we only consider the magnitude of the scores.</p> Source code in <code>auto_circuit/prune_algos/edge_attribution_patching.py</code> <pre><code>def edge_attribution_patching_prune_scores(\n    model: PatchableModel,\n    dataloader: PromptDataLoader,\n    official_edges: Optional[Set[Edge]],\n    answer_diff: bool = True,\n) -&gt; PruneScores:\n    \"\"\"\n    Prune scores by Edge Attribution patching.\n\n    This is an exact replication of the technique introduced in \"Attribution Patching\n    Outperforms Automated Circuit Discovery\"\n    [(Syed et al. (2023))](https://arxiv.org/abs/2310.10348), as\n    implemented in\n    [their codebase](https://github.com/Aaquib111/edge-attribution-patching/utils/prune_utils.py).\n\n    It is equivalent to\n    [`mask_gradient_prune_scores`][auto_circuit.prune_algos.mask_gradient.mask_gradient_prune_scores]\n    with `grad_function=\"logit\"` and `mask_val=0.0`. We verify that the output is\n    exactly the same in `test_edge_attribution_patching.py`. This\n    implementation is much slower, so we don't use it in practice, but it's useful for\n    validating the correctness of the fast implementation.\n\n    Args:\n        model: The model to find the circuit for.\n        dataloader: The dataloader to use for input and ablation.\n        official_edges: Not used.\n\n    Returns:\n        An ordering of the edges by importance to the task. Importance is equal to the\n            absolute value of the score assigned to the edge.\n\n    Note:\n        The implementation here uses `clean_act - corrupt_act`, as described in the\n        paper, rather than `corrupt_act - clean_act`, as in author's implementation. It\n        doesn't matter either way as we only consider the magnitude of the scores.\n    \"\"\"\n    model = model\n    assert model.is_transformer\n    out_slice = model.out_slice\n\n    set_all_masks(model, val=0.0)\n    model.train()\n    for param in model.parameters():\n        param.requires_grad = True\n    model.zero_grad()\n    prune_scores = model.new_prune_scores()\n\n    for batch in dataloader:\n        clean_grad_cache = {}\n\n        def backward_cache_hook(act: t.Tensor, hook: tl.hook_points.HookPoint):\n            clean_grad_cache[hook.name] = act.detach()\n\n        incoming_ends = [\n            \"hook_q_input\",\n            \"hook_k_input\",\n            \"hook_v_input\",\n            f\"blocks.{model.cfg.n_layers-1}.hook_resid_post\",\n        ]\n        if not model.cfg.attn_only:\n            incoming_ends.append(\"hook_mlp_in\")\n\n        def edge_acdcpp_back_filter(name: str) -&gt; bool:\n            return name.endswith(tuple(incoming_ends + [\"hook_q\", \"hook_k\", \"hook_v\"]))\n\n        model.add_hook(edge_acdcpp_back_filter, backward_cache_hook, \"bwd\")\n        logits = model(batch.clean)[out_slice]\n        if answer_diff:\n            loss = -batch_avg_answer_diff(logits, batch)\n        else:\n            loss = -batch_avg_answer_val(logits, batch)\n        loss.backward()\n        model.reset_hooks()\n\n        _, corrupt_cache = model.run_with_cache(batch.corrupt, return_type=\"logits\")\n        _, clean_cache = model.run_with_cache(batch.clean, return_type=\"logits\")\n\n        for edge in model.edges:\n            if edge.dest.head_idx is None:\n                grad = clean_grad_cache[edge.dest.module_name]\n            else:\n                grad = clean_grad_cache[edge.dest.module_name][:, :, edge.dest.head_idx]\n            if edge.src.head_idx is None:\n                src_clean_act = clean_cache[edge.src.module_name]\n                src_corrupt_act = corrupt_cache[edge.src.module_name]\n            else:\n                src_clean_act = clean_cache[edge.src.module_name][\n                    :, :, edge.src.head_idx\n                ]\n                src_corrupt_act = corrupt_cache[edge.src.module_name][\n                    :, :, edge.src.head_idx\n                ]\n            assert grad is not None\n            score = (grad * (src_corrupt_act - src_clean_act)).sum().item()\n            prune_scores[edge.dest.module_name][edge.patch_idx] += score\n    model.eval()\n    for param in model.parameters():\n        param.requires_grad = False\n    return prune_scores  # type: ignore\n</code></pre>"},{"location":"reference/prune_algos/ground_truth/","title":"Ground truth","text":""},{"location":"reference/prune_algos/ground_truth/#auto_circuit.prune_algos.ground_truth","title":"auto_circuit.prune_algos.ground_truth","text":""},{"location":"reference/prune_algos/ground_truth/#auto_circuit.prune_algos.ground_truth-attributes","title":"Attributes","text":""},{"location":"reference/prune_algos/ground_truth/#auto_circuit.prune_algos.ground_truth-classes","title":"Classes","text":""},{"location":"reference/prune_algos/ground_truth/#auto_circuit.prune_algos.ground_truth-functions","title":"Functions","text":""},{"location":"reference/prune_algos/ground_truth/#auto_circuit.prune_algos.ground_truth.ground_truth_prune_scores","title":"ground_truth_prune_scores","text":"<pre><code>ground_truth_prune_scores(model: PatchableModel, dataloader: PromptDataLoader, official_edges: Optional[Set[Edge]]) -&gt; PruneScores\n</code></pre> <p>Assigns <code>1</code> for edges that are in the ground truth circuit, <code>0</code> otherwise.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>PatchableModel</code> <p>The model on which this circuit was discovered.</p> required <code>dataloader</code> <code>PromptDataLoader</code> <p>Not used.</p> required <code>official_edges</code> <code>Optional[Set[Edge]]</code> <p>The edges of the circuit.</p> required <p>Returns:</p> Type Description <code>PruneScores</code> <p>An ordering of the edges by importance to the task. Importance is equal to the absolute value of the score assigned to the edge.</p> Source code in <code>auto_circuit/prune_algos/ground_truth.py</code> <pre><code>def ground_truth_prune_scores(\n    model: PatchableModel,\n    dataloader: PromptDataLoader,\n    official_edges: Optional[Set[Edge]],\n) -&gt; PruneScores:\n    \"\"\"\n    Assigns `1` for edges that are in the ground truth circuit, `0` otherwise.\n\n    Args:\n        model: The model on which this circuit was discovered.\n        dataloader: Not used.\n        official_edges: The edges of the circuit.\n\n    Returns:\n        An ordering of the edges by importance to the task. Importance is equal to the\n            absolute value of the score assigned to the edge.\n    \"\"\"\n    prune_scores: PruneScores = model.new_prune_scores()\n\n    if official_edges is None:\n        raise ValueError(\"Official edges must be provided for ground truth pruning.\")\n    for edge in official_edges:\n        prune_scores[edge.dest.module_name][edge.patch_idx] = 1.0\n    return prune_scores\n</code></pre>"},{"location":"reference/prune_algos/mask_gradient/","title":"Mask gradient","text":""},{"location":"reference/prune_algos/mask_gradient/#auto_circuit.prune_algos.mask_gradient","title":"auto_circuit.prune_algos.mask_gradient","text":""},{"location":"reference/prune_algos/mask_gradient/#auto_circuit.prune_algos.mask_gradient-attributes","title":"Attributes","text":""},{"location":"reference/prune_algos/mask_gradient/#auto_circuit.prune_algos.mask_gradient-classes","title":"Classes","text":""},{"location":"reference/prune_algos/mask_gradient/#auto_circuit.prune_algos.mask_gradient-functions","title":"Functions","text":""},{"location":"reference/prune_algos/mask_gradient/#auto_circuit.prune_algos.mask_gradient.mask_gradient_prune_scores","title":"mask_gradient_prune_scores","text":"<pre><code>mask_gradient_prune_scores(model: PatchableModel, dataloader: PromptDataLoader, official_edges: Optional[Set[Edge]], grad_function: Literal['logit', 'prob', 'logprob', 'logit_exp'], answer_function: Literal['avg_diff', 'avg_val', 'mse'], mask_val: Optional[float] = None, integrated_grad_samples: Optional[int] = None, ablation_type: AblationType = AblationType.RESAMPLE, clean_corrupt: Optional[Literal['clean', 'corrupt']] = 'corrupt') -&gt; PruneScores\n</code></pre> <p>Prune scores equal to the gradient of the mask values that interpolates the edges between the clean activations and the ablated activations.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>PatchableModel</code> <p>The model to find the circuit for.</p> required <code>dataloader</code> <code>PromptDataLoader</code> <p>The dataloader to use for input.</p> required <code>official_edges</code> <code>Optional[Set[Edge]]</code> <p>Not used.</p> required <code>grad_function</code> <code>Literal['logit', 'prob', 'logprob', 'logit_exp']</code> <p>Function to apply to the logits before taking the gradient.</p> required <code>answer_function</code> <code>Literal['avg_diff', 'avg_val', 'mse']</code> <p>Loss function of the model output which the gradient is taken with respect to.</p> required <code>mask_val</code> <code>Optional[float]</code> <p>Value of the mask to use for the forward pass. Cannot be used if <code>integrated_grad_samples</code> is not <code>None</code>.</p> <code>None</code> <code>integrated_grad_samples</code> <code>Optional[int]</code> <p>If not <code>None</code>, we compute an approximation of the Integrated Gradients (Sundararajan et al., 2017) of the model output with respect to the mask values. This is computed by averaging the mask gradients over <code>integrated_grad_samples</code> samples of the mask values interpolated between 0 and 1. Cannot be used if <code>mask_val</code> is not <code>None</code>.</p> <code>None</code> <code>ablation_type</code> <code>AblationType</code> <p>The type of ablation to perform.</p> <code>RESAMPLE</code> <code>clean_corrupt</code> <code>Optional[Literal['clean', 'corrupt']]</code> <p>Whether to use the clean or corrupt inputs to calculate the ablations.</p> <code>'corrupt'</code> <p>Returns:</p> Type Description <code>PruneScores</code> <p>An ordering of the edges by importance to the task. Importance is equal to the absolute value of the score assigned to the edge.</p> Note <p>When <code>grad_function=\"logit\"</code> and <code>mask_val=0</code> this function is exactly equivalent to <code>edge_attribution_patching_prune_scores</code>.</p> Source code in <code>auto_circuit/prune_algos/mask_gradient.py</code> <pre><code>def mask_gradient_prune_scores(\n    model: PatchableModel,\n    dataloader: PromptDataLoader,\n    official_edges: Optional[Set[Edge]],\n    grad_function: Literal[\"logit\", \"prob\", \"logprob\", \"logit_exp\"],\n    answer_function: Literal[\"avg_diff\", \"avg_val\", \"mse\"],\n    mask_val: Optional[float] = None,\n    integrated_grad_samples: Optional[int] = None,\n    ablation_type: AblationType = AblationType.RESAMPLE,\n    clean_corrupt: Optional[Literal[\"clean\", \"corrupt\"]] = \"corrupt\",\n) -&gt; PruneScores:\n    \"\"\"\n    Prune scores equal to the gradient of the mask values that interpolates the edges\n    between the clean activations and the ablated activations.\n\n    Args:\n        model: The model to find the circuit for.\n        dataloader: The dataloader to use for input.\n        official_edges: Not used.\n        grad_function: Function to apply to the logits before taking the gradient.\n        answer_function: Loss function of the model output which the gradient is taken\n            with respect to.\n        mask_val: Value of the mask to use for the forward pass. Cannot be used if\n            `integrated_grad_samples` is not `None`.\n        integrated_grad_samples: If not `None`, we compute an approximation of the\n            Integrated Gradients\n            [(Sundararajan et al., 2017)](https://arxiv.org/abs/1703.01365) of the model\n            output with respect to the mask values. This is computed by averaging the\n            mask gradients over `integrated_grad_samples` samples of the mask values\n            interpolated between 0 and 1. Cannot be used if `mask_val` is not `None`.\n        ablation_type: The type of ablation to perform.\n        clean_corrupt: Whether to use the clean or corrupt inputs to calculate the\n            ablations.\n\n    Returns:\n        An ordering of the edges by importance to the task. Importance is equal to the\n            absolute value of the score assigned to the edge.\n\n    Note:\n        When `grad_function=\"logit\"` and `mask_val=0` this function is exactly\n        equivalent to\n        [`edge_attribution_patching_prune_scores`][auto_circuit.prune_algos.edge_attribution_patching.edge_attribution_patching_prune_scores].\n    \"\"\"\n    assert (mask_val is not None) ^ (integrated_grad_samples is not None)  # ^ means XOR\n    model = model\n    out_slice = model.out_slice\n\n    src_outs: Dict[BatchKey, t.Tensor] = batch_src_ablations(\n        model,\n        dataloader,\n        ablation_type=ablation_type,\n        clean_corrupt=clean_corrupt,\n    )\n\n    with train_mask_mode(model):\n        for sample in (ig_pbar := tqdm(range((integrated_grad_samples or 0) + 1))):\n            ig_pbar.set_description_str(f\"Sample: {sample}\")\n            # Interpolate the mask value if integrating gradients. Else set the value.\n            if integrated_grad_samples is not None:\n                set_all_masks(model, val=sample / integrated_grad_samples)\n            else:\n                assert mask_val is not None and integrated_grad_samples is None\n                set_all_masks(model, val=mask_val)\n\n            for batch in dataloader:\n                patch_src_outs = src_outs[batch.key].clone().detach()\n                with patch_mode(model, patch_src_outs):\n                    logits = model(batch.clean)[out_slice]\n                    if grad_function == \"logit\":\n                        token_vals = logits\n                    elif grad_function == \"prob\":\n                        token_vals = t.softmax(logits, dim=-1)\n                    elif grad_function == \"logprob\":\n                        token_vals = log_softmax(logits, dim=-1)\n                    elif grad_function == \"logit_exp\":\n                        numerator = t.exp(logits)\n                        denominator = numerator.sum(dim=-1, keepdim=True)\n                        token_vals = numerator / denominator.detach()\n                    else:\n                        raise ValueError(f\"Unknown grad_function: {grad_function}\")\n\n                    if answer_function == \"avg_diff\":\n                        loss = -batch_avg_answer_diff(token_vals, batch)\n                    elif answer_function == \"avg_val\":\n                        loss = -batch_avg_answer_val(token_vals, batch)\n                    elif answer_function == \"mse\":\n                        loss = t.nn.functional.mse_loss(token_vals, batch.answers)\n                    else:\n                        raise ValueError(f\"Unknown answer_function: {answer_function}\")\n\n                    loss.backward()\n\n    prune_scores: PruneScores = {}\n    for dest_wrapper in model.dest_wrappers:\n        grad = dest_wrapper.patch_mask.grad\n        assert grad is not None\n        prune_scores[dest_wrapper.module_name] = grad.detach().clone()\n    return prune_scores\n</code></pre>"},{"location":"reference/prune_algos/parameter_integrated_gradients/","title":"Parameter integrated gradients","text":""},{"location":"reference/prune_algos/parameter_integrated_gradients/#auto_circuit.prune_algos.parameter_integrated_gradients","title":"auto_circuit.prune_algos.parameter_integrated_gradients","text":""},{"location":"reference/prune_algos/prune_algos/","title":"Prune algos","text":""},{"location":"reference/prune_algos/prune_algos/#auto_circuit.prune_algos.prune_algos","title":"auto_circuit.prune_algos.prune_algos","text":""},{"location":"reference/prune_algos/prune_algos/#auto_circuit.prune_algos.prune_algos-attributes","title":"Attributes","text":""},{"location":"reference/prune_algos/prune_algos/#auto_circuit.prune_algos.prune_algos-classes","title":"Classes","text":""},{"location":"reference/prune_algos/prune_algos/#auto_circuit.prune_algos.prune_algos.PruneAlgo","title":"PruneAlgo  <code>dataclass</code>","text":"<pre><code>PruneAlgo(key: AlgoKey, name: str, func: Callable[[PatchableModel, PromptDataLoader, Optional[Set[Edge]]], PruneScores], _short_name: Optional[str] = None)\n</code></pre> <p>An algorithm that finds the importance of each edge in a model for a given task.</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>AlgoKey</code> <p>A unique identifier for the algorithm.</p> required <code>name</code> <code>str</code> <p>The name of the algorithm.</p> required <code>func</code> <code>Callable[[PatchableModel, PromptDataLoader, Optional[Set[Edge]]], PruneScores]</code> <p>The function that computes the importance of each edge.</p> required <code>_short_name</code> <code>Optional[str]</code> <p>A short name for the algorithm. If not provided, <code>name</code> is used.</p> <code>None</code>"},{"location":"reference/prune_algos/prune_algos/#auto_circuit.prune_algos.prune_algos-functions","title":"Functions","text":""},{"location":"reference/prune_algos/prune_algos/#auto_circuit.prune_algos.prune_algos.run_prune_algos","title":"run_prune_algos","text":"<pre><code>run_prune_algos(tasks: List[Task], prune_algos: List[PruneAlgo]) -&gt; TaskPruneScores\n</code></pre> <p>Run a list of pruning algorithms on a list of tasks.</p> <p>Parameters:</p> Name Type Description Default <code>tasks</code> <code>List[Task]</code> <p>The tasks to run the algorithms on.</p> required <code>prune_algos</code> <code>List[PruneAlgo]</code> <p>The algorithms to run on the tasks.</p> required <p>Returns:</p> Type Description <code>TaskPruneScores</code> <p>A nested dictionary of the prune scores for each task and algorithm.</p> Source code in <code>auto_circuit/prune_algos/prune_algos.py</code> <pre><code>def run_prune_algos(tasks: List[Task], prune_algos: List[PruneAlgo]) -&gt; TaskPruneScores:\n    \"\"\"\n    Run a list of pruning algorithms on a list of tasks.\n\n    Args:\n        tasks: The tasks to run the algorithms on.\n        prune_algos: The algorithms to run on the tasks.\n\n    Returns:\n        A nested dictionary of the prune scores for each task and algorithm.\n    \"\"\"\n    task_prune_scores: TaskPruneScores = {}\n    for task in (experiment_pbar := tqdm(tasks)):\n        experiment_pbar.set_description_str(f\"Task: {task.name}\")\n        prune_scores_dict: AlgoPruneScores = {}\n        for prune_algo in (prune_score_pbar := tqdm(prune_algos)):\n            prune_score_pbar.set_description_str(f\"Prune scores: {prune_algo.name}\")\n            ps = prune_algo.func(task.model, task.train_loader, task.true_edges)\n            prune_scores_dict[prune_algo.key] = ps\n        task_prune_scores[task.key] = prune_scores_dict\n    return task_prune_scores\n</code></pre>"},{"location":"reference/prune_algos/random_edges/","title":"Random edges","text":""},{"location":"reference/prune_algos/random_edges/#auto_circuit.prune_algos.random_edges","title":"auto_circuit.prune_algos.random_edges","text":""},{"location":"reference/prune_algos/random_edges/#auto_circuit.prune_algos.random_edges-attributes","title":"Attributes","text":""},{"location":"reference/prune_algos/random_edges/#auto_circuit.prune_algos.random_edges-classes","title":"Classes","text":""},{"location":"reference/prune_algos/random_edges/#auto_circuit.prune_algos.random_edges-functions","title":"Functions","text":""},{"location":"reference/prune_algos/random_edges/#auto_circuit.prune_algos.random_edges.random_prune_scores","title":"random_prune_scores","text":"<pre><code>random_prune_scores(model: PatchableModel, dataloader: PromptDataLoader, official_edges: Optional[Set[Edge]]) -&gt; PruneScores\n</code></pre> <p>Prune scores are the mean activation magnitude of each edge.</p> Source code in <code>auto_circuit/prune_algos/random_edges.py</code> <pre><code>def random_prune_scores(\n    model: PatchableModel,\n    dataloader: PromptDataLoader,\n    official_edges: Optional[Set[Edge]],\n) -&gt; PruneScores:\n    \"\"\"Prune scores are the mean activation magnitude of each edge.\"\"\"\n    \"\"\"\n    Random baseline circuit discovery algorithm. Prune scores are random values.\n\n    Args:\n        model: The model to find the circuit for.\n        dataloader: Not used.\n        official_edges: Not used.\n\n    Returns:\n        An ordering of the edges by importance to the task. Importance is equal to the\n            absolute value of the score assigned to the edge.\n    \"\"\"\n    prune_scores: PruneScores = {}\n    for mod_name, patch_mask in model.patch_masks.items():\n        prune_scores[mod_name] = t.rand_like(patch_mask.data)\n    return prune_scores\n</code></pre>"},{"location":"reference/prune_algos/subnetwork_probing/","title":"Subnetwork probing","text":""},{"location":"reference/prune_algos/subnetwork_probing/#auto_circuit.prune_algos.subnetwork_probing","title":"auto_circuit.prune_algos.subnetwork_probing","text":""},{"location":"reference/prune_algos/subnetwork_probing/#auto_circuit.prune_algos.subnetwork_probing-attributes","title":"Attributes","text":""},{"location":"reference/prune_algos/subnetwork_probing/#auto_circuit.prune_algos.subnetwork_probing-classes","title":"Classes","text":""},{"location":"reference/prune_algos/subnetwork_probing/#auto_circuit.prune_algos.subnetwork_probing-functions","title":"Functions","text":""},{"location":"reference/prune_algos/subnetwork_probing/#auto_circuit.prune_algos.subnetwork_probing.subnetwork_probing_prune_scores","title":"subnetwork_probing_prune_scores","text":"<pre><code>subnetwork_probing_prune_scores(model: PatchableModel, dataloader: PromptDataLoader, official_edges: Optional[Set[Edge]], learning_rate: float = 0.1, epochs: int = 20, regularize_lambda: float = 10, mask_fn: MaskFn = 'hard_concrete', dropout_p: float = 0.0, init_val: float = init_mask_val, show_train_graph: bool = False, circuit_size: Optional[int] = None, tree_optimisation: bool = False, avoid_edges: Optional[Set[Edge]] = None, avoid_lambda: float = 1.0, faithfulness_target: SP_FAITHFULNESS_TARGET = 'kl_div', validation_dataloader: Optional[PromptDataLoader] = None) -&gt; PruneScores\n</code></pre> <p>Optimize the edge mask values using gradient descent to maximize the faithfulness of and minimize the number of edges in the circuit. This is based loosely on Subnetwork Probing (Cao et al., 2021).</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>PatchableModel</code> <p>The model to find the circuit for.</p> required <code>dataloader</code> <code>PromptDataLoader</code> <p>The dataloader to use for training input and ablation.</p> required <code>official_edges</code> <code>Optional[Set[Edge]]</code> <p>Not used.</p> required <code>learning_rate</code> <code>float</code> <p>The learning rate for the optimizer.</p> <code>0.1</code> <code>epochs</code> <code>int</code> <p>The number of epochs to train for.</p> <code>20</code> <code>regularize_lambda</code> <code>float</code> <p>The weight of the regularization term, that tries to minimize the number of edges in the circuit.</p> <code>10</code> <code>mask_fn</code> <code>MaskFn</code> <p>The function to use to transform the mask values before they are used to interpolate edges between the clean and ablated activations. Note that <code>\"hard_concrete\"</code> is generally recommended and is often critical for strong performance. See <code>MaskFn</code> for more details.</p> <code>'hard_concrete'</code> <code>dropout_p</code> <code>float</code> <p>The dropout probability of the masks to use during training.</p> <code>0.0</code> <code>init_val</code> <code>float</code> <p>The initial value of the mask values. This can be sensitive when using the <code>\"hard_concrete\"</code> mask function. The default value is the value used by Cao et al.</p> <code>init_mask_val</code> <code>show_train_graph</code> <code>bool</code> <p>Whether to show a graph of the training loss.</p> <code>False</code> <code>circuit_size</code> <code>Optional[int]</code> <p>The size of the circuit to aim for. When this is not <code>None</code>, the regularization term equals <code>ReLU(n_mask - circuit_size)</code> (sign is corrected for the value of <code>tree_optimisation</code>).</p> <code>None</code> <code>tree_optimisation</code> <code>bool</code> <p>If <code>True</code>, the input to the model is the clean input, and the mask values are optimized to ablate as many edges as possible. If <code>False</code>, the corrupt input is used, and the mask values are optimized to Resample Ablate (with the clean activations) as few edges as possible.</p> <code>False</code> <code>avoid_edges</code> <code>Optional[Set[Edge]]</code> <p>A set of edges to avoid. An extra penalty is added to the loss for each edge in this set that is included in the circuit.</p> <code>None</code> <code>avoid_lambda</code> <code>float</code> <p>The weight of the penalty for <code>avoid_edges</code>.</p> <code>1.0</code> <code>faithfulness_target</code> <code>SP_FAITHFULNESS_TARGET</code> <p>The faithfulness metric to optimize the circuit for.</p> <code>'kl_div'</code> <code>validation_dataloader</code> <code>Optional[PromptDataLoader]</code> <p>If not <code>None</code> the faithfulness metric is also computed on this dataloader and plotted in the training graph (if <code>show_train_graph</code> is <code>True</code>).</p> <code>None</code> <p>Returns:</p> Type Description <code>PruneScores</code> <p>An ordering of the edges by importance to the task. Importance is equal to the absolute value of the score assigned to the edge.</p> Source code in <code>auto_circuit/prune_algos/subnetwork_probing.py</code> <pre><code>def subnetwork_probing_prune_scores(\n    model: PatchableModel,\n    dataloader: PromptDataLoader,\n    official_edges: Optional[Set[Edge]],\n    learning_rate: float = 0.1,\n    epochs: int = 20,\n    regularize_lambda: float = 10,\n    mask_fn: MaskFn = \"hard_concrete\",\n    dropout_p: float = 0.0,\n    init_val: float = init_mask_val,\n    show_train_graph: bool = False,\n    circuit_size: Optional[int] = None,\n    tree_optimisation: bool = False,\n    avoid_edges: Optional[Set[Edge]] = None,\n    avoid_lambda: float = 1.0,\n    faithfulness_target: SP_FAITHFULNESS_TARGET = \"kl_div\",\n    validation_dataloader: Optional[PromptDataLoader] = None,\n) -&gt; PruneScores:\n    \"\"\"\n    Optimize the edge mask values using gradient descent to maximize the faithfulness of\n    and minimize the number of edges in the circuit. This is based loosely on Subnetwork\n    Probing [(Cao et al., 2021)](https://arxiv.org/abs/2104.03514).\n\n    Args:\n        model: The model to find the circuit for.\n        dataloader: The dataloader to use for training input and ablation.\n        official_edges: Not used.\n        learning_rate: The learning rate for the optimizer.\n        epochs: The number of epochs to train for.\n        regularize_lambda: The weight of the regularization term, that tries to minimize\n            the number of edges in the circuit.\n        mask_fn: The function to use to transform the mask values before they are used\n            to interpolate edges between the clean and ablated activations. Note that\n            `\"hard_concrete\"` is generally recommended and is often critical for strong\n            performance. See [`MaskFn`][auto_circuit.types.MaskFn] for more details.\n        dropout_p: The dropout probability of the masks to use during training.\n        init_val: The initial value of the mask values. This can be sensitive when using\n            the `\"hard_concrete\"` mask function. The default value is the value used by\n            Cao et al.\n        show_train_graph: Whether to show a graph of the training loss.\n        circuit_size: The size of the circuit to aim for. When this is not `None`, the\n            regularization term equals `ReLU(n_mask - circuit_size)` (sign is corrected\n            for the value of `tree_optimisation`).\n        tree_optimisation: If `True`, the input to the model is the clean input, and the\n            mask values are optimized to ablate as many edges as possible. If `False`,\n            the corrupt input is used, and the mask values are optimized to Resample\n            Ablate (with the clean activations) as few edges as possible.\n        avoid_edges: A set of edges to avoid. An extra penalty is added to the loss for\n            each edge in this set that is included in the circuit.\n        avoid_lambda: The weight of the penalty for `avoid_edges`.\n        faithfulness_target: The faithfulness metric to optimize the circuit for.\n        validation_dataloader: If not `None` the faithfulness metric is also computed on\n            this dataloader and plotted in the training graph (if `show_train_graph` is\n            `True`).\n\n    Returns:\n        An ordering of the edges by importance to the task. Importance is equal to the\n            absolute value of the score assigned to the edge.\n    \"\"\"\n    assert len(dataloader) &gt; 0, \"Dataloader is empty\"\n\n    out_slice = model.out_slice\n    n_edges = model.n_edges\n    n_avoid = len(avoid_edges or [])\n\n    clean_logits: Dict[BatchKey, t.Tensor] = {}\n    with t.inference_mode():\n        for batch in dataloader:\n            clean_logits[batch.key] = model(batch.clean)[out_slice]\n\n    val_clean_logits: Optional[Dict[BatchKey, t.Tensor]] = None\n    if validation_dataloader is not None:\n        val_clean_logits = {}\n        with t.inference_mode():\n            for batch in validation_dataloader:\n                val_clean_out = model(batch.clean)[out_slice]\n                val_clean_logits[batch.key] = log_softmax(val_clean_out, dim=-1)\n\n    src_outs: Dict[BatchKey, t.Tensor] = batch_src_ablations(\n        model,\n        dataloader,\n        # ablation_type=AblationType.RESAMPLE,\n        ablation_type=AblationType.TOKENWISE_MEAN_CORRUPT,\n        # clean_corrupt=\"corrupt\" if tree_optimisation else \"clean\",\n    )\n\n    val_src_outs: Optional[Dict[BatchKey, t.Tensor]] = None\n    if validation_dataloader is not None:\n        val_src_outs = batch_src_ablations(\n            model,\n            validation_dataloader,\n            # ablation_type=AblationType.RESAMPLE,\n            ablation_type=AblationType.TOKENWISE_MEAN_CORRUPT,\n            # clean_corrupt=\"corrupt\" if tree_optimisation else \"clean\",\n        )\n\n    losses, faiths, val_faiths, val_stds, regularizes = [], [], [], [], []\n    set_all_masks(model, val=init_val if tree_optimisation else -init_val)\n    with train_mask_mode(model) as patch_masks, mask_fn_mode(model, mask_fn, dropout_p):\n        mask_params = patch_masks.values()\n        optim = t.optim.Adam(mask_params, lr=learning_rate)\n        for epoch in (epoch_pbar := tqdm(range(epochs))):\n            faith_str = f\"{faithfulness_target}: {faiths[-1]:.3f}\" if epoch &gt; 0 else \"\"\n            desc = f\"Loss: {losses[-1]:.3f}, {faith_str}\" if epoch &gt; 0 else \"\"\n            epoch_pbar.set_description_str(f\"{SP} Epoch {epoch} \" + desc, refresh=False)\n            for batch_idx, batch in enumerate(dataloader):\n                input_batch = batch.clean if tree_optimisation else batch.corrupt\n                patch_outs = src_outs[batch.key].clone().detach()\n                with patch_mode(model, patch_outs):\n                    train_logits = model(input_batch)[out_slice]\n                    if faithfulness_target == \"kl_div\":\n                        faithful_term = multibatch_kl_div(\n                            log_softmax(train_logits, dim=-1),\n                            log_softmax(clean_logits[batch.key], dim=-1),\n                        )\n                    elif faithfulness_target == \"mse\":\n                        faithful_term = mse_loss(train_logits, batch.answers)\n                    elif faithfulness_target == \"correct_percent\":\n                        faithful_term = correct_answer_proportion(train_logits, batch)\n                    elif faithfulness_target == \"logit_diff_percent\":\n                        logit_diffs = batch_answer_diff_percents(\n                            train_logits, clean_logits[batch.key], batch\n                        )\n                        logit_diff_term = t.abs(100 - logit_diffs).mean()\n                        faithful_term = logit_diff_term\n                    else:\n                        assert faithfulness_target in [\"answer\", \"wrong_answer\"]\n                        wrong = faithfulness_target == \"wrong_answer\"\n                        faithful_term = -batch_avg_answer_val(\n                            train_logits, batch, wrong\n                        )\n                    masks = t.cat([patch_mask.flatten() for patch_mask in mask_params])\n                    if mask_fn == \"hard_concrete\":\n                        masks = sample_hard_concrete(masks, batch_size=1)\n                    elif mask_fn == \"sigmoid\":\n                        masks = t.sigmoid(masks)\n                    n_mask = n_edges - masks.sum() if tree_optimisation else masks.sum()\n                    if circuit_size:\n                        n_mask = t.relu(n_mask - circuit_size)\n                    regularize = n_mask / (circuit_size if circuit_size else n_edges)\n                    for edge in avoid_edges or []:  # Penalize banned edges\n                        wgt = (-1 if tree_optimisation else 1) * avoid_lambda / n_avoid\n                        penalty = edge.patch_mask(model)[edge.patch_idx]\n                        const = regularize_const if mask_fn == \"hard_concrete\" else 0.0\n                        if mask_fn is not None:\n                            penalty = t.sigmoid(penalty - const)\n                        regularize += wgt * penalty\n                    loss = faithful_term + regularize * regularize_lambda\n                    losses.append(loss.item())\n                    faiths.append(faithful_term.item())\n                    regularizes.append(regularize.item() * regularize_lambda)\n                    model.zero_grad()\n                    loss.backward()\n                    optim.step()\n\n                if validation_dataloader is not None:\n                    assert val_src_outs is not None and val_clean_logits is not None\n                    val_batch = next(iter(validation_dataloader))\n                    for validation_idx, validation_batch in enumerate(\n                        validation_dataloader\n                    ):\n                        if validation_idx == batch_idx:\n                            val_batch = validation_batch\n                    val_patch_outs = val_src_outs[val_batch.key].clone().detach()\n                    with patch_mode(model, val_patch_outs), t.no_grad():\n                        val_input_batch = (\n                            val_batch.clean if tree_optimisation else val_batch.corrupt\n                        )\n                        val_logits = model(val_input_batch)[out_slice]\n                        val_faithful_term = batch_answer_diff_percents(\n                            log_softmax(val_logits, dim=-1),\n                            val_clean_logits[val_batch.key],\n                            val_batch,\n                        )\n                        val_stds.append(t.std(val_faithful_term).item())\n                        val_faiths.append(val_faithful_term.mean().item())\n\n        xtreme_f = max if tree_optimisation else min\n        xtreme_torch_f = t.max if tree_optimisation else t.min\n        xtreme_val = abs(xtreme_f([xtreme_torch_f(msk).item() for msk in mask_params]))\n\n    if show_train_graph:\n        fig = go.Figure()\n        fig.add_trace(go.Scatter(y=losses, name=\"Loss\"))\n        fig.add_trace(go.Scatter(y=faiths, name=faithfulness_target.title()))\n        fig.add_trace(\n            go.Scatter(\n                y=val_faiths,\n                error_y=dict(type=\"data\", array=val_stds),\n                name=f\"Val {faithfulness_target.title()}\",\n            )\n        )\n        fig.add_trace(go.Scatter(y=regularizes, name=\"Regularization\"))\n        fig.update_layout(title=\"Subnetwork Probing\", xaxis_title=\"Step\")\n        fig.show()\n\n    sign = -1 if tree_optimisation else 1\n    prune_scores: PruneScores = {}\n    for mod_name, patch_mask in model.patch_masks.items():\n        prune_scores[mod_name] = xtreme_val + sign * patch_mask.detach().clone()\n    return prune_scores\n</code></pre>"},{"location":"reference/utils/ablation_activations/","title":"Ablation activations","text":""},{"location":"reference/utils/ablation_activations/#auto_circuit.utils.ablation_activations","title":"auto_circuit.utils.ablation_activations","text":""},{"location":"reference/utils/ablation_activations/#auto_circuit.utils.ablation_activations-attributes","title":"Attributes","text":""},{"location":"reference/utils/ablation_activations/#auto_circuit.utils.ablation_activations-classes","title":"Classes","text":""},{"location":"reference/utils/ablation_activations/#auto_circuit.utils.ablation_activations-functions","title":"Functions","text":""},{"location":"reference/utils/ablation_activations/#auto_circuit.utils.ablation_activations.batch_src_ablations","title":"batch_src_ablations","text":"<pre><code>batch_src_ablations(model: PatchableModel, dataloader: PromptDataLoader, ablation_type: AblationType = AblationType.RESAMPLE, clean_corrupt: Optional[Literal['clean', 'corrupt']] = None) -&gt; Dict[BatchKey, Tensor]\n</code></pre> <p>Wrapper of <code>src_ablations</code> that returns ablations for each batch in a dataloader.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>PatchableModel</code> <p>The model to get the ablations for.</p> required <code>dataloader</code> <code>PromptDataLoader</code> <p>The input data to get the ablations for.</p> required <code>ablation_type</code> <code>AblationType</code> <p>The type of ablation to perform.</p> <code>RESAMPLE</code> <code>clean_corrupt</code> <code>Optional[Literal['clean', 'corrupt']]</code> <p>Whether to use the clean or corrupt inputs to calculate the ablations.</p> <code>None</code> <p>Returns:</p> Type Description <code>Dict[BatchKey, Tensor]</code> <p>A dictionary mapping <code>BatchKey</code>s to the activations used to ablate each <code>Edge</code> in the model on the corresponding batch.</p> Source code in <code>auto_circuit/utils/ablation_activations.py</code> <pre><code>def batch_src_ablations(\n    model: PatchableModel,\n    dataloader: PromptDataLoader,\n    ablation_type: AblationType = AblationType.RESAMPLE,\n    clean_corrupt: Optional[Literal[\"clean\", \"corrupt\"]] = None,\n) -&gt; Dict[BatchKey, t.Tensor]:\n    \"\"\"\n    Wrapper of [`src_ablations`][auto_circuit.utils.ablation_activations.src_ablations]\n    that returns ablations for each batch in a dataloader.\n\n    Args:\n        model: The model to get the ablations for.\n        dataloader: The input data to get the ablations for.\n        ablation_type: The type of ablation to perform.\n        clean_corrupt: Whether to use the clean or corrupt inputs to calculate the\n            ablations.\n\n    Returns:\n        A dictionary mapping [`BatchKey`][auto_circuit.data.BatchKey]s to the\n            activations used to ablate each [`Edge`][auto_circuit.types.Edge] in the\n            model on the corresponding batch.\n    \"\"\"\n    batch_specific_ablation = [\n        AblationType.RESAMPLE,\n        AblationType.BATCH_TOKENWISE_MEAN,\n        AblationType.BATCH_ALL_TOK_MEAN,\n    ]\n    assert (clean_corrupt is not None) == (ablation_type in batch_specific_ablation)\n\n    patch_outs: Dict[BatchKey, t.Tensor] = {}\n    if ablation_type.mean_over_dataset:\n        mean_patch = src_ablations(model, dataloader, ablation_type)\n        patch_outs = {batch.key: mean_patch for batch in dataloader}\n    else:\n        for batch in dataloader:\n            if ablation_type == AblationType.ZERO:\n                input_batch = batch.clean\n            else:\n                input_batch = batch.clean if clean_corrupt == \"clean\" else batch.corrupt\n            patch_outs[batch.key] = src_ablations(model, input_batch, ablation_type)\n    return patch_outs\n</code></pre>"},{"location":"reference/utils/ablation_activations/#auto_circuit.utils.ablation_activations.src_ablations","title":"src_ablations","text":"<pre><code>src_ablations(model: PatchableModel, sample: Tensor | PromptDataLoader, ablation_type: AblationType = AblationType.RESAMPLE) -&gt; Tensor\n</code></pre> <p>Get the activations used to ablate each <code>Edge</code> in a model, given a particular set of model inputs and an ablation type. See <code>AblationType</code> for the different types of ablations that can be computed.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>PatchableModel</code> <p>The model to get the ablations for.</p> required <code>sample</code> <code>Tensor | PromptDataLoader</code> <p>The data sample to get the ablations for. This is not used for all <code>ablation_type</code>s. Either a single batch of inputs or a DataLoader.</p> required <code>ablation_type</code> <code>AblationType</code> <p>The type of ablation to perform.</p> <code>RESAMPLE</code> <p>Returns:</p> Type Description <code>Tensor</code> <p>A tensor of activations used to ablated each <code>Edge</code> model on the given input.  Shape is <code>[Srcs, ...]</code> where <code>Srcs</code> is the number of <code>SrcNode</code>s in the model and <code>...</code> is the shape of the activations of the model. In a transformer this will be <code>[Srcs, batch, seq, d_model]</code>.</p> Source code in <code>auto_circuit/utils/ablation_activations.py</code> <pre><code>def src_ablations(\n    model: PatchableModel,\n    sample: t.Tensor | PromptDataLoader,\n    ablation_type: AblationType = AblationType.RESAMPLE,\n) -&gt; t.Tensor:\n    \"\"\"\n    Get the activations used to ablate each [`Edge`][auto_circuit.types.Edge] in a\n    model, given a particular set of model inputs and an ablation type. See\n    [`AblationType`][auto_circuit.types.AblationType] for the different types of\n    ablations that can be computed.\n\n    Args:\n        model: The model to get the ablations for.\n        sample: The data sample to get the ablations for. This is not used for all\n            `ablation_type`s. Either a single batch of inputs or a DataLoader.\n        ablation_type: The type of ablation to perform.\n\n    Returns:\n        A tensor of activations used to ablated each [`Edge`][auto_circuit.types.Edge]\n            model on the given input.  Shape is `[Srcs, ...]` where `Srcs` is the number\n            of [`SrcNode`][auto_circuit.types.SrcNode]s in the model and `...` is the\n            shape of the activations of the model. In a transformer this will be\n            `[Srcs, batch, seq, d_model]`.\n    \"\"\"\n    src_outs: Dict[SrcNode, t.Tensor] = {}\n    src_modules: Dict[t.nn.Module, List[SrcNode]] = defaultdict(list)\n    [src_modules[src.module(model)].append(src) for src in model.srcs]\n    with remove_hooks() as handles, inference_mode():\n        # Install hooks to collect activations at each src module\n        for mod, src_nodes in src_modules.items():\n            hook_fn = partial(\n                mean_src_out_hook if ablation_type.mean_over_dataset else src_out_hook,\n                src_nodes=src_nodes,\n                src_outs=src_outs,\n                ablation_type=ablation_type,\n            )\n            handles.add(mod.register_forward_hook(hook_fn))\n\n        if ablation_type.mean_over_dataset:\n            # Collect activations over the entire dataset and take the mean\n            assert isinstance(sample, PromptDataLoader)\n            for batch in sample:\n                if ablation_type.clean_dataset:\n                    model(batch.clean)\n                if ablation_type.corrupt_dataset:\n                    model(batch.corrupt)\n            # PromptDataLoader has equal size batches, so we can take the mean of means\n            mult = int(ablation_type.clean_dataset) + int(ablation_type.corrupt_dataset)\n            assert mult == 2 or mult == 1\n            for src, src_out in src_outs.items():\n                src_outs[src] = src_out / (len(sample) * mult)\n        else:\n            # Collect activations for a single batch\n            assert isinstance(sample, t.Tensor)\n            model(sample)\n\n    # Sort the src_outs dict by node idx\n    src_outs = dict(sorted(src_outs.items(), key=lambda x: x[0].src_idx))\n    assert [src.src_idx for src in src_outs.keys()] == list(range(len(src_outs)))\n    return t.stack(list(src_outs.values())).detach()\n</code></pre>"},{"location":"reference/utils/custom_tqdm/","title":"Custom tqdm","text":""},{"location":"reference/utils/custom_tqdm/#auto_circuit.utils.custom_tqdm","title":"auto_circuit.utils.custom_tqdm","text":""},{"location":"reference/utils/custom_tqdm/#auto_circuit.utils.custom_tqdm-attributes","title":"Attributes","text":""},{"location":"reference/utils/custom_tqdm/#auto_circuit.utils.custom_tqdm.tqdm","title":"tqdm  <code>module-attribute</code>","text":"<pre><code>tqdm = partial(tqdm, dynamic_ncols=True, bar_format='{desc}{bar}{r_bar}', leave=None, delay=0)\n</code></pre> <p>Wrapper around <code>tqdm</code> with default settings for the project. Note that the <code>tqdm</code> dependency in this repo is my fork that fixes a rendering issue in Jupyter notebooks. I've made a pull request to the original repo but it hasn't been merged yet.</p>"},{"location":"reference/utils/graph_utils/","title":"Graph utils","text":""},{"location":"reference/utils/graph_utils/#auto_circuit.utils.graph_utils","title":"auto_circuit.utils.graph_utils","text":""},{"location":"reference/utils/graph_utils/#auto_circuit.utils.graph_utils-attributes","title":"Attributes","text":""},{"location":"reference/utils/graph_utils/#auto_circuit.utils.graph_utils-classes","title":"Classes","text":""},{"location":"reference/utils/graph_utils/#auto_circuit.utils.graph_utils-functions","title":"Functions","text":""},{"location":"reference/utils/graph_utils/#auto_circuit.utils.graph_utils.edge_counts_util","title":"edge_counts_util","text":"<pre><code>edge_counts_util(edges: Set[Edge], test_counts: Optional[TestEdges] = None, prune_scores: Optional[PruneScores] = None, zero_edges: Optional[bool] = None, all_edges: Optional[bool] = None, true_edge_count: Optional[int] = None) -&gt; List[int]\n</code></pre> <p>Calculate a set of [number of edges in the circuit] to test.</p> <p>Parameters:</p> Name Type Description Default <code>edges</code> <code>Set[Edge]</code> <p>The set of all edges in the model (just used to count the maximum circuit size).</p> required <code>test_counts</code> <code>Optional[TestEdges]</code> <p>The method to determine the set of edge counts. If None, the function will try to infer the best method based on the number of edges and the <code>prune_scores</code>. See <code>TestEdges</code> and <code>EdgeCounts</code> for full details.</p> <code>None</code> <code>prune_scores</code> <code>Optional[PruneScores]</code> <p>The scores to use to determine the edge counts. Used to make a better inference of the best set to use when <code>test_counts</code> is None. Also used when <code>test_counts</code> is <code>EdgeCounts.GROUPS</code> to group the edges by their scores.</p> <code>None</code> <code>zero_edges</code> <code>Optional[bool]</code> <p>Whether to include <code>0</code> edges.</p> <code>None</code> <code>all_edges</code> <code>Optional[bool]</code> <p>Whether to include <code>n_edges</code> edges (where <code>n_edges</code> is the number of edges in the model).</p> <code>None</code> <code>true_edge_count</code> <code>Optional[int]</code> <p>Inserts an extra specified edge count into the list. Useful when you want to test the number of edges in the candidate circuit.</p> <code>None</code> <p>Returns:</p> Type Description <code>List[int]</code> <p>The list of edge counts to test.</p> Source code in <code>auto_circuit/utils/graph_utils.py</code> <pre><code>def edge_counts_util(\n    edges: Set[Edge],\n    test_counts: Optional[TestEdges] = None,  # None means default\n    prune_scores: Optional[PruneScores] = None,\n    zero_edges: Optional[bool] = None,  # None means default\n    all_edges: Optional[bool] = None,  # None means default\n    true_edge_count: Optional[int] = None,\n) -&gt; List[int]:\n    \"\"\"\n    Calculate a set of [number of edges in the circuit] to test.\n\n    Args:\n        edges: The set of all edges in the model (just used to count the maximum circuit\n            size).\n        test_counts: The method to determine the set of edge counts. If None, the\n            function will try to infer the best method based on the number of edges and\n            the `prune_scores`. See [`TestEdges`][auto_circuit.types.TestEdges] and\n            [`EdgeCounts`][auto_circuit.types.EdgeCounts] for full details.\n        prune_scores: The scores to use to determine the edge counts. Used to make a\n            better inference of the best set to use when `test_counts` is None. Also\n            used when `test_counts` is `EdgeCounts.GROUPS` to group the edges by their\n            scores.\n        zero_edges: Whether to include `0` edges.\n        all_edges: Whether to include `n_edges` edges (where `n_edges` is the number of\n            edges in the model).\n        true_edge_count: Inserts an extra specified edge count into the list. Useful\n            when you want to test the number of edges in the candidate circuit.\n\n    Returns:\n        The list of edge counts to test.\n    \"\"\"\n    n_edges = len(edges)\n\n    # Work out default setting for test_counts\n    sorted_ps_count: Optional[t.Tensor] = None\n    if test_counts is None:\n        test_counts = EdgeCounts.LOGARITHMIC if n_edges &gt; 200 else EdgeCounts.ALL\n        if prune_scores is not None:\n            flat_ps = desc_prune_scores(prune_scores)\n            unique_ps, sorted_ps_count = flat_ps.unique(sorted=True, return_counts=True)\n            if list(unique_ps.size())[0] &lt; min(n_edges / 2, 100):\n                test_counts = EdgeCounts.GROUPS\n\n    # Calculate the test counts\n    if test_counts == EdgeCounts.ALL:\n        counts_list = [n for n in range(n_edges + 1)]\n    elif test_counts == EdgeCounts.LOGARITHMIC:\n        counts_list = [\n            n\n            for n in range(1, n_edges)\n            # if n % (10 ** max(math.floor(math.log10(n)) - 1, 0)) == 0\n            if n % (10 ** max(math.floor(math.log10(n)), 0)) == 0\n        ]\n    elif test_counts == EdgeCounts.GROUPS:\n        assert prune_scores is not None\n        if sorted_ps_count is None:\n            flat_ps = desc_prune_scores(prune_scores)\n            _, sorted_ps_count = flat_ps.unique(sorted=True, return_counts=True)\n        assert sorted_ps_count is not None\n        counts_list = sorted_ps_count.flip(dims=(0,)).cumsum(dim=0).tolist()\n    elif isinstance(test_counts, List):\n        counts_list = [n if type(n) == int else int(n_edges * n) for n in test_counts]\n    else:\n        raise NotImplementedError(f\"Unknown test_counts: {test_counts}\")\n\n    # Choose default. If len(count_lists) &lt;= 2, this is likely a binary circuit encoding\n    if zero_edges is None:\n        zero_edges = True if len(counts_list) &gt; 2 else False\n    if all_edges is None:\n        all_edges = True if len(counts_list) &gt; 2 else False\n\n    # Add zero and all edges if necessary\n    if zero_edges and 0 not in counts_list:\n        counts_list = [0] + counts_list\n    if all_edges and n_edges not in counts_list:\n        counts_list.append(n_edges)\n    if not zero_edges and 0 in counts_list:\n        counts_list.remove(0)\n    if not all_edges and n_edges in counts_list:\n        counts_list.remove(n_edges)\n    # Insert true_edge_count at the correct position\n    if true_edge_count is not None and true_edge_count not in counts_list:\n        counts_list.append(true_edge_count)\n    counts_list.sort()\n\n    return counts_list\n</code></pre>"},{"location":"reference/utils/graph_utils/#auto_circuit.utils.graph_utils.graph_edges","title":"graph_edges","text":"<pre><code>graph_edges(model: Module, factorized: bool, separate_qkv: Optional[bool] = None, seq_len: Optional[int] = None) -&gt; Tuple[Set[Node], Set[SrcNode], Set[DestNode], Dict[int | None, List[Edge]], Set[Edge], int, Optional[int]]\n</code></pre> <p>Get the nodes and edges of the computation graph of the model used for ablation.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>Module</code> <p>The model to get the edges for.</p> required <code>factorized</code> <code>bool</code> <p>Whether the model is factorized, for Edge Ablation. Otherwise, only Node Ablation is possible.</p> required <code>separate_qkv</code> <code>Optional[bool]</code> <p>Whether the model has separate query, key, and value inputs. Only used for transformers.</p> <code>None</code> <code>seq_len</code> <code>Optional[int]</code> <p>The sequence length of the model inputs. If <code>None</code>, all token positions are simultaneously ablated.</p> <code>None</code> <p>Returns:</p> Type Description <code>Tuple[Set[Node], Set[SrcNode], Set[DestNode], Dict[int | None, List[Edge]], Set[Edge], int, Optional[int]]</code> <p>Tuple containing:</p> <ol> <li>The set of all nodes in the model.</li> <li>The set of all source nodes in the model.</li> <li>The set of all destination nodes in the model.</li> <li>A dictionary mapping sequence positions to the edges at that         position.</li> <li>The set of all edges in the model.</li> <li>The sequence dimension of the model. This is the dimension on which         new inputs are concatenated. For transformers, this is         <code>1</code> because the activations are of shape         <code>[batch_size, seq_len, hidden_dim]</code>.     <li>The sequence length of the model inputs.</li> Source code in <code>auto_circuit/utils/graph_utils.py</code> <pre><code>def graph_edges(\n    model: t.nn.Module,\n    factorized: bool,\n    separate_qkv: Optional[bool] = None,\n    seq_len: Optional[int] = None,\n) -&gt; Tuple[\n    Set[Node],\n    Set[SrcNode],\n    Set[DestNode],\n    Dict[int | None, List[Edge]],\n    Set[Edge],\n    int,\n    Optional[int],\n]:\n    \"\"\"\n    Get the nodes and edges of the computation graph of the model used for ablation.\n\n    Args:\n        model: The model to get the edges for.\n        factorized: Whether the model is factorized, for Edge Ablation. Otherwise,\n            only Node Ablation is possible.\n        separate_qkv: Whether the model has separate query, key, and value inputs. Only\n            used for transformers.\n        seq_len: The sequence length of the model inputs. If `None`, all token positions\n            are simultaneously ablated.\n\n    Returns:\n        Tuple containing:\n            &lt;ol&gt;\n                &lt;li&gt;The set of all nodes in the model.&lt;/li&gt;\n                &lt;li&gt;The set of all source nodes in the model.&lt;/li&gt;\n                &lt;li&gt;The set of all destination nodes in the model.&lt;/li&gt;\n                &lt;li&gt;A dictionary mapping sequence positions to the edges at that\n                    position.&lt;/li&gt;\n                &lt;li&gt;The set of all edges in the model.&lt;/li&gt;\n                &lt;li&gt;The sequence dimension of the model. This is the dimension on which\n                    new inputs are concatenated. For transformers, this is\n                    &lt;code&gt;1&lt;/code&gt; because the activations are of shape\n                    &lt;code&gt;[batch_size, seq_len, hidden_dim]&lt;/code&gt;.\n                &lt;li&gt;The sequence length of the model inputs.&lt;/li&gt;\n            &lt;/ol&gt;\n    \"\"\"\n    seq_dim = 1\n    edge_dict: Dict[Optional[int], List[Edge]] = defaultdict(list)\n    if not factorized:\n        if isinstance(model, MicroModel):\n            srcs, dests = mm_utils.simple_graph_nodes(model)\n        elif isinstance(model, HookedTransformer):\n            srcs, dests = tl_utils.simple_graph_nodes(model)\n        else:\n            raise NotImplementedError(model)\n        for i in [None] if seq_len is None else range(seq_len):\n            pairs = product(srcs, dests)\n            edge_dict[i] = [Edge(s, d, i) for s, d in pairs if s.layer + 1 == d.layer]\n    else:\n        if isinstance(model, MicroModel):\n            srcs: Set[SrcNode] = mm_utils.factorized_src_nodes(model)\n            dests: Set[DestNode] = mm_utils.factorized_dest_nodes(model)\n        elif isinstance(model, HookedTransformer):\n            assert separate_qkv is not None, \"separate_qkv must be specified for LLM\"\n            srcs: Set[SrcNode] = tl_utils.factorized_src_nodes(model)\n            dests: Set[DestNode] = tl_utils.factorized_dest_nodes(model, separate_qkv)\n        elif isinstance(model, AutoencoderTransformer):\n            assert separate_qkv is not None, \"separate_qkv must be specified for LLM\"\n            srcs: Set[SrcNode] = sae_utils.factorized_src_nodes(model)\n            dests: Set[DestNode] = sae_utils.factorized_dest_nodes(model, separate_qkv)\n        else:\n            raise NotImplementedError(model)\n        for i in [None] if seq_len is None else range(seq_len):\n            pairs = product(srcs, dests)\n            edge_dict[i] = [Edge(s, d, i) for s, d in pairs if s.layer &lt; d.layer]\n    nodes: Set[Node] = set(srcs | dests)\n    edges = set(list(chain.from_iterable(edge_dict.values())))\n\n    return nodes, srcs, dests, edge_dict, edges, seq_dim, seq_len\n</code></pre>"},{"location":"reference/utils/graph_utils/#auto_circuit.utils.graph_utils.make_model_patchable","title":"make_model_patchable","text":"<pre><code>make_model_patchable(model: Module, factorized: bool, src_nodes: Set[SrcNode], nodes: Set[Node], device: device, seq_len: Optional[int] = None, seq_dim: Optional[int] = None) -&gt; Tuple[Set[PatchWrapperImpl], Set[PatchWrapperImpl], Set[PatchWrapperImpl]]\n</code></pre> <p>Injects <code>PatchWrapper</code>s into the model at the node positions to enable patching.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>Module</code> <p>The model to make patchable.</p> required <code>factorized</code> <code>bool</code> <p>Whether the model is factorized, for Edge Ablation. Otherwise, only Node Ablation is possible.</p> required <code>src_nodes</code> <code>Set[SrcNode]</code> <p>The source nodes in the model.</p> required <code>nodes</code> <code>Set[Node]</code> <p>All the nodes in the model.</p> required <code>device</code> <code>device</code> <p>The device to put the patch masks on.</p> required <code>seq_len</code> <code>Optional[int]</code> <p>The sequence length of the model inputs. If <code>None</code>, all token positions are simultaneously ablated.</p> <code>None</code> <code>seq_dim</code> <code>Optional[int]</code> <p>The sequence dimension of the model. This is the dimension on which new inputs are concatenated. In transformers, this is <code>1</code> because the activations are of shape <code>[batch_size, seq_len, hidden_dim]</code>.</p> <code>None</code> <p>Returns:</p> Type Description <code>Tuple[Set[PatchWrapperImpl], Set[PatchWrapperImpl], Set[PatchWrapperImpl]]</code> <p>Tuple containing:</p> <ol> <li>The set of all PatchWrapper modules in the model.</li> <li>The set of all PatchWrapper modules that wrap source nodes.</li> <li>The set of all PatchWrapper modules that wrap destination         nodes.</li> </ol> Warning <p>This function modifies the model in place.</p> Source code in <code>auto_circuit/utils/graph_utils.py</code> <pre><code>def make_model_patchable(\n    model: t.nn.Module,\n    factorized: bool,\n    src_nodes: Set[SrcNode],\n    nodes: Set[Node],\n    device: t.device,\n    seq_len: Optional[int] = None,\n    seq_dim: Optional[int] = None,\n) -&gt; Tuple[Set[PatchWrapperImpl], Set[PatchWrapperImpl], Set[PatchWrapperImpl]]:\n    \"\"\"\n    Injects [`PatchWrapper`][auto_circuit.types.PatchWrapper]s into the model at the\n    node positions to enable patching.\n\n    Args:\n        model: The model to make patchable.\n        factorized: Whether the model is factorized, for Edge Ablation. Otherwise,\n            only Node Ablation is possible.\n        src_nodes: The source nodes in the model.\n        nodes: All the nodes in the model.\n        device: The device to put the patch masks on.\n        seq_len: The sequence length of the model inputs. If `None`, all token positions\n            are simultaneously ablated.\n        seq_dim: The sequence dimension of the model. This is the dimension on which new\n            inputs are concatenated. In transformers, this is `1` because the\n            activations are of shape `[batch_size, seq_len, hidden_dim]`.\n\n    Returns:\n        Tuple containing:\n            &lt;ol&gt;\n                &lt;li&gt;The set of all PatchWrapper modules in the model.&lt;/li&gt;\n                &lt;li&gt;The set of all PatchWrapper modules that wrap source nodes.&lt;/li&gt;\n                &lt;li&gt;The set of all PatchWrapper modules that wrap destination\n                    nodes.&lt;/li&gt;\n            &lt;/ol&gt;\n\n    Warning:\n        This function modifies the model in place.\n    \"\"\"\n    node_dict: Dict[str, Set[Node]] = defaultdict(set)\n    [node_dict[node.module_name].add(node) for node in nodes]\n    wrappers, src_wrappers, dest_wrappers = set(), set(), set()\n    dtype = next(model.parameters()).dtype\n\n    for module_name, module_nodes in node_dict.items():\n        module = module_by_name(model, module_name)\n        src_idxs_slice = None\n        a_node = next(iter(module_nodes))\n        head_dim = a_node.head_dim\n        assert all([node.head_dim == head_dim for node in module_nodes])\n\n        if is_src := any([type(node) == SrcNode for node in module_nodes]):\n            src_idxs = [n.src_idx for n in module_nodes if type(n) == SrcNode]\n            src_idxs_slice = slice(min(src_idxs), max(src_idxs) + 1)\n            assert src_idxs_slice.stop - src_idxs_slice.start == len(src_idxs)\n\n        mask, in_srcs = None, None\n        if is_dest := any([type(node) == DestNode for node in module_nodes]):\n            module_dest_count = len([n for n in module_nodes if type(n) == DestNode])\n            if factorized:\n                n_in_src = len([n for n in src_nodes if n.layer &lt; a_node.layer])\n                n_ignore_src = 0\n            else:\n                n_in_src = len([n for n in src_nodes if n.layer + 1 == a_node.layer])\n                n_ignore_src = len([n for n in src_nodes if n.layer + 1 &lt; a_node.layer])\n            in_srcs = slice(n_ignore_src, n_ignore_src + n_in_src)\n            seq_shape = [seq_len] if seq_len is not None else []\n            head_shape = [module_dest_count] if head_dim is not None else []\n            mask_shape = seq_shape + head_shape + [n_in_src]\n            mask = t.zeros(mask_shape, device=device, dtype=dtype, requires_grad=False)\n\n        wrapper = PatchWrapperImpl(\n            module_name=module_name,\n            module=module,\n            head_dim=head_dim,\n            seq_dim=None if seq_len is None else seq_dim,  # Patch tokens separately\n            is_src=is_src,\n            src_idxs=src_idxs_slice,\n            is_dest=is_dest,\n            patch_mask=mask,\n            in_srcs=in_srcs,\n        )\n        set_module_by_name(model, module_name, wrapper)\n        wrappers.add(wrapper)\n        src_wrappers.add(wrapper) if is_src else None\n        dest_wrappers.add(wrapper) if is_dest else None\n\n    return wrappers, src_wrappers, dest_wrappers\n</code></pre>"},{"location":"reference/utils/graph_utils/#auto_circuit.utils.graph_utils.mask_fn_mode","title":"mask_fn_mode","text":"<pre><code>mask_fn_mode(model: PatchableModel, mask_fn: MaskFn, dropout_p: float = 0.0)\n</code></pre> <p>Context manager to enable the specified <code>mask_fn</code> and <code>dropout_p</code> for a patchable model.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>PatchableModel</code> <p>The patchable model to alter.</p> required <code>mask_fn</code> <code>MaskFn</code> <p>The function to apply to the mask values before they are used to interpolate between the clean and ablated activations.</p> required <code>dropout_p</code> <code>float</code> <p>The dropout probability to apply to the mask values.</p> <code>0.0</code> Warning <p>This function modifies the state of the model! This is a likely source of bugs.</p> Source code in <code>auto_circuit/utils/graph_utils.py</code> <pre><code>@contextmanager\ndef mask_fn_mode(model: PatchableModel, mask_fn: MaskFn, dropout_p: float = 0.0):\n    \"\"\"\n    Context manager to enable the specified `mask_fn` and `dropout_p` for a patchable\n    model.\n\n    Args:\n        model: The patchable model to alter.\n        mask_fn: The function to apply to the mask values before they are used to\n            interpolate between the clean and ablated activations.\n        dropout_p: The dropout probability to apply to the mask values.\n\n    Warning:\n        This function modifies the state of the model! This is a likely source of bugs.\n    \"\"\"\n    for wrapper in model.dest_wrappers:\n        wrapper.mask_fn = mask_fn\n        wrapper.dropout_layer.p = dropout_p  # type: ignore\n    try:\n        yield\n    finally:\n        for wrapper in model.dest_wrappers:\n            wrapper.mask_fn = None\n            wrapper.dropout_layer.p = 0.0  # type: ignore\n</code></pre>"},{"location":"reference/utils/graph_utils/#auto_circuit.utils.graph_utils.patch_mode","title":"patch_mode","text":"<pre><code>patch_mode(model: PatchableModel, patch_src_outs: Tensor, edges: Optional[Collection[str | Edge]] = None, curr_src_outs: Optional[Tensor] = None)\n</code></pre> <p>Context manager to enable patching in the model.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>PatchableModel</code> <p>The patchable model to alter.</p> required <code>patch_src_outs</code> <code>Tensor</code> <p>The activations with which to ablate the model. Mask values interpolate the edge activations between the default activations (<code>0</code>) and these activations (<code>1</code>).</p> required <code>edges</code> <code>Optional[Collection[str | Edge]]</code> <p>A collection of edges to patch. The corresponding patch mask elements will be set to <code>1.0</code> and all other mask elements are set to <code>0.0</code>. If <code>None</code>, masks are not modified.</p> <code>None</code> <code>curr_src_outs</code> <code>Tensor</code> <p>Stores the outputs of each src node during the current forward pass. The only time this need to be initialized is when you are starting the forward pass at a middle layer because the outputs of previous <code>SrcNode</code>s won't be cached automatically (used in ACDC, as a performance optimization).</p> <code>None</code> Warning <p>This function modifies the state of the model! This is a likely source of bugs.</p> Source code in <code>auto_circuit/utils/graph_utils.py</code> <pre><code>@contextmanager\ndef patch_mode(\n    model: PatchableModel,\n    patch_src_outs: t.Tensor,\n    edges: Optional[Collection[str | Edge]] = None,\n    curr_src_outs: Optional[t.Tensor] = None,\n):\n    \"\"\"\n    Context manager to enable patching in the model.\n\n    Args:\n        model: The patchable model to alter.\n        patch_src_outs: The activations with which to ablate the model. Mask values\n            interpolate the edge activations between the default activations (`0`) and\n            these activations (`1`).\n        edges: A collection of edges to patch. The corresponding patch mask elements\n            will be set to `1.0` and all other mask elements are set to `0.0`.\n            If `None`, masks are not modified.\n        curr_src_outs (t.Tensor, optional): Stores the outputs of each src node during\n            the current forward pass. The only time this need to be initialized is when\n            you are starting the forward pass at a middle layer because the outputs of\n            previous\n            [`SrcNode`][auto_circuit.types.SrcNode]s won't be cached automatically (used\n            in ACDC, as a performance optimization).\n\n    Warning:\n        This function modifies the state of the model! This is a likely source of bugs.\n    \"\"\"\n    if curr_src_outs is None:\n        curr_src_outs = t.zeros_like(patch_src_outs)\n\n    # TODO: Raise an error if one of the edge names doesn't exist.\n    if edges is not None:\n        set_all_masks(model, val=0.0)\n        for edge in model.edges:\n            if edge in edges or edge.name in edges:\n                edge.patch_mask(model).data[edge.patch_idx] = 1.0\n\n    for wrapper in model.wrappers:\n        wrapper.patch_mode = True\n        wrapper.curr_src_outs = curr_src_outs\n        if wrapper.is_dest:\n            wrapper.patch_src_outs = patch_src_outs\n    try:\n        yield\n    finally:\n        for wrapper in model.wrappers:\n            wrapper.patch_mode = False\n            wrapper.curr_src_outs = None\n            if wrapper.is_dest:\n                wrapper.patch_src_outs = None\n        del curr_src_outs, patch_src_outs\n</code></pre>"},{"location":"reference/utils/graph_utils/#auto_circuit.utils.graph_utils.patchable_model","title":"patchable_model","text":"<pre><code>patchable_model(model: Module, factorized: bool, slice_output: OutputSlice = None, seq_len: Optional[int] = None, separate_qkv: Optional[bool] = None, kv_caches: Tuple[Optional[HookedTransformerKeyValueCache], ...] = (None), device: device = t.device('cpu')) -&gt; PatchableModel\n</code></pre> <p>Wrap a model and inject <code>PatchWrapper</code>s into the node modules to enable patching.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>Module</code> <p>The model to make patchable.</p> required <code>factorized</code> <code>bool</code> <p>Whether the model is factorized, for Edge Ablation. Otherwise, only Node Ablation is possible.</p> required <code>slice_output</code> <code>OutputSlice</code> <p>Specifies the index/slice of the output of the model to be considered for the task. For example, <code>\"last_seq\"</code> will consider the last token's output in transformer models.</p> <code>None</code> <code>seq_len</code> <code>Optional[int]</code> <p>The sequence length of the model inputs. If <code>None</code>, all token positions are simultaneously ablated.</p> <code>None</code> <code>separate_qkv</code> <code>Optional[bool]</code> <p>Whether the model has separate query, key, and value inputs. Only used for transformers.</p> <code>None</code> <code>kv_caches</code> <code>Tuple[Optional[HookedTransformerKeyValueCache], ...]</code> <p>The key and value caches for the transformer. Only used for transformers.</p> <code>(None)</code> <code>device</code> <code>device</code> <p>The device that the model is on.</p> <code>device('cpu')</code> <p>Returns:</p> Type Description <code>PatchableModel</code> <p>The patchable model.</p> Warning <p>This function modifies the model, it does not return a new model.</p> Source code in <code>auto_circuit/utils/graph_utils.py</code> <pre><code>def patchable_model(\n    model: t.nn.Module,\n    factorized: bool,\n    slice_output: OutputSlice = None,\n    seq_len: Optional[int] = None,\n    separate_qkv: Optional[bool] = None,\n    kv_caches: Tuple[Optional[HookedTransformerKeyValueCache], ...] = (None,),\n    device: t.device = t.device(\"cpu\"),\n) -&gt; PatchableModel:\n    \"\"\"\n    Wrap a model and inject [`PatchWrapper`][auto_circuit.types.PatchWrapper]s into the\n    node modules to enable patching.\n\n    Args:\n        model: The model to make patchable.\n        factorized: Whether the model is factorized, for Edge Ablation. Otherwise,\n            only Node Ablation is possible.\n        slice_output: Specifies the index/slice of the output of the model to be\n            considered for the task. For example, `\"last_seq\"` will consider the last\n            token's output in transformer models.\n        seq_len: The sequence length of the model inputs. If `None`, all token positions\n            are simultaneously ablated.\n        separate_qkv: Whether the model has separate query, key, and value inputs. Only\n            used for transformers.\n        kv_caches: The key and value caches for the transformer. Only used for\n            transformers.\n        device: The device that the model is on.\n\n    Returns:\n        The patchable model.\n\n    Warning:\n        This function modifies the model, it does not return a new model.\n    \"\"\"\n    assert not isinstance(model, PatchableModel), \"Model is already patchable\"\n    nodes, srcs, dests, edge_dict, edges, seq_dim, seq_len = graph_edges(\n        model, factorized, separate_qkv, seq_len\n    )\n    wrappers, src_wrappers, dest_wrappers = make_model_patchable(\n        model, factorized, srcs, nodes, device, seq_len, seq_dim\n    )\n    if slice_output is None:\n        out_slice: Tuple[slice | int, ...] = (slice(None),)\n    else:\n        last_slice = [-1] if slice_output == \"last_seq\" else [slice(1, None)]\n        out_slice: Tuple[slice | int, ...] = tuple([slice(None)] * seq_dim + last_slice)\n    is_tl_transformer = isinstance(model, HookedTransformer)\n    is_autoencoder_transformer = isinstance(model, AutoencoderTransformer)\n    is_transformer = is_tl_transformer or is_autoencoder_transformer\n    return PatchableModel(\n        nodes=nodes,\n        srcs=srcs,\n        dests=dests,\n        edge_dict=edge_dict,\n        edges=edges,\n        seq_dim=seq_dim,\n        seq_len=seq_len,\n        wrappers=wrappers,\n        src_wrappers=src_wrappers,\n        dest_wrappers=dest_wrappers,\n        out_slice=out_slice,\n        is_factorized=factorized,\n        is_transformer=is_transformer,\n        separate_qkv=separate_qkv,\n        kv_caches=kv_caches,\n        wrapped_model=model,\n    )\n</code></pre>"},{"location":"reference/utils/graph_utils/#auto_circuit.utils.graph_utils.set_all_masks","title":"set_all_masks","text":"<pre><code>set_all_masks(model: PatchableModel, val: float) -&gt; None\n</code></pre> <p>Set all the patch masks in the model to the specified value.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>PatchableModel</code> <p>The patchable model to alter.</p> required <code>val</code> <code>float</code> <p>The value to set the patch masks to.</p> required Warning <p>This function modifies the state of the model! This is a likely source of bugs.</p> Source code in <code>auto_circuit/utils/graph_utils.py</code> <pre><code>def set_all_masks(model: PatchableModel, val: float) -&gt; None:\n    \"\"\"\n    Set all the patch masks in the model to the specified value.\n\n    Args:\n        model: The patchable model to alter.\n        val: The value to set the patch masks to.\n\n    Warning:\n        This function modifies the state of the model! This is a likely source of bugs.\n    \"\"\"\n    for wrapper in model.wrappers:\n        if wrapper.is_dest:\n            t.nn.init.constant_(wrapper.patch_mask, val)\n</code></pre>"},{"location":"reference/utils/graph_utils/#auto_circuit.utils.graph_utils.set_mask_batch_size","title":"set_mask_batch_size","text":"<pre><code>set_mask_batch_size(model: PatchableModel, batch_size: int | None)\n</code></pre> <p>Context manager to set the batch size of the patch masks in the model.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>PatchableModel</code> <p>The patchable model to alter.</p> required <code>batch_size</code> <code>int | None</code> <p>The batch size to set the patch masks to. If <code>None</code>, the batch size is not modified.</p> required Warning <p>This function breaks other functions of the library while the context is active and should be considered an experimental feature. This function modifies the state of the model! This is a likely source of bugs.</p> Source code in <code>auto_circuit/utils/graph_utils.py</code> <pre><code>@contextmanager\ndef set_mask_batch_size(model: PatchableModel, batch_size: int | None):\n    \"\"\"\n    Context manager to set the batch size of the patch masks in the model.\n\n    Args:\n        model: The patchable model to alter.\n        batch_size: The batch size to set the patch masks to. If `None`, the batch size\n            is not modified.\n\n    Warning:\n        This function breaks other functions of the library while the context is active\n        and should be considered an experimental feature.\n        This function modifies the state of the model! This is a likely source of bugs.\n    \"\"\"\n    for wrapper in model.dest_wrappers:\n        wrapper.set_mask_batch_size(batch_size)\n    try:\n        yield\n    finally:\n        for wrapper in model.dest_wrappers:\n            wrapper.set_mask_batch_size(None)\n</code></pre>"},{"location":"reference/utils/graph_utils/#auto_circuit.utils.graph_utils.train_mask_mode","title":"train_mask_mode","text":"<pre><code>train_mask_mode(model: PatchableModel, requires_grad: bool = True) -&gt; Iterator[Dict[str, Parameter]]\n</code></pre> <p>Context manager that sets the <code>requires_grad</code> attribute of the patch masks for the duration of the context and yields the parameters.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>PatchableModel</code> <p>The patchable model to alter.</p> required <code>requires_grad</code> <code>bool</code> <p>Whether to enable gradient tracking on the patch masks.</p> <code>True</code> <p>Yields:</p> Type Description <code>Dict[str, Parameter]</code> <p>The patch mask <code>Parameter</code>s of the model as a dictionary with the module name as</p> <code>Dict[str, Parameter]</code> <p>the key.</p> Warning <p>This function modifies the state of the model! This is a likely source of bugs.</p> Source code in <code>auto_circuit/utils/graph_utils.py</code> <pre><code>@contextmanager\ndef train_mask_mode(\n    model: PatchableModel, requires_grad: bool = True\n) -&gt; Iterator[Dict[str, t.nn.Parameter]]:\n    \"\"\"\n    Context manager that sets the `requires_grad` attribute of the patch masks for the\n    duration of the context and yields the parameters.\n\n    Args:\n        model: The patchable model to alter.\n        requires_grad: Whether to enable gradient tracking on the patch masks.\n\n    Yields:\n        The patch mask `Parameter`s of the model as a dictionary with the module name as\n        the key.\n\n    Warning:\n        This function modifies the state of the model! This is a likely source of bugs.\n    \"\"\"\n    model.eval()\n    model.zero_grad()\n    parameters: Dict[str, t.nn.Parameter] = {}\n    for wrapper in model.dest_wrappers:\n        patch_mask = wrapper.patch_mask\n        patch_mask.detach_().requires_grad_(requires_grad)\n        parameters[wrapper.module_name] = patch_mask\n        wrapper.train()\n    try:\n        yield parameters\n    finally:\n        for wrapper in model.dest_wrappers:\n            wrapper.eval()\n            wrapper.patch_mask.detach_().requires_grad_(False)\n</code></pre>"},{"location":"reference/utils/graph_utils/#auto_circuit.utils.graph_utils-modules","title":"Modules","text":""},{"location":"reference/utils/misc/","title":"Misc","text":""},{"location":"reference/utils/misc/#auto_circuit.utils.misc","title":"auto_circuit.utils.misc","text":""},{"location":"reference/utils/misc/#auto_circuit.utils.misc-functions","title":"Functions","text":""},{"location":"reference/utils/misc/#auto_circuit.utils.misc.get_most_similar_embeddings","title":"get_most_similar_embeddings","text":"<pre><code>get_most_similar_embeddings(model: Module, out: Tensor, answer: Optional[str] = None, top_k: int = 10, apply_ln_final: bool = False, apply_unembed: bool = False, apply_embed: bool = False)\n</code></pre> <p>Helper function to print the top <code>top_k</code> most similar embeddings to a given vector. Can be used for either embeddings or unembeddings.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>Module</code> <p>The model to get the embeddings from.</p> required <code>out</code> <code>Tensor</code> <p>The vector to get the most similar embeddings to.</p> required <code>answer</code> <code>Optional[str]</code> <p>Token to show the rank of in the output logits.</p> <code>None</code> <code>top_k</code> <code>int</code> <p>The number of top output logits to show.</p> <code>10</code> <code>apply_ln_final</code> <code>bool</code> <p>Whether to apply the final layer normalization to the vector before getting the most similar embeddings.</p> <code>False</code> <code>apply_unembed</code> <code>bool</code> <p>If <code>True</code>, compare to the unembeddings.</p> <code>False</code> <code>apply_embed</code> <code>bool</code> <p>If <code>True</code>, compare to the embeddings.</p> <code>False</code> Source code in <code>auto_circuit/utils/misc.py</code> <pre><code>def get_most_similar_embeddings(\n    model: t.nn.Module,\n    out: t.Tensor,\n    answer: Optional[str] = None,\n    top_k: int = 10,\n    apply_ln_final: bool = False,\n    apply_unembed: bool = False,\n    apply_embed: bool = False,\n):\n    \"\"\"\n    Helper function to print the top `top_k` most similar embeddings to a given vector.\n    Can be used for either embeddings or unembeddings.\n\n    Args:\n        model: The model to get the embeddings from.\n        out: The vector to get the most similar embeddings to.\n        answer: Token to show the rank of in the output logits.\n        top_k: The number of top output logits to show.\n        apply_ln_final: Whether to apply the final layer normalization to the vector\n            before getting the most similar embeddings.\n        apply_unembed: If `True`, compare to the unembeddings.\n        apply_embed: If `True`, compare to the embeddings.\n    \"\"\"\n    assert not (apply_embed and apply_unembed), \"Can't apply both embed and unembed\"\n    show_answer_rank = answer is not None\n    answer = \" cheese\" if answer is None else answer\n    out = out.unsqueeze(0).unsqueeze(0) if out.ndim == 1 else out\n    out = model.ln_final(out) if apply_ln_final else out\n    if apply_embed:\n        unembeded = einsum(\n            out, model.embed.W_E, \"batch pos d_model, vocab d_model -&gt; batch pos vocab\"\n        )\n    elif apply_unembed:\n        unembeded = model.unembed(out)\n    else:\n        unembeded = out\n    answer_token = model.to_tokens(answer, prepend_bos=False).squeeze()\n    answer_str_token = model.to_str_tokens(answer, prepend_bos=False)\n    assert len(answer_str_token) == 1\n    logits = unembeded.squeeze()  # type: ignore\n    probs = logits.softmax(dim=-1)\n\n    sorted_token_probs, sorted_token_values = probs.sort(descending=True)\n    # Janky way to get the index of the token in the sorted list\n    if answer is not None:\n        correct_rank = t.arange(len(sorted_token_values))[\n            (sorted_token_values == answer_token).cpu()\n        ].item()\n    else:\n        correct_rank = -1\n    if show_answer_rank:\n        print(\n            f'\\n\"{answer_str_token[0]}\" token rank:',\n            f\"{correct_rank: &lt;8}\",\n            f\"\\nLogit: {logits[answer_token].item():5.2f}\",\n            f\"Prob: {probs[answer_token].item():6.2%}\",\n        )\n    for i in range(top_k):\n        print(\n            f\"Top {i}th token. Logit: {logits[sorted_token_values[i]].item():5.2f}\",\n            f\"Prob: {sorted_token_probs[i].item():6.2%}\",\n            f'Token: \"{model.to_string(sorted_token_values[i])}\"',\n        )\n</code></pre>"},{"location":"reference/utils/misc/#auto_circuit.utils.misc.load_cache","title":"load_cache","text":"<pre><code>load_cache(folder_name: str, filename: str) -&gt; Dict[Any, Any]\n</code></pre> <p>Load a dictionary from a cache file.</p> <p>Parameters:</p> Name Type Description Default <code>folder_name</code> <code>str</code> <p>The name of the folder to load the cache from.</p> required <code>filename</code> <code>str</code> <p>The name of the file to load the cache from.</p> required <p>Returns:</p> Type Description <code>Dict[Any, Any]</code> <p>The loaded dictionary.</p> Source code in <code>auto_circuit/utils/misc.py</code> <pre><code>def load_cache(folder_name: str, filename: str) -&gt; Dict[Any, Any]:\n    \"\"\"\n    Load a dictionary from a cache file.\n\n    Args:\n        folder_name: The name of the folder to load the cache from.\n        filename: The name of the file to load the cache from.\n\n    Returns:\n        The loaded dictionary.\n    \"\"\"\n    folder = repo_path_to_abs_path(folder_name)\n    return t.load(folder / filename)\n</code></pre>"},{"location":"reference/utils/misc/#auto_circuit.utils.misc.module_by_name","title":"module_by_name","text":"<pre><code>module_by_name(model: Any, module_name: str) -&gt; Module\n</code></pre> <p>Gets a module from a model by name.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>Any</code> <p>The model to get the module from.</p> required <code>module_name</code> <code>str</code> <p>The name of the module to get.</p> required <p>Returns:</p> Type Description <code>Module</code> <p>The module.</p> Source code in <code>auto_circuit/utils/misc.py</code> <pre><code>def module_by_name(model: Any, module_name: str) -&gt; t.nn.Module:\n    \"\"\"\n    Gets a module from a model by name.\n\n    Args:\n        model: The model to get the module from.\n        module_name: The name of the module to get.\n\n    Returns:\n        The module.\n    \"\"\"\n    init_mod = [model.wrapped_model] if hasattr(model, \"wrapped_model\") else [model]\n    return reduce(getattr, init_mod + module_name.split(\".\"))  # type: ignore\n</code></pre>"},{"location":"reference/utils/misc/#auto_circuit.utils.misc.percent_gpu_mem_used","title":"percent_gpu_mem_used","text":"<pre><code>percent_gpu_mem_used(total_gpu_mib: int = 49000) -&gt; str\n</code></pre> <p>Get the percentage of GPU memory used.</p> <p>Parameters:</p> Name Type Description Default <code>total_gpu_mib</code> <code>int</code> <p>The total amount of GPU memory in MiB.</p> <code>49000</code> <p>Returns:</p> Type Description <code>str</code> <p>The percentage of GPU memory used.</p> Source code in <code>auto_circuit/utils/misc.py</code> <pre><code>def percent_gpu_mem_used(total_gpu_mib: int = 49000) -&gt; str:\n    \"\"\"\n    Get the percentage of GPU memory used.\n\n    Args:\n        total_gpu_mib: The total amount of GPU memory in MiB.\n\n    Returns:\n        The percentage of GPU memory used.\n    \"\"\"\n    return (\n        \"Memory used {:.1f}\".format(\n            ((t.cuda.memory_allocated() / (2**20)) / total_gpu_mib) * 100\n        )\n        + \"%\"\n    )\n</code></pre>"},{"location":"reference/utils/misc/#auto_circuit.utils.misc.remove_hooks","title":"remove_hooks","text":"<pre><code>remove_hooks() -&gt; Iterator[Set[RemovableHandle]]\n</code></pre> <p>Context manager that makes it easier to use temporary PyTorch hooks without accidentally leaving them attached.</p> <p>Add hooks to the set yielded by this context manager, and they will be removed when the context manager exits.</p> <p>Yields:</p> Type Description <code>Set[RemovableHandle]</code> <p>An empty set that can be used to store the handles of the hooks.</p> Source code in <code>auto_circuit/utils/misc.py</code> <pre><code>@contextmanager\ndef remove_hooks() -&gt; Iterator[Set[RemovableHandle]]:\n    \"\"\"\n    Context manager that makes it easier to use temporary PyTorch hooks without\n    accidentally leaving them attached.\n\n    Add hooks to the set yielded by this context manager, and they will be removed when\n    the context manager exits.\n\n    Yields:\n        An empty set that can be used to store the handles of the hooks.\n    \"\"\"\n    handles: Set[RemovableHandle] = set()\n    try:\n        yield handles\n    finally:\n        for handle in handles:\n            handle.remove()\n</code></pre>"},{"location":"reference/utils/misc/#auto_circuit.utils.misc.repo_path_to_abs_path","title":"repo_path_to_abs_path","text":"<pre><code>repo_path_to_abs_path(path: str) -&gt; Path\n</code></pre> <p>Convert a path relative to the repository root to an absolute path.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>A path relative to the repository root.</p> required <p>Returns:</p> Type Description <code>Path</code> <p>The absolute path.</p> Source code in <code>auto_circuit/utils/misc.py</code> <pre><code>def repo_path_to_abs_path(path: str) -&gt; Path:\n    \"\"\"\n    Convert a path relative to the repository root to an absolute path.\n\n    Args:\n        path: A path relative to the repository root.\n\n    Returns:\n        The absolute path.\n    \"\"\"\n    repo_abs_path = Path(__file__).parent.parent.parent.absolute()\n    return repo_abs_path / path\n</code></pre>"},{"location":"reference/utils/misc/#auto_circuit.utils.misc.run_prompt","title":"run_prompt","text":"<pre><code>run_prompt(model: Module, prompt: str, answer: Optional[str] = None, top_k: int = 10, prepend_bos: bool = False)\n</code></pre> <p>Helper function to run a string prompt through a TransformerLens <code>HookedTransformer</code> model and print the top <code>top_k</code> output logits.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>Module</code> <p>The model to run the prompt through.</p> required <code>prompt</code> <code>str</code> <p>The prompt to run through the model.</p> required <code>answer</code> <code>Optional[str]</code> <p>Token to show the rank of in the output logits.</p> <code>None</code> <code>top_k</code> <code>int</code> <p>The number of top output logits to show.</p> <code>10</code> <code>prepend_bos</code> <code>bool</code> <p>Whether to prepend the <code>BOS</code> token to the prompt.</p> <code>False</code> Source code in <code>auto_circuit/utils/misc.py</code> <pre><code>def run_prompt(\n    model: t.nn.Module,\n    prompt: str,\n    answer: Optional[str] = None,\n    top_k: int = 10,\n    prepend_bos: bool = False,\n):\n    \"\"\"\n    Helper function to run a string prompt through a TransformerLens `HookedTransformer`\n    model and print the top `top_k` output logits.\n\n    Args:\n        model: The model to run the prompt through.\n        prompt: The prompt to run through the model.\n        answer: Token to show the rank of in the output logits.\n        top_k: The number of top output logits to show.\n        prepend_bos: Whether to prepend the `BOS` token to the prompt.\n    \"\"\"\n    print(\" \")\n    print(\"Testing prompt\", model.to_str_tokens(prompt))\n    toks = model.to_tokens(prompt, prepend_bos=prepend_bos)\n    logits = model(toks)\n    get_most_similar_embeddings(model, logits[0, -1], answer, top_k=top_k)\n</code></pre>"},{"location":"reference/utils/misc/#auto_circuit.utils.misc.save_cache","title":"save_cache","text":"<pre><code>save_cache(data_dict: Dict[Any, Any], folder_name: str, base_filename: str)\n</code></pre> <p>Save a dictionary to a cache file.</p> <p>Parameters:</p> Name Type Description Default <code>data_dict</code> <code>Dict[Any, Any]</code> <p>The dictionary to save.</p> required <code>folder_name</code> <code>str</code> <p>The name of the folder to save the cache in.</p> required <code>base_filename</code> <code>str</code> <p>The base name of the file to save the cache in. The current date and time will be appended to the base filename.</p> required Source code in <code>auto_circuit/utils/misc.py</code> <pre><code>def save_cache(data_dict: Dict[Any, Any], folder_name: str, base_filename: str):\n    \"\"\"\n    Save a dictionary to a cache file.\n\n    Args:\n        data_dict: The dictionary to save.\n        folder_name: The name of the folder to save the cache in.\n        base_filename: The base name of the file to save the cache in. The current date\n            and time will be appended to the base filename.\n    \"\"\"\n    folder = repo_path_to_abs_path(folder_name)\n    folder.mkdir(parents=True, exist_ok=True)\n    dt_string = datetime.now().strftime(\"%d-%m-%Y_%H-%M-%S\")\n    file_path = folder / f\"{base_filename}-{dt_string}.pkl\"\n    print(f\"Saving cache to {file_path}\")\n    t.save(data_dict, file_path)\n</code></pre>"},{"location":"reference/utils/misc/#auto_circuit.utils.misc.set_module_by_name","title":"set_module_by_name","text":"<pre><code>set_module_by_name(model: Any, module_name: str, new_module: Module)\n</code></pre> <p>Sets a module in a model by name.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>Any</code> <p>The model to set the module in.</p> required <code>module_name</code> <code>str</code> <p>The name of the module to set.</p> required <code>new_module</code> <code>Module</code> <p>The module to replace the existing module with.</p> required Warning <p>This function modifies the model in place.</p> Source code in <code>auto_circuit/utils/misc.py</code> <pre><code>def set_module_by_name(model: Any, module_name: str, new_module: t.nn.Module):\n    \"\"\"\n    Sets a module in a model by name.\n\n    Args:\n        model: The model to set the module in.\n        module_name: The name of the module to set.\n        new_module: The module to replace the existing module with.\n\n    Warning:\n        This function modifies the model in place.\n    \"\"\"\n    parent = model\n    init_mod = [model.wrapped_model] if hasattr(model, \"wrapped_model\") else [model]\n    if \".\" in module_name:\n        parent = reduce(getattr, init_mod + module_name.split(\".\")[:-1])  # type: ignore\n    setattr(parent, module_name.split(\".\")[-1], new_module)\n</code></pre>"},{"location":"reference/utils/patch_wrapper/","title":"Patch wrapper","text":""},{"location":"reference/utils/patch_wrapper/#auto_circuit.utils.patch_wrapper","title":"auto_circuit.utils.patch_wrapper","text":""},{"location":"reference/utils/patch_wrapper/#auto_circuit.utils.patch_wrapper-attributes","title":"Attributes","text":""},{"location":"reference/utils/patch_wrapper/#auto_circuit.utils.patch_wrapper-classes","title":"Classes","text":""},{"location":"reference/utils/patch_wrapper/#auto_circuit.utils.patch_wrapper.PatchWrapperImpl","title":"PatchWrapperImpl","text":"<pre><code>PatchWrapperImpl(module_name: str, module: Module, head_dim: Optional[int] = None, seq_dim: Optional[int] = None, is_src: bool = False, src_idxs: Optional[slice] = None, is_dest: bool = False, patch_mask: Optional[Tensor] = None, in_srcs: Optional[slice] = None)\n</code></pre> <p>             Bases: <code>PatchWrapper</code></p> <p>PyTorch module that wraps another module, a <code>Node</code> in the computation graph of the model. Implements the abstract <code>PatchWrapper</code> class, which exists to work around circular import issues.</p> <p>If the wrapped module is a <code>SrcNode</code>, the tensor <code>self.curr_src_outs</code> (a single instance of which is shared by all PatchWrappers in the model) is updated with the output of the wrapped module.</p> <p>If the wrapped module is a <code>DestNode</code>, the input to the wrapped module is adjusted in order to interpolate the activations of the incoming edges between the default activations (<code>self.curr_src_outs</code>) and the ablated activations (<code>self.patch_src_outs</code>).</p> Note <p>Most <code>PatchWrapper</code>s are both <code>SrcNode</code>s and <code>DestNode</code>s.</p> <p>Parameters:</p> Name Type Description Default <code>module_name</code> <code>str</code> <p>Name of the wrapped module.</p> required <code>module</code> <code>Module</code> <p>The module to wrap.</p> required <code>head_dim</code> <code>Optional[int]</code> <p>The dimension along which to split the heads. In TransformerLens <code>HookedTransformer</code>s this is <code>2</code> because the activations have shape <code>[batch, seq_len, n_heads, head_dim]</code>.</p> <code>None</code> <code>seq_dim</code> <code>Optional[int]</code> <p>The sequence dimension of the model. This is the dimension on which new inputs are concatenated. In transformers, this is <code>1</code> because the activations are of shape <code>[batch_size, seq_len, hidden_dim]</code>.</p> <code>None</code> <code>is_src</code> <code>bool</code> <p>Whether the wrapped module is a <code>SrcNode</code>.</p> <code>False</code> <code>src_idxs</code> <code>Optional[slice]</code> <p>The slice of the list of indices of <code>SrcNode</code>s which output from this module. This is used to slice the shared <code>curr_src_outs</code> tensor when updating the activations of the current forward pass.</p> <code>None</code> <code>is_dest</code> <code>bool</code> <p>Whether the wrapped module is a <code>DestNode</code>.</p> <code>False</code> <code>patch_mask</code> <code>Optional[Tensor]</code> <p>The mask that interpolates between the default activations (<code>curr_src_outs</code>) and the ablation activations (<code>patch_src_outs</code>).</p> <code>None</code> <code>in_srcs</code> <code>Optional[slice]</code> <p>The slice of the list of indices of <code>SrcNode</code>s which input to this module. This is used to slice the shared <code>curr_src_outs</code> tensor and the shared <code>patch_src_outs</code> tensor, when interpolating the activations of the incoming edges.</p> <code>None</code> Source code in <code>auto_circuit/utils/patch_wrapper.py</code> <pre><code>def __init__(\n    self,\n    module_name: str,\n    module: t.nn.Module,\n    head_dim: Optional[int] = None,\n    seq_dim: Optional[int] = None,\n    is_src: bool = False,\n    src_idxs: Optional[slice] = None,\n    is_dest: bool = False,\n    patch_mask: Optional[t.Tensor] = None,\n    in_srcs: Optional[slice] = None,\n):\n    super().__init__()\n    self.module_name: str = module_name\n    self.module: t.nn.Module = module\n    self.head_dim: Optional[int] = head_dim\n    self.seq_dim: Optional[int] = seq_dim\n    self.curr_src_outs: Optional[t.Tensor] = None\n    self.in_srcs: Optional[slice] = in_srcs\n\n    self.is_src = is_src\n    if self.is_src:\n        assert src_idxs is not None\n        self.src_idxs: slice = src_idxs\n\n    self.is_dest = is_dest\n    if self.is_dest:\n        assert patch_mask is not None\n        self.patch_mask: t.nn.Parameter = t.nn.Parameter(patch_mask)\n        self.patch_src_outs: Optional[t.Tensor] = None\n        self.mask_fn: MaskFn = None\n        self.dropout_layer: t.nn.Module = t.nn.Dropout(p=0.0)\n    self.patch_mode = False\n    self.batch_size = None\n\n    assert head_dim is None or seq_dim is None or head_dim &gt; seq_dim\n    dims = range(1, max(head_dim if head_dim else 2, seq_dim if seq_dim else 2))\n    self.dims = \" \".join([\"seq\" if i == seq_dim else f\"d{i}\" for i in dims])\n</code></pre>"},{"location":"reference/utils/patch_wrapper/#auto_circuit.utils.patch_wrapper.PatchWrapperImpl-functions","title":"Functions","text":""},{"location":"reference/utils/patch_wrapper/#auto_circuit.utils.patch_wrapper.PatchWrapperImpl.set_mask_batch_size","title":"set_mask_batch_size","text":"<pre><code>set_mask_batch_size(batch_size: int | None)\n</code></pre> <p>Set the batch size of the patch mask. Should only be used by context manager <code>set_mask_batch_size</code></p> <p>The current primary use case is to collect gradients on the patch mask for each input in the batch.</p> Warning <p>This is an exmperimental feature that breaks some parts of the library and should be used with caution.</p> <p>Parameters:</p> Name Type Description Default <code>batch_size</code> <code>int | None</code> <p>The batch size of the patch mask.</p> required Source code in <code>auto_circuit/utils/patch_wrapper.py</code> <pre><code>def set_mask_batch_size(self, batch_size: int | None):\n    \"\"\"\n    Set the batch size of the patch mask. Should only be used by context manager\n    [`set_mask_batch_size`][auto_circuit.utils.graph_utils.set_mask_batch_size]\n\n    The current primary use case is to collect gradients on the patch mask for\n    each input in the batch.\n\n    Warning:\n        This is an exmperimental feature that breaks some parts of the library and\n        should be used with caution.\n\n    Args:\n        batch_size: The batch size of the patch mask.\n    \"\"\"\n    if batch_size is None and self.batch_size is None:\n        return\n    if batch_size is None:  # removing batch dim\n        self.patch_mask = t.nn.Parameter(self.patch_mask[0].clone())\n    elif self.batch_size is None:  # adding batch_dim\n        self.patch_mask = t.nn.Parameter(\n            self.patch_mask.repeat(batch_size, *((1,) * self.patch_mask.ndim))\n        )\n    elif self.batch_size != batch_size:  # modifying batch dim\n        self.patch_mask = t.nn.Parameter(\n            self.patch_mask[0]\n            .clone()\n            .repeat(batch_size, *((1,) * self.patch_mask.ndim))\n        )\n    self.batch_size = batch_size\n</code></pre>"},{"location":"reference/utils/patch_wrapper/#auto_circuit.utils.patch_wrapper-functions","title":"Functions","text":""},{"location":"reference/utils/patchable_model/","title":"Patchable model","text":""},{"location":"reference/utils/patchable_model/#auto_circuit.utils.patchable_model","title":"auto_circuit.utils.patchable_model","text":""},{"location":"reference/utils/patchable_model/#auto_circuit.utils.patchable_model-attributes","title":"Attributes","text":""},{"location":"reference/utils/patchable_model/#auto_circuit.utils.patchable_model-classes","title":"Classes","text":""},{"location":"reference/utils/patchable_model/#auto_circuit.utils.patchable_model.PatchableModel","title":"PatchableModel","text":"<pre><code>PatchableModel(nodes: Set[Node], srcs: Set[SrcNode], dests: Set[DestNode], edge_dict: Dict[int | None, List[Edge]], edges: Set[Edge], seq_dim: int, seq_len: Optional[int], wrappers: Set[PatchWrapperImpl], src_wrappers: Set[PatchWrapperImpl], dest_wrappers: Set[PatchWrapperImpl], out_slice: Tuple[slice | int, ...], is_factorized: bool, is_transformer: bool, separate_qkv: Optional[bool], kv_caches: Tuple[Optional[HookedTransformerKeyValueCache], ...], wrapped_model: Module)\n</code></pre> <p>             Bases: <code>Module</code></p> <p>A model that can be ablated along individual edges in its computation graph.</p> <p>This class has many of the same methods and attributes as TransformerLens' <code>HookedTransformer</code>s. These are simple wrappers which pass through to the implementation in the wrapped model. These methods and attributes are:</p> <ul> <li><code>forward</code></li> <li><code>run_with_cache</code></li> <li><code>add_hook</code></li> <li><code>reset_hooks</code></li> <li><code>cfg</code></li> <li><code>tokenizer</code></li> <li><code>input_to_embed</code></li> <li><code>blocks</code></li> <li><code>to_tokens</code></li> <li><code>to_str_tokens</code></li> <li><code>to_string</code></li> </ul> <p>Parameters:</p> Name Type Description Default <code>nodes</code> <code>Set[Node]</code> <p>The set of all nodes in the computation graph.</p> required <code>srcs</code> <code>Set[SrcNode]</code> <p>The set of all source nodes in the computation graph.</p> required <code>dests</code> <code>Set[DestNode]</code> <p>The set of all destination nodes in the computation graph.</p> required <code>edge_dict</code> <code>Dict[int | None, List[Edge]]</code> <p>A dictionary mapping sequence positions to the edges at that position.</p> required <code>edges</code> <code>Set[Edge]</code> <p>The set of all edges in the computation graph.</p> required <code>seq_dim</code> <code>int</code> <p>The sequence dimension of the model. This is the dimension on which new inputs are concatenated. In transformers, this is <code>1</code> because the activations are of shape <code>[batch_size, seq_len, hidden_dim]</code>.</p> required <code>seq_len</code> <code>Optional[int]</code> <p>The sequence length of the model inputs. If <code>None</code>, all token positions are simultaneously ablated.</p> required <code>wrappers</code> <code>Set[PatchWrapperImpl]</code> <p>The set of all <code>PatchWrapper</code>s in the model.</p> required <code>src_wrappers</code> <code>Set[PatchWrapperImpl]</code> <p>The set of all <code>PatchWrapper</code>s that are source nodes.</p> required <code>dest_wrappers</code> <code>Set[PatchWrapperImpl]</code> <p>The set of all <code>PatchWrapper</code>s that are destination nodes.</p> required <code>out_slice</code> <code>Tuple[slice | int, ...]</code> <p>Specifies the index/slice of the output of the model to be considered for the task.</p> required <code>is_factorized</code> <code>bool</code> <p>Whether the model is factorized, for Edge Ablation. Otherwise, only Node Ablation is possible.</p> required <code>is_transformer</code> <code>bool</code> <p>Whether the model is a transformer.</p> required <code>separate_qkv</code> <code>Optional[bool]</code> <p>Whether the model has separate query, key, and value inputs. Only used for transformers.</p> required <code>kv_caches</code> <code>Tuple[Optional[HookedTransformerKeyValueCache], ...]</code> <p>A dictionary mapping batch sizes to the past key-value caches of the transformer. Only used for transformers.</p> required <code>wrapped_model</code> <code>Module</code> <p>The model to wrap which is being made patchable.</p> required Source code in <code>auto_circuit/utils/patchable_model.py</code> <pre><code>def __init__(\n    self,\n    nodes: Set[Node],\n    srcs: Set[SrcNode],\n    dests: Set[DestNode],\n    edge_dict: Dict[int | None, List[Edge]],\n    edges: Set[Edge],\n    seq_dim: int,\n    seq_len: Optional[int],\n    wrappers: Set[PatchWrapperImpl],\n    src_wrappers: Set[PatchWrapperImpl],\n    dest_wrappers: Set[PatchWrapperImpl],\n    out_slice: Tuple[slice | int, ...],\n    is_factorized: bool,\n    is_transformer: bool,\n    separate_qkv: Optional[bool],\n    kv_caches: Tuple[Optional[HookedTransformerKeyValueCache], ...],\n    wrapped_model: t.nn.Module,\n) -&gt; None:\n    super().__init__()\n    self.nodes = nodes\n    self.srcs = srcs\n    self.dests = dests\n    self.edge_dict = edge_dict\n    self.edges = edges\n    self.n_edges = len(edges)\n    self.edge_name_dict = defaultdict(dict)\n    for edge in edges:\n        self.edge_name_dict[edge.seq_idx][edge.name] = edge\n    self.seq_dim = seq_dim\n    self.seq_len = seq_len\n    self.wrappers = wrappers\n    self.src_wrappers = src_wrappers\n    self.dest_wrappers = dest_wrappers\n    self.patch_masks = {}\n    for dest_wrapper in self.dest_wrappers:\n        self.patch_masks[dest_wrapper.module_name] = dest_wrapper.patch_mask\n    self.out_slice = out_slice\n    self.is_factorized = is_factorized\n    self.is_transformer = is_transformer\n    if is_transformer:\n        assert separate_qkv is not None\n    self.separate_qkv = separate_qkv\n    if all([kv_cache is None for kv_cache in kv_caches]) or len(kv_caches) == 0:\n        self.kv_caches = None\n    else:\n        self.kv_caches = {}\n        for kv_cache in kv_caches:\n            if kv_cache is not None:\n                batch_size = kv_cache.previous_attention_mask.shape[0]\n                self.kv_caches[batch_size] = kv_cache\n    self.wrapped_model = wrapped_model\n</code></pre>"},{"location":"reference/utils/patchable_model/#auto_circuit.utils.patchable_model.PatchableModel-functions","title":"Functions","text":""},{"location":"reference/utils/patchable_model/#auto_circuit.utils.patchable_model.PatchableModel.circuit_prune_scores","title":"circuit_prune_scores","text":"<pre><code>circuit_prune_scores(edges: Optional[Collection[Edge | str]] = None, edge_dict: Optional[Dict[Edge, float] | Dict[str, float]] = None, bool: bool = False) -&gt; PruneScores\n</code></pre> <p>Convert a set of edges to a corresponding <code>PruneScores</code> object.</p> <p>Parameters:</p> Name Type Description Default <code>edges</code> <code>Optional[Collection[Edge | str]]</code> <p>The set of edges or edge names to convert to prune scores.</p> <code>None</code> <code>bool</code> <code>bool</code> <p>Whether to return the prune scores as boolean type tensors.</p> <code>False</code> <p>Returns:</p> Type Description <code>PruneScores</code> <p>The prune scores corresponding to the set of edges.</p> Source code in <code>auto_circuit/utils/patchable_model.py</code> <pre><code>def circuit_prune_scores(\n    self,\n    edges: Optional[Collection[Edge | str]] = None,\n    edge_dict: Optional[Dict[Edge, float] | Dict[str, float]] = None,\n    bool: bool = False,\n) -&gt; PruneScores:\n    \"\"\"\n    Convert a set of edges to a corresponding\n    [`PruneScores`][auto_circuit.types.PruneScores] object.\n\n    Args:\n        edges: The set of edges or edge names to convert to prune scores.\n        bool: Whether to return the prune scores as boolean type tensors.\n\n    Returns:\n        The prune scores corresponding to the set of edges.\n    \"\"\"\n    ps = self.new_prune_scores()\n    assert not (edges is None and edge_dict is None), \"Must specify edges\"\n\n    # TODO: Raise an error if one of the edge names doesn't exist.\n    if edges is not None:\n        for edge in self.edges:\n            if edge in edges or edge.name in edges:\n                ps[edge.dest.module_name][edge.patch_idx] = 1.0\n    else:\n        assert edge_dict is not None\n        for e in self.edges:\n            if e in edge_dict.keys():\n                ps[e.dest.module_name][e.patch_idx] = edge_dict[e]  # type: ignore\n            if e.name in edge_dict.keys():\n                ps[e.dest.module_name][e.patch_idx] = edge_dict[e]  # type: ignore\n    if bool:\n        return dict([(mod, mask.bool()) for (mod, mask) in ps.items()])\n    else:\n        return ps\n</code></pre>"},{"location":"reference/utils/patchable_model/#auto_circuit.utils.patchable_model.PatchableModel.current_patch_masks_as_prune_scores","title":"current_patch_masks_as_prune_scores","text":"<pre><code>current_patch_masks_as_prune_scores() -&gt; PruneScores\n</code></pre> <p>Convert the current patch masks to a corresponding <code>PruneScores</code> object.</p> <p>Returns:</p> Type Description <code>PruneScores</code> <p>The prune scores corresponding to the current patch masks.</p> Source code in <code>auto_circuit/utils/patchable_model.py</code> <pre><code>def current_patch_masks_as_prune_scores(self) -&gt; PruneScores:\n    \"\"\"\n    Convert the current patch masks to a corresponding\n    [`PruneScores`][auto_circuit.types.PruneScores] object.\n\n    Returns:\n        The prune scores corresponding to the current patch masks.\n    \"\"\"\n    return dict([(mod, mask.data) for (mod, mask) in self.patch_masks.items()])\n</code></pre>"},{"location":"reference/utils/patchable_model/#auto_circuit.utils.patchable_model.PatchableModel.forward","title":"forward","text":"<pre><code>forward(*args: Any, **kwargs: Any) -&gt; Any\n</code></pre> <p>Wrapper around the forward method of the wrapped model. If <code>kv_caches</code> is not <code>None</code>, the KV cache is passed to the wrapped model as a keyword argument.</p> Source code in <code>auto_circuit/utils/patchable_model.py</code> <pre><code>def forward(self, *args: Any, **kwargs: Any) -&gt; Any:\n    \"\"\"\n    Wrapper around the forward method of the wrapped model. If `kv_caches` is not\n    `None`, the KV cache is passed to the wrapped model as a keyword argument.\n    \"\"\"\n    if self.kv_caches is None or \"past_kv_cache\" in kwargs:\n        return self.wrapped_model(*args, **kwargs)\n    else:\n        batch_size = args[0].shape[0]\n        kv = self.kv_caches[batch_size]\n        return self.wrapped_model(*args, past_kv_cache=kv, **kwargs)\n</code></pre>"},{"location":"reference/utils/patchable_model/#auto_circuit.utils.patchable_model.PatchableModel.input_to_embed","title":"input_to_embed","text":"<pre><code>input_to_embed(*args: Any, **kwargs: Any) -&gt; Any\n</code></pre> <p>Wrapper around the <code>input_to_embed</code> method of the wrapped TransformerLens <code>HookedTransformer</code>. If <code>kv_caches</code> is not <code>None</code>, the KV cache is passed to the wrapped model as a keyword argument.</p> Source code in <code>auto_circuit/utils/patchable_model.py</code> <pre><code>def input_to_embed(self, *args: Any, **kwargs: Any) -&gt; Any:\n    \"\"\"\n    Wrapper around the `input_to_embed` method of the wrapped TransformerLens\n    `HookedTransformer`. If `kv_caches` is not `None`, the KV cache is passed to the\n    wrapped model as a keyword argument.\n    \"\"\"\n    if self.kv_caches is None:\n        return self.wrapped_model.input_to_embed(*args, **kwargs)\n    else:\n        batch_size = args[0].shape[0]\n        kv = self.kv_caches[batch_size]\n        return self.wrapped_model.input_to_embed(*args, past_kv_cache=kv, **kwargs)\n</code></pre>"},{"location":"reference/utils/patchable_model/#auto_circuit.utils.patchable_model.PatchableModel.new_prune_scores","title":"new_prune_scores","text":"<pre><code>new_prune_scores(init_val: float = 0.0) -&gt; PruneScores\n</code></pre> <p>A new <code>PruneScores</code> instance with the same keys and shapes as the current patch masks, initialized to <code>init_val</code>.</p> <p>Parameters:</p> Name Type Description Default <code>init_val</code> <code>float</code> <p>The initial value to set all the prune scores to.</p> <code>0.0</code> <p>Returns:</p> Type Description <code>PruneScores</code> <p>A new <code>PruneScores</code> instance.</p> Source code in <code>auto_circuit/utils/patchable_model.py</code> <pre><code>def new_prune_scores(self, init_val: float = 0.0) -&gt; PruneScores:\n    \"\"\"\n    A new [`PruneScores`][auto_circuit.types.PruneScores] instance with the same\n    keys and shapes as the current patch masks, initialized to `init_val`.\n\n    Args:\n        init_val: The initial value to set all the prune scores to.\n\n    Returns:\n        A new [`PruneScores`][auto_circuit.types.PruneScores] instance.\n    \"\"\"\n    prune_scores: PruneScores = {}\n    for (mod_name, mask) in self.patch_masks.items():\n        prune_scores[mod_name] = t.full_like(mask.data, init_val)\n    return prune_scores\n</code></pre>"},{"location":"reference/utils/patchable_model/#auto_circuit.utils.patchable_model.PatchableModel.run_with_cache","title":"run_with_cache","text":"<pre><code>run_with_cache(*args: Any, **kwargs: Any) -&gt; Any\n</code></pre> <p>Wrapper around the <code>run_with_cache</code> method of the wrapped TransformerLens <code>HookedTransformer</code>. If <code>kv_caches</code> is not <code>None</code>, the KV cache is passed to the wrapped model as a keyword argument.</p> Source code in <code>auto_circuit/utils/patchable_model.py</code> <pre><code>def run_with_cache(self, *args: Any, **kwargs: Any) -&gt; Any:\n    \"\"\"\n    Wrapper around the `run_with_cache` method of the wrapped TransformerLens\n    `HookedTransformer`. If `kv_caches` is not `None`, the KV cache is passed to the\n    wrapped model as a keyword argument.\n    \"\"\"\n    if self.kv_caches is None:\n        return self.wrapped_model.run_with_cache(*args, **kwargs)\n    else:\n        batch_size = args[0].shape[0]\n        kv = self.kv_caches[batch_size]\n        return self.wrapped_model.run_with_cache(*args, past_kv_cache=kv, **kwargs)\n</code></pre>"},{"location":"reference/utils/tensor_ops/","title":"Tensor ops","text":""},{"location":"reference/utils/tensor_ops/#auto_circuit.utils.tensor_ops","title":"auto_circuit.utils.tensor_ops","text":""},{"location":"reference/utils/tensor_ops/#auto_circuit.utils.tensor_ops-attributes","title":"Attributes","text":""},{"location":"reference/utils/tensor_ops/#auto_circuit.utils.tensor_ops-classes","title":"Classes","text":""},{"location":"reference/utils/tensor_ops/#auto_circuit.utils.tensor_ops-functions","title":"Functions","text":""},{"location":"reference/utils/tensor_ops/#auto_circuit.utils.tensor_ops.batch_answer_diff_percents","title":"batch_answer_diff_percents","text":"<pre><code>batch_answer_diff_percents(pred_vals: Tensor, target_vals: Tensor, batch: PromptPairBatch) -&gt; Tensor\n</code></pre> <p>Find the percentage difference between the predicted logit differences and the target logit differences.</p> <p>Parameters:</p> Name Type Description Default <code>pred_vals</code> <code>Tensor</code> <p>The predicted logit values or some tensor of the same shape.</p> required <code>target_vals</code> <code>Tensor</code> <p>The target logit values or some tensor of the same shape.</p> required <code>batch</code> <code>PromptPairBatch</code> <p>The batch of prompts and answers.</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>The percentage difference between the predicted logit differences and the target</p> <code>Tensor</code> <p>logit differences.</p> Source code in <code>auto_circuit/utils/tensor_ops.py</code> <pre><code>def batch_answer_diff_percents(\n    pred_vals: t.Tensor, target_vals: t.Tensor, batch: PromptPairBatch\n) -&gt; t.Tensor:\n    \"\"\"\n    Find the percentage difference between the predicted logit differences and the\n    target logit differences.\n\n    Args:\n        pred_vals: The predicted logit values or some tensor of the same shape.\n        target_vals: The target logit values or some tensor of the same shape.\n        batch: The batch of prompts and answers.\n\n    Returns:\n        The percentage difference between the predicted logit differences and the target\n        logit differences.\n    \"\"\"\n    target_answer_diff = batch_answer_diffs(target_vals, batch)\n    pred_answer_diff = batch_answer_diffs(pred_vals, batch)\n    return (pred_answer_diff / target_answer_diff) * 100\n</code></pre>"},{"location":"reference/utils/tensor_ops/#auto_circuit.utils.tensor_ops.batch_answer_diffs","title":"batch_answer_diffs","text":"<pre><code>batch_answer_diffs(vals: Tensor, batch: PromptPairBatch) -&gt; Tensor\n</code></pre> <p>Find the difference between the average value of the correct answers and the average value of the wrong answers for each prompt in the batch.</p> <p>If the batch answers are a <code>List</code>, rather than a <code>Tensor</code>, the function will be much slower.</p> <p>Parameters:</p> Name Type Description Default <code>vals</code> <code>Tensor</code> <p>The logits values or some tensor of the same shape.</p> required <code>batch</code> <code>PromptPairBatch</code> <p>The batch of prompts and answers.</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>The difference between the average value of the correct answers and the average</p> <code>Tensor</code> <p>value of the wrong answers for each prompt in the batch.</p> Source code in <code>auto_circuit/utils/tensor_ops.py</code> <pre><code>def batch_answer_diffs(vals: t.Tensor, batch: PromptPairBatch) -&gt; t.Tensor:\n    \"\"\"\n    Find the difference between the average value of the correct answers and the average\n    value of the wrong answers for each prompt in the batch.\n\n    If the batch answers are a `List`, rather than a `Tensor`, the function will be much\n    slower.\n\n    Args:\n        vals: The logits values or some tensor of the same shape.\n        batch: The batch of prompts and answers.\n\n    Returns:\n        The difference between the average value of the correct answers and the average\n        value of the wrong answers for each prompt in the batch.\n    \"\"\"\n    answers = batch.answers\n    wrong_answers = batch.wrong_answers\n    if isinstance(answers, t.Tensor) and isinstance(wrong_answers, t.Tensor):\n        # We don't use vocab_avg_val here because we need to calculate the average\n        # difference between the correct and wrong answers not the difference between\n        # the average correct and average incorrect answers\n        # We do take the mean over each set of correct and incorrect answers (often\n        # there is only one of each, eg. in the IOI task).\n        ans_avgs = t.gather(vals, dim=-1, index=answers).mean(dim=-1)\n        wrong_avgs = t.gather(vals, dim=-1, index=wrong_answers).mean(dim=-1)\n        return ans_avgs - wrong_avgs\n    else:\n        # If each prompt has a different number of answers we have a list of tensors\n        assert isinstance(answers, list) and isinstance(wrong_answers, list)\n        ans_avgs = [vocab_avg_val(v, a) for v, a in zip(vals, answers)]\n        wrong_avgs = [vocab_avg_val(v, w) for v, w in zip(vals, wrong_answers)]\n        return t.stack(ans_avgs) - t.stack(wrong_avgs)\n</code></pre>"},{"location":"reference/utils/tensor_ops/#auto_circuit.utils.tensor_ops.batch_avg_answer_diff","title":"batch_avg_answer_diff","text":"<pre><code>batch_avg_answer_diff(vals: Tensor, batch: PromptPairBatch) -&gt; Tensor\n</code></pre> <p>Wrapper of <code>batch_answer_diffs</code> that returns the mean of the differences.</p> Source code in <code>auto_circuit/utils/tensor_ops.py</code> <pre><code>def batch_avg_answer_diff(vals: t.Tensor, batch: PromptPairBatch) -&gt; t.Tensor:\n    \"\"\"\n    Wrapper of [`batch_answer_diffs`][auto_circuit.utils.tensor_ops.batch_answer_diffs]\n    that returns the mean of the differences.\n    \"\"\"\n    return batch_answer_diffs(vals, batch).mean()\n</code></pre>"},{"location":"reference/utils/tensor_ops/#auto_circuit.utils.tensor_ops.batch_avg_answer_val","title":"batch_avg_answer_val","text":"<pre><code>batch_avg_answer_val(vals: Tensor, batch: PromptPairBatch, wrong_answer: bool = False) -&gt; Tensor\n</code></pre> <p>Get the average value of the logits (or some function of them) for the correct answers in the batch.</p> <p>Parameters:</p> Name Type Description Default <code>vals</code> <code>Tensor</code> <p>The logits values or some tensor of the same shape.</p> required <code>batch</code> <code>PromptPairBatch</code> <p>The batch of prompts and answers.</p> required <code>wrong_answer</code> <code>bool</code> <p>Whether to get the average value of the wrong answers instead of the correct answers.</p> <code>False</code> <p>Returns:</p> Type Description <code>Tensor</code> <p>The average value of the logits for the correct answers in the batch.</p> Source code in <code>auto_circuit/utils/tensor_ops.py</code> <pre><code>def batch_avg_answer_val(\n    vals: t.Tensor, batch: PromptPairBatch, wrong_answer: bool = False\n) -&gt; t.Tensor:\n    \"\"\"\n    Get the average value of the logits (or some function of them) for the correct\n    answers in the batch.\n\n    Args:\n        vals: The logits values or some tensor of the same shape.\n        batch: The batch of prompts and answers.\n        wrong_answer: Whether to get the average value of the wrong answers instead of\n            the correct answers.\n\n    Returns:\n        The average value of the logits for the correct answers in the batch.\n    \"\"\"\n    answers = batch.answers if not wrong_answer else batch.wrong_answers\n    if isinstance(answers, t.Tensor):\n        return vocab_avg_val(vals, answers)\n    else:\n        # If each prompt has a different number of answers we have a list of tensor\n        assert isinstance(answers, list)\n        return t.stack([vocab_avg_val(v, a) for v, a in zip(vals, answers)]).mean()\n</code></pre>"},{"location":"reference/utils/tensor_ops/#auto_circuit.utils.tensor_ops.correct_answer_greater_than_incorrect_proportion","title":"correct_answer_greater_than_incorrect_proportion","text":"<pre><code>correct_answer_greater_than_incorrect_proportion(logits: Tensor, batch: PromptPairBatch) -&gt; Tensor\n</code></pre> <p>What proportion of the logits have the correct answer with a greater value than all the wrong answers?</p> <p>Parameters:</p> Name Type Description Default <code>logits</code> <code>Tensor</code> <p>The logits values or some tensor of the same shape.</p> required <code>batch</code> <code>PromptPairBatch</code> <p>The batch of prompts and answers.</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>The proportion of the logits that have the correct answer with a greater value</p> <code>Tensor</code> <p>than all the wrong answers.</p> Source code in <code>auto_circuit/utils/tensor_ops.py</code> <pre><code>def correct_answer_greater_than_incorrect_proportion(\n    logits: t.Tensor, batch: PromptPairBatch\n) -&gt; t.Tensor:\n    \"\"\"\n    What proportion of the logits have the correct answer with a greater value than all\n    the wrong answers?\n\n    Args:\n        logits: The logits values or some tensor of the same shape.\n        batch: The batch of prompts and answers.\n\n    Returns:\n        The proportion of the logits that have the correct answer with a greater value\n        than all the wrong answers.\n    \"\"\"\n    answers = batch.answers\n    wrong_answers = batch.wrong_answers\n    if isinstance(answers, t.Tensor) and isinstance(wrong_answers, t.Tensor):\n        assert answers.shape[-1] == 1\n        answer_logits = t.gather(logits, dim=-1, index=answers)\n        wrong_logits = t.gather(logits, dim=-1, index=wrong_answers)\n        combined_logits = t.cat([answer_logits, wrong_logits], dim=-1)\n        max_idxs = combined_logits.argmax(dim=-1)\n        return (max_idxs == 0).float().mean()\n    else:\n        assert isinstance(answers, list) and isinstance(wrong_answers, list)\n        corrects = []\n        for i, (prompt_ans, prompt_wrong_ans) in enumerate(zip(answers, wrong_answers)):\n            assert prompt_ans.shape == (1,)\n            answer_logits = t.gather(logits[i], dim=-1, index=prompt_ans)\n            wrong_logits = t.gather(logits[i], dim=-1, index=prompt_wrong_ans)\n            combined_logits = t.cat([answer_logits, wrong_logits], dim=-1)\n            max_idxs = combined_logits.argmax(dim=-1)\n            corrects.append(max_idxs == 0)\n        return t.stack(corrects).float().mean()\n</code></pre>"},{"location":"reference/utils/tensor_ops/#auto_circuit.utils.tensor_ops.correct_answer_proportion","title":"correct_answer_proportion","text":"<pre><code>correct_answer_proportion(logits: Tensor, batch: PromptPairBatch) -&gt; Tensor\n</code></pre> <p>What proportion of the logits have the correct answer as the maximum?</p> <p>Parameters:</p> Name Type Description Default <code>logits</code> <code>Tensor</code> <p>The logits values or some tensor of the same shape.</p> required <code>batch</code> <code>PromptPairBatch</code> <p>The batch of prompts and answers.</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>The proportion of the logits that have the correct answer as the maximum.</p> Source code in <code>auto_circuit/utils/tensor_ops.py</code> <pre><code>def correct_answer_proportion(logits: t.Tensor, batch: PromptPairBatch) -&gt; t.Tensor:\n    \"\"\"\n    What proportion of the logits have the correct answer as the maximum?\n\n    Args:\n        logits: The logits values or some tensor of the same shape.\n        batch: The batch of prompts and answers.\n\n    Returns:\n        The proportion of the logits that have the correct answer as the maximum.\n    \"\"\"\n    answers = batch.answers\n    if isinstance(answers, t.Tensor):\n        assert answers.shape[-1] == 1\n        max_idxs = t.argmax(logits, dim=-1, keepdim=True)\n        return (max_idxs == answers).float().mean()\n    else:\n        # If each prompt has a different number of answers we have a list of tensors\n        assert isinstance(answers, list)\n        corrects = []\n        for prompt_idx, prompt_answer in enumerate(answers):\n            assert prompt_answer.shape == (1,)\n            corrects.append((t.argmax(logits[prompt_idx], dim=-1) == prompt_answer))\n        return t.stack(corrects).float().mean()\n</code></pre>"},{"location":"reference/utils/tensor_ops/#auto_circuit.utils.tensor_ops.desc_prune_scores","title":"desc_prune_scores","text":"<pre><code>desc_prune_scores(prune_scores: PruneScores) -&gt; Tensor\n</code></pre> <p>Flatten the prune scores into a single, 1-dimensional tensor and sort them in descending order.</p> <p>Parameters:</p> Name Type Description Default <code>prune_scores</code> <code>PruneScores</code> <p>The prune scores to flatten and sort.</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>The flattened and sorted prune scores.</p> Source code in <code>auto_circuit/utils/tensor_ops.py</code> <pre><code>def desc_prune_scores(prune_scores: PruneScores) -&gt; t.Tensor:\n    \"\"\"\n    Flatten the prune scores into a single, 1-dimensional tensor and sort them in\n    descending order.\n\n    Args:\n        prune_scores: The prune scores to flatten and sort.\n\n    Returns:\n        The flattened and sorted prune scores.\n    \"\"\"\n    return flat_prune_scores(prune_scores).abs().sort(descending=True).values\n</code></pre>"},{"location":"reference/utils/tensor_ops/#auto_circuit.utils.tensor_ops.flat_prune_scores","title":"flat_prune_scores","text":"<pre><code>flat_prune_scores(prune_scores: PruneScores) -&gt; Tensor\n</code></pre> <p>Flatten the prune scores into a single, 1-dimensional tensor.</p> <p>Parameters:</p> Name Type Description Default <code>prune_scores</code> <code>PruneScores</code> <p>The prune scores to flatten.</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>The flattened prune scores.</p> Source code in <code>auto_circuit/utils/tensor_ops.py</code> <pre><code>def flat_prune_scores(prune_scores: PruneScores) -&gt; t.Tensor:\n    \"\"\"\n    Flatten the prune scores into a single, 1-dimensional tensor.\n\n    Args:\n        prune_scores: The prune scores to flatten.\n\n    Returns:\n        The flattened prune scores.\n    \"\"\"\n    return t.cat([ps.flatten() for _, ps in prune_scores.items()])\n</code></pre>"},{"location":"reference/utils/tensor_ops/#auto_circuit.utils.tensor_ops.multibatch_kl_div","title":"multibatch_kl_div","text":"<pre><code>multibatch_kl_div(input_logprobs: Tensor, target_logprobs: Tensor) -&gt; Tensor\n</code></pre> <p>Compute the average KL divergence between two sets of log probabilities. Assumes the last dimension of <code>input_logprobs</code> and <code>target_logprobs</code> is the log probability of each class. The other dimensions are batch dimensions.</p> <p>Parameters:</p> Name Type Description Default <code>input_logprobs</code> <code>Tensor</code> <p>The input log probabilities.</p> required <code>target_logprobs</code> <code>Tensor</code> <p>The target log probabilities.</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>The average KL divergence between the input and target log probabilities.</p> Source code in <code>auto_circuit/utils/tensor_ops.py</code> <pre><code>def multibatch_kl_div(input_logprobs: t.Tensor, target_logprobs: t.Tensor) -&gt; t.Tensor:\n    \"\"\"\n    Compute the average KL divergence between two sets of log probabilities.\n    Assumes the last dimension of `input_logprobs` and `target_logprobs` is the log\n    probability of each class. The other dimensions are batch dimensions.\n\n    Args:\n        input_logprobs: The input log probabilities.\n        target_logprobs: The target log probabilities.\n\n    Returns:\n        The average KL divergence between the input and target log probabilities.\n    \"\"\"\n    assert input_logprobs.shape == target_logprobs.shape\n    kl_div_sum = t.nn.functional.kl_div(\n        input_logprobs,\n        target_logprobs,\n        reduction=\"sum\",\n        log_target=True,\n    )\n    n_batch = math.prod(input_logprobs.shape[:-1])\n    return kl_div_sum / n_batch\n</code></pre>"},{"location":"reference/utils/tensor_ops/#auto_circuit.utils.tensor_ops.prune_scores_threshold","title":"prune_scores_threshold","text":"<pre><code>prune_scores_threshold(prune_scores: PruneScores | Tensor, edge_count: int) -&gt; Tensor\n</code></pre> <p>Return the minimum absolute value of the top <code>edge_count</code> prune scores. Supports passing in a pre-sorted tensor of prune scores to avoid re-sorting.</p> <p>Parameters:</p> Name Type Description Default <code>prune_scores</code> <code>PruneScores | Tensor</code> <p>The prune scores to threshold.</p> required <code>edge_count</code> <code>int</code> <p>The number of edges that should be above the threshold.</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>The threshold value.</p> Source code in <code>auto_circuit/utils/tensor_ops.py</code> <pre><code>def prune_scores_threshold(\n    prune_scores: PruneScores | t.Tensor, edge_count: int\n) -&gt; t.Tensor:\n    \"\"\"\n    Return the minimum absolute value of the top `edge_count` prune scores.\n    Supports passing in a pre-sorted tensor of prune scores to avoid re-sorting.\n\n    Args:\n        prune_scores: The prune scores to threshold.\n        edge_count: The number of edges that should be above the threshold.\n\n    Returns:\n        The threshold value.\n    \"\"\"\n    if edge_count == 0:\n        return t.tensor(float(\"inf\"))  # return the maximum value so no edges are pruned\n\n    if isinstance(prune_scores, t.Tensor):\n        assert prune_scores.ndim == 1\n        return prune_scores[edge_count - 1]\n    else:\n        return desc_prune_scores(prune_scores)[edge_count - 1]\n</code></pre>"},{"location":"reference/utils/tensor_ops/#auto_circuit.utils.tensor_ops.sample_hard_concrete","title":"sample_hard_concrete","text":"<pre><code>sample_hard_concrete(mask: Tensor, batch_size: int, mask_expanded: bool = False) -&gt; Tensor\n</code></pre> <p>Sample from the hard concrete distribution (Louizos et al., 2017).</p> <p>Parameters:</p> Name Type Description Default <code>mask</code> <code>Tensor</code> <p>The mask whose values parameterize the distribution.</p> required <code>batch_size</code> <code>int</code> <p>The number of samples to draw.</p> required <code>mask_expanded</code> <code>bool</code> <p>Whether the mask has a batch dimension at the start.</p> <code>False</code> <p>Returns:</p> Type Description <code>Tensor</code> <p>A sample for each element in the mask for each batch element. The returned</p> <code>Tensor</code> <p>tensor has shape <code>(batch_size, *mask.shape)</code>.</p> Source code in <code>auto_circuit/utils/tensor_ops.py</code> <pre><code>def sample_hard_concrete(\n    mask: t.Tensor, batch_size: int, mask_expanded: bool = False\n) -&gt; t.Tensor:\n    \"\"\"\n    Sample from the hard concrete distribution\n    ([Louizos et al., 2017](https://arxiv.org/abs/1712.01312)).\n\n    Args:\n        mask: The mask whose values parameterize the distribution.\n        batch_size: The number of samples to draw.\n        mask_expanded: Whether the mask has a batch dimension at the start.\n\n    Returns:\n        A sample for each element in the mask for each batch element. The returned\n        tensor has shape `(batch_size, *mask.shape)`.\n    \"\"\"\n    if not mask_expanded:\n        mask = mask.repeat(batch_size, *([1] * mask.ndim))\n    else:\n        assert mask.size(0) == batch_size\n    u = t.zeros_like(mask).uniform_().clamp(0.0001, 0.9999)\n    s = t.sigmoid((u.log() - (1 - u).log() + mask) / temp)\n    s_bar = s * (right - left) + left\n    return s_bar.clamp(min=0.0, max=1.0)\n</code></pre>"}]}